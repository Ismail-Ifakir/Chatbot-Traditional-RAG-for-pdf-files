{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc3IuOTg7-hc",
        "outputId": "d4686983-504c-4ac2-b223-9bf3934efd35"
      },
      "id": "gc3IuOTg7-hc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Document Structure\n",
        "\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "TKcFJNW2OtMi"
      },
      "id": "TKcFJNW2OtMi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=Document(\n",
        "    page_content=\"this is the main text content I am using to create RAG\",\n",
        "    metadata={\n",
        "        \"source\":\"exmaple.txt\",\n",
        "        \"pages\":1,\n",
        "        \"author\":\"Krish Naik\",\n",
        "        \"date_created\":\"2025-01-01\"\n",
        "    }\n",
        ")\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TftHwSB3Ou28",
        "outputId": "e4587cd2-b4e7-493e-c9ce-c31520dbfa08"
      },
      "id": "TftHwSB3Ou28",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'exmaple.txt', 'pages': 1, 'author': 'Krish Naik', 'date_created': '2025-01-01'}, page_content='this is the main text content I am using to create RAG')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a simple txt file\n",
        "import os\n",
        "os.makedirs(\"/content/drive/MyDrive/AI Engineering/Traditional RAG/data/text_files\",exist_ok=True)"
      ],
      "metadata": {
        "id": "oHWGSvFmOu6g"
      },
      "id": "oHWGSvFmOu6g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts={\n",
        "    \"/content/drive/MyDrive/AI Engineering/Traditional RAG/data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
        "\n",
        "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
        "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
        "programming languages in the world.\n",
        "\n",
        "Key Features:\n",
        "- Easy to learn and use\n",
        "- Extensive standard library\n",
        "- Cross-platform compatibility\n",
        "- Strong community support\n",
        "\n",
        "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
        "\n",
        "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
        "\n",
        "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
        "from experience without being explicitly programmed. It focuses on developing computer programs\n",
        "that can access data and use it to learn for themselves.\n",
        "\n",
        "Types of Machine Learning:\n",
        "1. Supervised Learning: Learning with labeled data\n",
        "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
        "3. Reinforcement Learning: Learning through rewards and penalties\n",
        "\n",
        "Applications include image recognition, speech processing, and recommendation systems\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "}\n",
        "\n",
        "for filepath,content in sample_texts.items():\n",
        "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "print(\"✅ Sample text files created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruNQo7srOu-P",
        "outputId": "15754a3e-feb5-4c45-fc6d-8a01644a9072"
      },
      "id": "ruNQo7srOu-P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sample text files created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TextLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader=TextLoader(\"/content/drive/MyDrive/AI Engineering/Traditional RAG/data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
        "document=loader.load()\n",
        "print(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nu6GKYQOvCs",
        "outputId": "81db7d2d-0fd5-4b1e-a63d-5ab093566adf"
      },
      "id": "-nu6GKYQOvCs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Directory Loader\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "## load all the text files from the directory\n",
        "dir_loader=DirectoryLoader(\n",
        "    \"/content/drive/MyDrive/AI Engineering/Traditional RAG/data/text_files\",\n",
        "    glob=\"**/*.txt\", ## Pattern to match files\n",
        "    loader_cls= TextLoader, ##loader class to use\n",
        "    loader_kwargs={'encoding': 'utf-8'},\n",
        "    show_progress=False\n",
        "\n",
        ")\n",
        "\n",
        "documents=dir_loader.load()\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TeJLL8VOvHb",
        "outputId": "b46cba20-b84a-488f-e9df-7e087cd4001d"
      },
      "id": "_TeJLL8VOvHb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlF_XyZOQCqx",
        "outputId": "e2ca10de-7e72-4e97-9c6a-f871e9ccb286"
      },
      "id": "YlF_XyZOQCqx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "\n",
        "## load all the text files from the directory\n",
        "dir_loader=DirectoryLoader(\n",
        "    \"/content/drive/MyDrive/AI Engineering/Traditional RAG/data\",\n",
        "    glob=\"**/*.pdf\", ## Pattern to match files\n",
        "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
        "    show_progress=False\n",
        "\n",
        ")\n",
        "\n",
        "pdf_documents=dir_loader.load()\n",
        "pdf_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edZjPJhqPz-E",
        "outputId": "ddfe16ba-730d-448c-e220-58c5d0f045ba"
      },
      "id": "edZjPJhqPz-E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 0}, page_content='MELD: A Multimodal Multi-Party Dataset\\nfor Emotion Recognition in Conversations\\nSoujanya Poria†, Devamanyu HazarikaΦ, Navonil Majumder‡,\\nGautam Naik¶, Erik Cambria¶, Rada Mihalceaι\\n†Information Systems Technology and Design, SUTD, Singapore\\nΦSchool of Computing, National University of Singapore, Singapore\\n‡Centro de Investigaci´on en Computaci´on, Instituto Polit´ecnico Nacional, Mexico\\n¶Computer Science & Engineering, Nanyang Technological University, Singapore\\nιComputer Science & Engineering, University of Michigan, USA\\nsporia@sutd.edu.sg, hazarika@comp.nus.edu.sg,\\nnavo@nlp.cic.ipn.mx, gautam@sentic.net,\\ncambria@ntu.edu.sg, mihalcea@umich.edu\\nAbstract\\nEmotion recognition in conversations (ERC) is\\na challenging task that has recently gained pop-\\nularity due to its potential applications. Un-\\ntil now, however, there has been no large-\\nscale multimodal multi-party emotional con-\\nversational database containing more than\\ntwo speakers per dialogue.\\nTo address this\\ngap, we propose the Multimodal EmotionLines\\nDataset (MELD), an extension and enhance-\\nment of EmotionLines. MELD contains about\\n13,000 utterances from 1,433 dialogues from\\nthe TV-series Friends. Each utterance is an-\\nnotated with emotion and sentiment labels,\\nand encompasses audio, visual, and textual\\nmodalities.\\nWe propose several strong mul-\\ntimodal baselines and show the importance\\nof contextual and multimodal information for\\nemotion recognition in conversations.\\nThe\\nfull dataset is available for use at http://\\naffective-meld.github.io.\\n1\\nIntroduction\\nWith the rapid growth of Artiﬁcial Intelligence (AI),\\nmultimodal emotion recognition has become a ma-\\njor research topic, primarily due to its potential\\napplications in many challenging tasks, such as\\ndialogue generation, user behavior understanding,\\nmultimodal interaction, and others. A conversa-\\ntional emotion recognition system can be used to\\ngenerate appropriate responses by analyzing user\\nemotions (Zhou et al., 2017; Rashkin et al., 2018).\\nAlthough signiﬁcant research work has been car-\\nried out on multimodal emotion recognition using\\naudio, visual, and text modalities (Zadeh et al.,\\n2016a; Wollmer et al., 2013), signiﬁcantly less\\nwork has been devoted to emotion recognition in\\nconversations (ERC). One main reason for this\\nis the lack of a large multimodal conversational\\ndataset.\\nAccording to Poria et al. (2019), ERC presents\\nseveral challenges such as conversational context\\nmodeling, emotion shift of the interlocutors, and\\nothers, which make the task more difﬁcult to ad-\\ndress. Recent work proposes solutions based on\\nmultimodal memory networks (Hazarika et al.,\\n2018). However, they are mostly limited to dyadic\\nconversations, and thus not scalable to ERC with\\nmultiple interlocutors. This calls for a multi-party\\nconversational data resource that can encourage\\nresearch in this direction.\\nIn a conversation, the participants’ utterances\\ngenerally depend on their conversational context.\\nThis is also true for their associated emotions. In\\nother words, the context acts as a set of parameters\\nthat may inﬂuence a person to speak an utterance\\nwhile expressing a certain emotion. Modeling this\\ncontext can be done in different ways, e.g., by us-\\ning recurrent neural networks (RNNs) and mem-\\nory networks (Hazarika et al., 2018; Poria et al.,\\n2017; Serban et al., 2017). Figure 1 shows an ex-\\nample where the speakers change their emotions\\n(emotion shifts) as the dialogue develops. The emo-\\ntional dynamics here depend on both the previous\\nutterances and their associated emotions. For ex-\\nample, the emotion shift in utterance eight (in the\\nﬁgure) is hard to determine unless cues are taken\\nfrom the facial expressions and the conversational\\nhistory of both speakers. Modeling such complex\\ninter-speaker dependencies is one of the major chal-\\nlenges in conversational modeling.\\nConversation in its natural form is multimodal.\\nIn dialogues, we rely on others’ facial expressions,\\nvocal tonality, language, and gestures to anticipate\\ntheir stance. For emotion recognition, multimodal-\\narXiv:1810.02508v6  [cs.CL]  4 Jun 2019'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 1}, page_content='1)\\nYou liked it? You \\nreally liked it?\\n2) Oh, yeah!\\n3) Which part \\nexactly?\\n4) The whole thing! \\nCan we go?\\n5) What about the \\nscene with the \\nkangaroo?\\n6) I was surprised to \\nsee a kangaroo in a \\nworld war epic.\\n7) You fell asleep!\\n8) Don’t go, \\nI’m sorry.\\nSurprise \\n(Positive)\\nNeutral \\n(Neutral)\\nNeutral \\n(Neutral)\\nAnger \\n(Negative)\\nDialogue\\nJoey\\nChandler\\nJoy \\n(Positive)\\nNeutral \\n(Neutral)\\nSurprise \\n(Negative)\\nSadness \\n(Negative)\\nEmotion \\n(Sentiment) :\\nFigure 1: Emotion shift of speakers in a dialogue in comparison with their previous emotions.\\nFigure 2: Importance of multimodal cues. Green shows\\nprimary modalities responsible for sentiment and emotion.\\nity is particularly important. For the utterances with\\nlanguage that is difﬁcult to understand, we often re-\\nsort to other modalities, such as prosodic and visual\\ncues, to identify their emotions. Figure 2 presents\\nexamples from the dataset where the presence of\\nmultimodal signals in addition to the text itself is\\nnecessary in order to make correct predictions of\\ntheir emotions and sentiments.\\nMultimodal emotion recognition of sequential\\nturns encounters several other challenges. One\\nsuch example is the classiﬁcation of short utter-\\nances. Utterances like “yeah”, “okay”, “no” can\\nexpress varied emotions depending on the con-\\ntext and discourse of the dialogue. However, due\\nto the difﬁculty of perceiving emotions from text\\nalone, most models resort to assigning the majority\\nclass (e.g., non-neutral in EmotionLines). Approx-\\nimately 42% of the utterances in MELD are shorter\\nthan ﬁve words. We thus provide access to the mul-\\ntimodal data sources for each dialogue and posit\\nthat this additional information would beneﬁt the\\nemotion recognition task by improving the context\\nrepresentation and supplementing the missing or\\nmisleading signals from other modalities. Surplus\\ninformation from attributes such as the speaker’s fa-\\ncial expressions or intonation in speech could guide\\nmodels for better classiﬁcation. We also provide\\nevidence for these claims through our experiments.\\nThe development of conversational AI thus de-\\npends on the use of both contextual and multimodal\\ninformation. The publicly available datasets for\\nmultimodal emotion recognition in conversations\\n– IEMOCAP and SEMAINE – have facilitated a\\nsigniﬁcant number of research projects, but also\\nhave limitations due to their relatively small num-\\nber of total utterances and the lack of multi-party\\nconversations. There are also other multimodal\\nemotion and sentiment analysis datasets, such as\\nMOSEI (Zadeh et al., 2018), MOSI (Zadeh et al.,\\n2016b), and MOUD (P´erez-Rosas et al., 2013), but\\nthey contain individual narratives instead of dia-\\nlogues. On the other hand, EmotionLines (Chen\\net al., 2018) is a dataset that contains dialogues\\nfrom the popular TV-series Friends with more than\\ntwo speakers. However, EmotionLines can only be\\nused for textual analysis as it does not provide data\\nfrom other modalities.\\nIn this work, we extend, improve, and further de-\\nvelop the EmotionLines dataset for the multimodal\\nscenario. We propose the Multimodal Emotion-\\nLines Dataset (MELD), which includes not only\\ntextual dialogues, but also their corresponding vi-\\nsual and audio counterparts. This paper makes\\nseveral contributions:\\n• MELD contains multi-party conversations that\\nare more challenging to classify than dyadic vari-\\nants available in previous datasets.\\n• There are more than 13,000 utterances in MELD,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 2}, page_content='which makes our dataset nearly double the size\\nof existing multimodal conversational datasets.\\n• MELD provides multimodal sources and can be\\nused in a multimodal affective dialogue system\\nfor enhanced grounded learning.\\n• We establish a strong baseline, proposed by Ma-\\njumder et al. (2019), which is capable of emo-\\ntion recognition in multi-party dialogues by inter-\\nparty dependency modeling.\\nThe remainder of the paper is organized as\\nfollows: Section 2 illustrates the EmotionLines\\ndataset; we then present MELD in Section 3; strong\\nbaselines and experiments are elaborated in Sec-\\ntion 4; future directions and applications of MELD\\nare covered in Section 5 and 6, respectively; ﬁnally,\\nSection 7 concludes the paper.\\n2\\nEmotionLines Dataset\\nThe MELD dataset has evolved from the Emo-\\ntionLines dataset developed by Chen et al. (2018).\\nEmotionLines contains dialogues from the popu-\\nlar sitcom Friends, where each dialogue contains\\nutterances from multiple speakers.\\nEmotionLines was created by crawling the dia-\\nlogues from each episode and then grouping them\\nbased on the number of utterances in a dialogue\\ninto four groups of [5, 9], [10, 14], [15, 19], and [20,\\n24] utterances respectively. Finally, 250 dialogues\\nwere sampled randomly from each of these groups,\\nresulting in the ﬁnal dataset of 1,000 dialogues.\\n2.1\\nAnnotation\\nThe utterances in each dialogue were annotated\\nwith the most appropriate emotion category. For\\nthis purpose, Ekman’s six universal emotions (Joy,\\nSadness, Fear, Anger, Surprise, and Disgust) were\\nconsidered as annotation labels. This annotation\\nlist was extended with two additional emotion la-\\nbels: Neutral and Non-Neutral.\\nEach utterance was annotated by ﬁve workers\\nfrom the Amazon Mechanical Turk (AMT) plat-\\nform. A majority voting scheme was applied to\\nselect a ﬁnal emotion label for each utterance. The\\noverall Fleiss’ kappa score of this annotation pro-\\ncess was 0.34.\\n3\\nMultimodal EmotionLines Dataset\\n(MELD)\\nWe start the construction of the MELD corpus by\\nextracting the starting and ending timestamps of\\nDataset\\n# Dialogues\\n# Utterances\\ntrain\\ndev\\ntest\\ntrain\\ndev\\ntest\\nEmotionLines\\n720\\n80\\n200\\n10561\\n1178\\n2764\\nMELD\\n1039\\n114\\n280\\n9989\\n1109\\n2610\\nTable 1: Comparison between the original EmotionLines\\ndataset and MELD.\\nall utterances from every dialogue in the Emo-\\ntionLines dataset. To accomplish this, we crawl\\nthrough the subtitles of all the episodes and heuris-\\ntically extract the respective timestamps. In partic-\\nular, we enforce the following constraints:\\n1. Timestamps of the utterances in a dialogue must\\nbe in an increasing order.\\n2. All the utterances in a dialogue have to belong\\nto the same episode and scene.\\nThese constraints revealed a few outliers in Emo-\\ntionLines where some dialogues span across scenes\\nor episodes. For example, the dialogue in Table 2\\ncontains two natural dialogues from episode 4 and\\n20 of season 6 and 5, respectively. We decided\\nto ﬁlter out these anomalies, thus resulting in a\\ndifferent number of total dialogues in MELD as\\ncompared to EmotionLines (see Table 1).\\nNext, we employ three annotators to label each\\nutterance, followed by a majority voting to decide\\nthe ﬁnal label of the utterances. We drop a few\\nutterances where all three annotations were differ-\\nent, and also remove their corresponding dialogues\\nto maintain coherence. A total of 89 utterances\\nspanning 11 dialogues fell under this category.\\nFinally, after obtaining the timestamp of each\\nutterance, we extract their corresponding audio-\\nvisual clips from the source episode followed by\\nthe extraction of audio content from these clips.\\nWe format the audio ﬁles as 16-bit PCM WAV ﬁles\\nfor further processing. The ﬁnal dataset includes\\nvisual, audio, and textual modalities for each utter-\\nance.1\\n3.1\\nDataset Re-annotation\\nThe utterances in the original EmotionLines dataset\\nwere annotated by looking only at the transcripts.\\nHowever, due to our focus on multimodality, we\\nre-annotate all the utterances by asking the three\\nannotators to also look at the available video clip\\nof the utterances. We then use majority-voting to\\nobtain the ﬁnal label for each utterance.\\n1We consulted a legal ofﬁce to verify that the usage and\\ndistribution of very short length videos fall under the fair use\\ncategory.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 3}, page_content='Episode\\nUtterance\\nSpeaker\\nEmotion\\nSentiment\\nS6.E4\\nWhat are you talkin about? I never left you! Youve always been my agent!\\nJoey\\nsurprise\\nnegative\\nReally?!\\nEstelle\\nsurprise\\npositive\\nYeah!\\nJoey\\njoy\\npositive\\nOh well, no harm, no foul.\\nEstelle\\nneutral\\nneutral\\nS5.E20\\nOkay, you guys free tonight?\\nGary\\nneutral\\nneutral\\nYeah!!\\nRoss\\njoy\\npositive\\nTonight? You-you didn’t say it was going to be at nighttime.\\nChandler\\nsurprise\\nnegative\\nTable 2: A dialogue in EmotionLines where utterances from two different episodes are present. The ﬁrst four utterances in this\\ndialogue have been taken from episode 4 of season 6. The last three utterances in red font are from episode 20 of season 5.\\nThe annotators were graduate students with high\\nproﬁciency in English speaking and writing. Be-\\nfore starting the annotation, they were briefed about\\nthe annotation process with a few examples.\\nWe achieve an overall Fleiss’ kappa score of 0.43\\nwhich is higher than the original EmotionLines an-\\nnotation whose kappa score was 0.34 (kappa of\\nIEMOCAP annotation process was 0.4), thus sug-\\ngesting the usefulness of the additional modalities\\nduring the annotation process.\\n2,772 utterances in the EmotionLines dataset\\nwere labeled as non-neutral where the annotators\\nagreed that the emotion is not neutral but they\\ncould not reach agreement regarding the correct\\nemotion label. This hampers classiﬁcation, as the\\nnon-neutral utterance space and the other emotion-\\nlabel spaces get conﬂated. In our case, we remove\\nthe utterances where the annotators fail to reach an\\nagreement on the deﬁnite emotion label.\\nThe number of disagreements in our annotation\\nprocess is 89, which is much lower than the 2,772\\ndisagreements in EmotionLines, reﬂecting again\\nthe annotation improvement obtained through a\\nmultimodal dataset. Table 3 shows examples of\\nutterances where the annotators failed to reach con-\\nsensus.\\nTable 4 shows the label-wise comparison be-\\ntween EmotionLines and MELD dataset. For most\\nof the utterances in MELD, the annotations match\\nthe original annotations in EmotionLines.\\nYet,\\nthere exists a signiﬁcant amount of samples whose\\nutterances have been changed in the re-annotation\\nprocess. For example, the utterance This guy fell\\nasleep! (see Table 5), was labeled as non-neutral\\nUtterance\\nAnnotator 1\\nAnnotator 2\\nAnnotator 3\\nYou know? Forget it!\\nsadness\\ndisgust\\nanger\\nOh no-no, give me\\nanger\\nsadness\\nneutral\\nsome speciﬁcs.\\nI was surprised to see a\\nsurprise\\nanger\\njoy\\nkangaroo in a World War epic.\\nOr, call an ambulance.\\nanger\\nsurprise\\nneutral\\nTable 3: Some examples of the utterances for which annota-\\ntors could not reach consensus.\\nEmotionLines\\nMELD\\nCategories\\nTrain\\nDev\\nTest\\nTrain\\nDev\\nTest\\nEmotion\\nanger\\n524\\n85\\n163\\n1109\\n153\\n345\\ndisgust\\n244\\n26\\n68\\n271\\n22\\n68\\nfear\\n190\\n29\\n36\\n268\\n40\\n50\\njoy\\n1283\\n123\\n304\\n1743\\n163\\n402\\nneutral\\n4752\\n491\\n1287\\n4710\\n470\\n1256\\nsadness\\n351\\n62\\n85\\n683\\n111\\n208\\nsurprise\\n1221\\n151\\n286\\n1205\\n150\\n281\\nSentiment\\nnegative\\n-\\n-\\n-\\n2945\\n406\\n833\\nneutral\\n-\\n-\\n-\\n4710\\n470\\n1256\\npositive\\n-\\n-\\n-\\n2334\\n233\\n521\\nTable 4: Emotion and Sentiment distribution in MELD vs.\\nEmotionLines.\\nin EmotionLines but after viewing the associated\\nvideo clip, it is correctly re-labeled as anger in\\nMELD.\\nThe video of this utterance reveals an angry and\\nfrustrated facial expression along with a high vocal\\npitch, thus helping to recognize its correct emotion.\\nThe annotators of EmotionLines had access to the\\ncontext, but this was not sufﬁcient, as the avail-\\nability of additional modalities can sometime bring\\nmore information for the classiﬁcation of such in-\\nstances. These scenarios justify both context and\\nmultimodality to be important aspects for emotion\\nrecognition in conversation.\\nTimestamp alignment.\\nThere are many utter-\\nances in the subtitles that are grouped within iden-\\ntical timestamps in the subtitle ﬁles. In order to\\nﬁnd the accurate timestamp for each utterance, we\\nuse a transcription alignment tool Gentle,2 which\\nautomatically aligns a transcript with the audio by\\nextracting word-level timestamps from the audio\\n(see Table 6). In Table 7, we show the ﬁnal format\\nof the MELD dataset.\\nDyadic MELD.\\nWe also provide another version\\nof MELD where all the non-extendable contiguous\\ndyadic sub-dialogues of MELD are extracted. For\\nexample, let a three-party dialogue in MELD with\\nspeaker ids 1,2,3 have their turns in the following\\n2http://github.com/lowerquality/gentle'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 4}, page_content='order: [1,2,1,2,3,2,1,2].\\nFrom this dialogue sequence, dyadic MELD\\nwill have the following sub-dialogues as samples:\\n[1,2,1,2],[2,3,2] and [2,1,2]. However, the re-\\nported results in this paper are obtained using only\\nthe multiparty variant of MELD.\\nUtterance\\nSpeaker\\nMELD\\nEmotionLines\\nI’m so sorry!\\nChandler\\nsadness\\nsadness\\nLook!\\nChandler\\nsurprise\\nsurprise\\nThis guy fell asleep!\\nChandler\\nanger\\nnon-neutral\\nTable 5: Difference in annotation between EmotionLines and\\nMELD.\\n3.2\\nDataset Exploration\\nAs mentioned before, we use seven emotions for\\nthe annotation, i.e., anger, disgust, fear, joy, neutral,\\nsadness, and surprise, across the training, develop-\\nment, and testing splits (see Table 4). It can be seen\\nthat the emotion distribution in the dataset is expect-\\nedly non-uniform with the majority emotion being\\nneutral. We have also converted these ﬁne-grained\\nemotion labels into more coarse-grained sentiment\\nclasses by considering anger, disgust, fear, sadness\\nas negative, joy as positive, and neutral as neutral\\nsentiment-bearing class. Surprise is an example of\\na complex emotion which can be expressed with\\nboth positive and negative sentiment. The three\\nannotators who performed the utterance annotation\\nfurther annotated the surprise utterances into either\\npositive or negative sentiment classes. The entire\\nsentiment annotation task reaches a Fleiss’ kappa\\nscore of 0.91. The distribution of positive, negative,\\nneutral sentiment classes is given in Table 4.\\nTable 8 presents several key statistics of the\\ndataset. The average utterance length – i.e. number\\nof words in an utterance – is nearly the same across\\ntraining, development, and testing splits. On aver-\\nage, three emotions are present in each dialogue of\\nthe dataset. The average duration of an utterance\\nis 3.59 seconds. The emotion shift of a speaker\\nin a dialogue makes emotion recognition task very\\nchallenging. We observe that the number of such\\nemotion shifts in successive utterances of a speaker\\nin a dialogue is very frequent: 4003, 427, and 1003\\nin train/dev/test splits, respectively. Figure 1 shows\\nan example where speaker’s emotion changes with\\ntime in the dialogue.\\nCharacter Distribution.\\nIn Figure 3, we present\\nthe distributional details of the primary characters\\nin MELD. Figure a and b illustrate the distribution\\nacross the emotion and sentiment labels, respec-\\ntively. Figure c shows the overall coverage of the\\nspeakers across the dataset. Multiple infrequent\\nspeakers (< 1% utterances) are grouped as Others.\\n3.3\\nRelated Datasets\\nMost of the available datasets in multimodal sen-\\ntiment analysis and emotion recognition are non-\\nconversational. MOSI (Zadeh et al., 2016b), MO-\\nSEI (Zadeh et al., 2018), and MOUD (P´erez-Rosas\\net al., 2013) are such examples that have drawn\\nsigniﬁcant interest from the research community.\\nOn the other hand, IEMOCAP and SEMAINE are\\ntwo popular dyadic conversational datasets where\\neach utterance in a dialogue is labeled by emotion.\\nThe SEMAINE Database is an audiovisual\\ndatabase created for building agents that can en-\\ngage a person in a sustained and emotional con-\\nversation (McKeown et al., 2012). It consists of\\ninteractions involving a human and an operator (ei-\\nther a machine or a person simulating a machine).\\nThe dataset contains 150 participants, 959 conver-\\nsations, each lasting around 5 minutes. A subset\\nof this dataset was used in AVEC 2012’s fully con-\\ntinuous sub-challenge (Schuller et al., 2012) that\\nrequires predictions of four continuous affective\\ndimensions: arousal, expectancy, power, and va-\\nlence. The gold annotations are available for every\\n0.2 second in each video for a total of 95 videos\\ncomprising 5,816 utterances.\\nThe Interactive Emotional Dyadic Motion\\nCapture Database (IEMOCAP) consists of\\nvideos of dyadic conversations among pairs of 10\\nspeakers spanning 10 hours of various dialogue sce-\\nnarios (Busso et al., 2008). Videos are segmented\\ninto utterances with annotations of ﬁne-grained\\nemotion categories: anger, happiness, sadness, neu-\\ntral, excitement, and frustration. IEMOCAP also\\nprovides continuous attributes: activation, valence,\\nand dominance. These two types of discrete and\\ncontinuous emotional descriptors facilitate the com-\\nplementary insights about the emotional expres-\\nsions of humans and emotional communications\\nbetween people. The labels in IEMOCAP were\\nannotated by at least three annotators per utterance\\nand self-assessment manikins (SAMs) were also\\nemployed to evaluate the corpus (Bradley and Lang,\\n1994).\\n3.4\\nComparison with MELD\\nBoth resources mentioned above are extensively\\nused in this ﬁeld of research and contain settings'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 5}, page_content='Incorrect Splits\\nCorrected Splits\\nUtterance\\nSeason\\nEpisode\\nStart Time\\nEnd Time\\nStart Time\\nEnd Time\\nChris says they’re closing\\n3\\n6\\n00:05:57,023\\n00:05:59,691\\n00:05:57,023\\n00:05:58,734\\ndown the bar.\\nNo way!\\n3\\n6\\n00:05:57,023\\n00:05:59,691\\n00:05:58,734\\n00:05:59,691\\nTable 6: Example of timestamp alignment using the Gentle alignment tool.\\nUtterance\\nSpeaker\\nEmotion\\nD ID\\nU ID\\nSeason\\nEpisode\\nStartTime\\nEndTime\\nBut then who? The waitress I went out\\nJoey\\nsurprise\\n1\\n0\\n9\\n23\\n00:36:40,364\\n00:36:42,824\\nwith last month?\\nYou know? Forget it!\\nRachel\\nsadness\\n1\\n1\\n9\\n23\\n00:36:44,368\\n00:36:46,578\\nTable 7: MELD dataset format for a dialogue. Notations: D ID = dialogue ID, U ID = utterance ID. StartTime and EndTime\\nare in hh:mm:ss,ms format.\\nthat are aligned to the components of MELD. How-\\never, MELD is different in terms of both com-\\nplexity and quantity. Both IEMOCAP and SE-\\nMAINE contain dyadic conversations, wherein the\\ndialogues in MELD are multi-party. Multi-party\\nconversations are more challenging compared to\\ndyadic. They provide a ﬂexible setting where multi-\\nple speakers can engage. From a research perspec-\\ntive, such availability also demands proposed dia-\\nlogue models to be scalable towards multiple speak-\\ners. MELD also includes more than 13000 emotion\\nlabeled utterances, which is nearly double the an-\\nnotated utterances in IEMOCAP and SEMAINE.\\nTable 9 provides information on the number of\\navailable dialogues and their constituent utterances\\nfor all three datasets, i.e., IEMOCAP, SEMAINE,\\nand MELD. Table 10 shows the distribution for\\ncommon emotions as well as highlights a few key\\nstatistics of IEMOCAP and MELD.\\n4\\nExperiments\\n4.1\\nFeature Extraction\\nWe follow Poria et al. (2017) to extract features\\nfor each utterance in MELD. For textual fea-\\ntures, we initialize each token with pre-trained\\n300-dimensional GloVe vectors (Pennington et al.,\\n2014) and feed them to a 1D-CNN to extract 100\\nMELD Statistics\\nTrain\\nDev\\nTest\\n# of modalities\\n{a,v,t}\\n{a,v,t}\\n{a,v,t}\\n# of unique words\\n10,643\\n2,384\\n4,361\\nAvg./Max utterance length\\n8.0/69\\n7.9/37\\n8.2/45\\n# of dialogues\\n1039\\n114\\n280\\n# of dialogues dyadic MELD\\n2560\\n270\\n577\\n# of utterances\\n9989\\n1109\\n2610\\n# of speakers\\n260\\n47\\n100\\nAvg. # of utterances per dialogue\\n9.6\\n9.7\\n9.3\\nAvg. # of emotions per dialogue\\n3.3\\n3.3\\n3.2\\nAvg./Max # of speakers per dialogue\\n2.7/9\\n3.0/8\\n2.6/8\\n# of emotion shift\\n4003\\n427\\n1003\\nAvg. duration of an utterance\\n3.59s\\n3.59s\\n3.58s\\nTable 8: Dataset Statistics. {a,v,t} = {audio, visual, text}\\ndimensional textual features. For audio, we use the\\npopular toolkit openSMILE (Eyben et al., 2010),\\nwhich extracts 6373 dimensional features constitut-\\ning several low-level descriptors and various sta-\\ntistical functionals of varied vocal and prosodic\\nfeatures. As the audio representation is high dimen-\\nsional, we employ L2-based feature selection with\\nsparse estimators, such as SVMs, to get a dense\\nrepresentation of the overall audio segment. For the\\nbaselines, we do not use visual features, as video-\\nbased speaker identiﬁcation and localization is an\\nopen problem. Bimodal features are obtained by\\nconcatenating audio and textual features.\\n4.2\\nBaseline Models\\nTo provide strong benchmarks for MELD, we per-\\nform experiments with multiple baselines. Hyper-\\nparameter details for each baseline can be found at\\nhttp://github.com/senticnet/meld.\\ntext-CNN applies CNN to the input utterances\\nwithout considering the context of the conversa-\\ntion (Kim, 2014). This model represents the sim-\\nplest baseline which does not leverage context or\\nmultimodality in its approach.\\nbcLSTM is a strong baseline proposed by Po-\\nria et al. (2017), which represents context using a\\nbi-directional RNN. It follows a two-step hierarchi-\\ncal process that models uni-modal context ﬁrst and\\nthen bi-modal context features. For unimodal text,\\na CNN-LSTM model extracts contextual represen-\\ntations for each utterance taking the GloVe em-\\nDataset\\nType\\n# dialogues\\n# utterances\\ntrain\\ndev\\ntest\\ntrain\\ndev\\ntest\\nIEMOCAP\\nacted\\n120\\n31\\n5810\\n1623\\nSEMAINE\\nacted\\n58\\n22\\n4386\\n1430\\nMELD\\nacted\\n1039\\n114\\n280\\n9989\\n1109\\n2610\\nTable 9: Comparison among IEMOCAP, SEMAINE, and\\nproposed MELD datasets'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 6}, page_content='0\\n750\\n1500\\n2250\\n3000\\nChandler\\nRoss\\nPhoebe\\nMonica\\nJoey\\nRachel\\nOthers\\nNeutral\\nSurprise\\nJoy\\nSadness\\nFear\\nAnger\\nDisgust\\n0\\n750\\n1500\\n2250\\n3000\\nChandler\\nRoss\\nPhoebe\\nMonica\\nJoey\\nRachel\\nOthers\\nPositive\\nNegative\\nNeutral\\na) Character-Emotion\\nb) Character-Sentiment\\nOthers\\n16%\\nRachel\\n16%\\nJoey\\n16%\\nMonica\\n14%\\nPhoebe\\n13%\\nRoss\\n14%\\nChandler\\n11%\\nc) Character-distribution\\nFigure 3: Character distribution across MELD.\\nDataset\\nEmotions\\nOther Statistics\\nHappy/Joy\\nAnger\\nDisgust\\nSadness\\nSurprise\\nNeutral\\nAvg.\\nutterence length\\n#Unique\\nwords\\nAvg.\\nconversation length\\nIEMOCAP\\n648\\n1103\\n2\\n1084\\n107\\n1708\\n15.8\\n3,598\\n49.2\\nMELD\\n2308\\n1607\\n361\\n1002\\n1636\\n6436\\n8.0\\n10,643\\n9.6\\nTable 10: Comparison among IEMOCAP and proposed MELD datasets.\\nbeddings as input. For unimodal audio, an LSTM\\nmodel gets audio representations for each audio ut-\\nterance feature vector. Finally, the contextual repre-\\nsentations from the unimodal variants are supplied\\nto the bimodal model for classiﬁcation. bcLSTM\\ndoes not distinguish among different speakers and\\nmodels a conversation as a single sequence.\\nDialogueRNN represents the current state of the\\nart for conversational emotion detection (Majumder\\net al., 2019). It is a strong baseline with effective\\nmechanisms to model context by tracking individ-\\nual speaker states throughout the conversation for\\nemotion classiﬁcation. DialogueRNN is capable of\\nhandling multi-party conversation so it can be di-\\nrectly applied on MELD. It employs three stages of\\ngated recurrent units (GRU) (Chung et al., 2014) to\\nmodel emotional context in conversations. The spo-\\nken utterances are fed into two GRUs: global and\\nparty GRU to update the context and speaker state,\\nrespectively. In each turn, the party GRU updates\\nits state based on 1) the utterance spoken, 2) the\\nspeaker’s previous state, and 3) the conversational\\ncontext summarized by the global GRU through an\\nattention mechanism. Finally, the updated speaker\\nstate is fed into the emotion GRU which models the\\nemotional information for classiﬁcation. Attention\\nmechanism is used on top of the emotion GRU to\\nleverage contextual utterances by different speak-\\ners at various distances. To analyze the role of\\nmultimodal signals, we analyze DialogueRNN and\\nbcLSTM on MELD for both uni and multimodal\\nsettings. Training involved usage of class weights\\nto alleviate imbalance issues.\\n4.3\\nResults\\nWe provide results for the two tasks of sentiment\\nand emotion classiﬁcation on MELD. Table 13\\nshows the performance of sentiment classiﬁcation\\nby using DialogueRNN, whose multimodal variant\\nachieves the best performance (67.56% F-score)\\nsurpassing multimodal bcLSTM (66.68% F-score).\\nMultimodal DialogueRNN also outperforms its uni-\\nmodal counterparts. However, the improvement\\ndue to fusion is about 1.4% higher than the textual\\nmodality which suggests the possibility of further\\nimprovement through better fusion mechanisms.\\nThe textual modality outperforms the audio modal-\\nity by about 17%, which indicates the importance\\nof spoken language in sentiment analysis. For posi-\\ntive sentiment, audio modality performs poorly. It\\nwould be interesting to analyze the clues speciﬁc to\\npositive sentiment bearing utterances in MELD that\\nthe audio modality could not capture. Future work\\nshould aim for enhanced audio feature extraction\\nschemes to improve the classiﬁcation performance.\\nTable 11 presents the results of the baseline models\\non MELD emotion classiﬁcation. The performance\\non the emotion classes disgust, fear, and sadness\\nare particularly poor. The primary reason for this\\nis the inherent imbalance in the dataset which has\\nfewer training instances for these mentioned emo-\\ntion classes (see Table 4). We partially tackle this\\nby using class-weights as hyper-parameters.\\nYet, the imbalance calls for further improvement\\nfor future work to address. We also observe high'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 7}, page_content='Models\\nEmotions\\nanger\\ndisgust\\nfear\\njoy\\nneutral\\nsadness\\nsurprise\\nw-avg.\\ntext-CNN\\n34.49\\n8.22\\n3.74\\n49.39\\n74.88\\n21.05\\n45.45\\n55.02\\ncMKL\\ntext+audio\\n39.50\\n16.10\\n3.75\\n51.39\\n72.73\\n23.95\\n46.25\\n55.51\\nbcLSTM\\ntext\\n42.06\\n21.69\\n7.75\\n54.31\\n71.63\\n26.92\\n48.15\\n56.44\\naudio\\n25.85\\n6.06\\n2.90\\n15.74\\n61.86\\n14.71\\n19.34\\n39.08\\ntext+audio\\n43.39\\n23.66\\n9.38\\n54.48\\n76.67\\n24.34\\n51.04\\n59.25\\nDialogueRNN\\ntext\\n40.59\\n2.04\\n8.93\\n50.27\\n75.75\\n24.19\\n49.38\\n57.03\\naudio\\n35.18\\n5.13\\n5.56\\n13.17\\n65.57\\n14.01\\n20.47\\n41.79\\ntext+audio\\n43.65\\n7.89\\n11.68\\n54.40\\n77.44\\n34.59\\n52.51\\n60.25\\nTable 11: Test-set weighted F-score results of DialogueRNN for emotion classiﬁcation in MELD. Note: w-avg denotes\\nweighted-average. text-CNN and cMKL: contextual information were not used.\\nmis-classiﬁcation rate between the anger, disgust,\\nand fear emotion categories as these emotions have\\nsubtle differences among them causing harder dis-\\nambiguation. Similar to sentiment classiﬁcation\\ntrends, the textual classiﬁer outperforms (57.03%\\nF-score) the audio classiﬁer (41.79% F-score).\\nMultimodal fusion helps in improving the emo-\\ntion recognition performance by 3%. However,\\nmultimodal classiﬁer performs worse than the tex-\\ntual classiﬁer in classifying sadness. To analyze fur-\\nther, we also run experiments on 5-class emotions\\nby dropping the infrequent fear and disgust emo-\\ntions (see Table 12). Not surprisingly, the results\\nimprove over the 7-class setting with signiﬁcantly\\nbetter performance by the multimodal variant.\\nOverall, emotion classiﬁcation performs poorer\\nthan sentiment classiﬁcation. This observation is\\nexpected as emotion classiﬁcation deals with clas-\\nsiﬁcation with more ﬁne-grained classes.\\n4.4\\nAdditional Analysis\\nRole of Context.\\nOne of the main purposes of\\nMELD is to train contextual modeling in a conver-\\nsation for emotion recognition. Table 11 and 13\\nshow that the improvement over the non-contextual\\nmodel such as text-CNN – which only uses a CNN\\n(see Section 4.1) – is 1.4% to 2.5%.\\nInter-speaker inﬂuence.\\nOne of the important\\nconsiderations while modeling conversational emo-\\nMode\\nEmotions\\nang\\njoy\\nneu\\nsad\\nsurp\\nw-avg.\\nbcLSTM\\nT+A\\n45.9\\n52.2\\n77.9\\n11.2\\n49.9\\n60.6\\ndRNN∗\\nT\\n41.7\\n53.7\\n77.8\\n21.2\\n47.7\\n60.8\\nA\\n34.1\\n18.8\\n66.2\\n16.0\\n16.6\\n44.3\\nT+A\\n48.2\\n53.2\\n77.7\\n20.3\\n48.5\\n61.6\\n∗dRNN: DialogueRNN, T: text, A: audio\\nTable 12: Test-set weighted F-score results of DialogueRNN\\nfor 5-class emotion classiﬁcation in MELD. Note: w-avg\\ndenotes weighted-average. surp: surprise emotion.\\ntion dynamics is the inﬂuence of fellow speakers\\nin the multi-party setting. We analyze this factor\\nby looking at the activation of the attention module\\non the global GRU in DialogueRNN. We observe\\nthat in 63% (882/1381) of the correct test predic-\\ntions, the highest historical attention is given to\\nutterances from different speakers. This signiﬁ-\\ncant proportion suggests inter-speaker inﬂuence to\\nbe an important parameter. Unlike DialogueRNN,\\nMode\\nSentiments\\npos.\\nneg.\\nneu.\\nw-avg.\\ntext-CNN\\n53.23\\n55.42\\n74.69\\n64.25\\nbcLSTM\\nT+A\\n74.68\\n57.87\\n60.04\\n66.68\\ndRNN∗\\nT\\n54.35\\n60.10\\n74.94\\n66.10\\nA\\n25.47\\n45.53\\n62.33\\n49.61\\nT+A\\n54.29\\n58.18\\n78.40\\n67.56\\nTable 13: Test set weighted F-score results of DialogueRNN\\nfor sentiment classiﬁcation in MELD.\\nbcLSTM does not utilize speaker information while\\ndetecting emotion. Table 11 shows that in all the\\nexperiments, DialogueRNN outperforms bcLSTM\\nby 1-2% margin. This result supports the claim\\nby Majumder et al. (2019) that speaker-speciﬁc\\nmodeling of emotion recognition is beneﬁcial as\\nit helps in improving context representation and\\nincorporates important clues such as inter-speaker\\nrelations.\\nEmotion shifts.\\nThe ability to anticipate the emo-\\ntion shifts within speakers throughout the course\\nof a dialogue has synergy with better emotion clas-\\nsiﬁcation. In our results, DialogueRNN achieves\\na recall of 66% for detecting emotion shifts. How-\\never, in the ideal scenario, we would want to detect\\nshift along with the correct emotion class. For\\nthis setting, DialogueRNN gets a recall of 36.7%.\\nThe deterioration observed is expected as solving\\nboth tasks together has a higher complexity. Future\\nmethods would need to improve upon their capa-\\nbilities of detecting shifts to improve the emotion'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 8}, page_content='classiﬁcation.\\nContextual distance.\\nFigure 4 presents the dis-\\ntribution of distances between the target utterance\\nand its second highest attended utterance within\\nthe conversation by DialogueRNN in its emotion\\nGRU. For the highest attention, the model largely\\nfocuses on utterances nearby to the target utter-\\nance. However, the dependency on distant utter-\\nances increases with the second highest attention.\\nMoreover, it is interesting to see that the depen-\\ndency exists both towards the historical and the\\nfuture utterances, thus incentivizing utilization of\\nbi-directional models.\\n5\\nFuture Directions\\nFuture research using this dataset should focus\\non improving contextual modeling. Helping mod-\\nels reason about their decisions, exploring emo-\\ntional inﬂuences, and identifying emotion shifts\\nare promising aspects. Another direction is to use\\nvisual information available in the raw videos. Iden-\\ntifying face of the speaker in a video where multi-\\nple other persons are present is very challenging.\\nThis is the case for MELD too as it is a multi-party\\n15\\n10\\n5\\n0\\n5\\n10\\n15\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\nFrequency of correct predictions\\nΔt\\nDistance between test  \\nutterance and\\nHighest attention\\n2nd highest attention\\nFigure 4: Histogram of ∆t = distance between the target and\\nits context utterance based on emotion GRU attention scores.\\ndataset. Enhancements can be made by extracting\\nrelevant visual features through processes utilizing\\naudio-visual speaker diarization. Such procedures\\nwould enable utilizing a visual modality in the base-\\nlines. In our results, audio features do not help\\nsigniﬁcantly. Thus, we believe that it is necessary\\nto improve the feature extraction for these auxiliary\\nmodalities in order to improve the performance\\nfurther.\\nSo far, we have only used concatenation as a\\nfeature fusion approach, and showed that it out-\\nperforms the unimodal baselines by about 1-3%.\\nWe believe there is room for further improvement\\nusing other more advanced fusion methods such as\\nMARN (Zadeh et al., 2018).\\n6\\nApplications of MELD\\nMELD has multiple use-cases.\\nIt can be used\\nto train emotion classiﬁers to be further used as\\nemotional receptors in generative dialogue systems.\\nThese systems can be used to generate empathetic\\nresponses (Zhou et al., 2017). It can also be used\\nfor emotion and personality modeling of users in\\nconversations (Li et al., 2016).\\nBy being multimodal, MELD can also be used\\nto train multimodal dialogue systems. Although by\\nitself it is not large enough to train an end-to-end\\ndialogue system (Table 1), the procedures used to\\ncreate MELD can be adopted to generate a large-\\nscale corpus from any multimodal source such as\\npopular sitcoms. We deﬁne multimodal dialogue\\nsystem as a platform where the system has access\\nto the speaker’s voice and facial expressions which\\nit exploits to generate responses. Multimodal di-\\nalogue systems can be very useful for real time\\npersonal assistants such as Siri, Google Assistant\\nwhere the users can use both voice and text and\\nfacial expressions to communicate.\\n7\\nConclusion\\nIn this work, we introduced MELD, a multimodal\\nmulti-party conversational emotion recognition\\ndataset. We described the process of building this\\ndataset, and provided results obtained with strong\\nbaseline methods applied on this dataset. MELD\\ncontains raw videos, audio segments, and tran-\\nscripts for multimodal processing. Additionally,\\nwe also provide the features used in our baseline\\nexperiments. We believe this dataset will also be\\nuseful as a training corpus for both conversational\\nemotion recognition and multimodal empathetic\\nresponse generation. Building upon this dataset,\\nfuture research can explore the design of efﬁcient\\nmultimodal fusion algorithms, novel ERC frame-\\nworks, as well as the extraction of new features\\nfrom the audio, visual, and textual modalities.\\nAcknowledgments\\nThis material is based in part upon work sup-\\nported by the National Science Foundation (grant\\n#1815291), by the John Templeton Founda-\\ntion (grant #61156), and by DARPA (grant\\n#HR001117S0026-AIDA-FP-045).'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-06-05T00:42:23+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MELD Dataset.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-05T00:42:23+00:00', 'trapped': '', 'modDate': 'D:20190605004223Z', 'creationDate': 'D:20190605004223Z', 'page': 9}, page_content='References\\nMargaret M Bradley and Peter J Lang. 1994. Measur-\\ning emotion: the self-assessment manikin and the\\nsemantic differential. Journal of behavior therapy\\nand experimental psychiatry, 25(1):49–59.\\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\\nnette N Chang, Sungbok Lee, and Shrikanth S\\nNarayanan. 2008. Iemocap: Interactive emotional\\ndyadic motion capture database.\\nLanguage re-\\nsources and evaluation, 42(4):335–359.\\nSheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo,\\nLun-Wei Ku, et al. 2018. Emotionlines: An emotion\\ncorpus of multi-party conversations. arXiv preprint\\narXiv:1802.08379.\\nJunyoung Chung, C¸ aglar G¨ulc¸ehre, KyungHyun Cho,\\nand Yoshua Bengio. 2014.\\nEmpirical Evaluation\\nof Gated Recurrent Neural Networks on Sequence\\nModeling. CoRR, abs/1412.3555.\\nFlorian Eyben, Martin W¨ollmer, and Bj¨orn Schuller.\\n2010.\\nOpensmile: the munich versatile and fast\\nopen-source audio feature extractor. In Proceedings\\nof the 18th ACM international conference on Multi-\\nmedia, pages 1459–1462. ACM.\\nDevamanyu Hazarika, Soujanya Poria, Amir Zadeh,\\nErik Cambria, Louis-Philippe Morency, and Roger\\nZimmermann. 2018.\\nConversational memory net-\\nwork for emotion recognition in dyadic dialogue\\nvideos. In NAACL, volume 1, pages 2122–2132.\\nYoon Kim. 2014. Convolutional neural networks for\\nsentence classiﬁcation.\\nIn EMNLP, pages 1746–\\n1751.\\nJiwei Li, Michel Galley, Chris Brockett, Georgios Sp-\\nithourakis, Jianfeng Gao, and Bill Dolan. 2016. A\\npersona-based neural conversation model. In ACL,\\nvolume 1, pages 994–1003.\\nNavonil Majumder, Soujanya Poria, Devamanyu Haz-\\narika, Rada Mihalcea, Alexander Gelbukh, and Erik\\nCambria. 2019. DialogueRNN: An attentive RNN\\nfor emotion detection in conversations. Thirty-Third\\nAAAI Conference on Artiﬁcial Intelligence.\\nGary McKeown, Michel Valstar, Roddy Cowie, Maja\\nPantic, and Marc Schroder. 2012.\\nThe semaine\\ndatabase: Annotated multimodal records of emotion-\\nally colored conversations between a person and a\\nlimited agent. Affective Computing, IEEE Transac-\\ntions on, 3(1):5–17.\\nJeffrey Pennington, Richard Socher, and Christopher\\nManning. 2014. Glove: Global vectors for word rep-\\nresentation. In EMNLP, pages 1532–1543.\\nVer´onica P´erez-Rosas, Rada Mihalcea, and Louis-\\nPhilippe Morency. 2013.\\nUtterance-level multi-\\nmodal sentiment analysis. In ACL (1), pages 973–\\n982.\\nSoujanya Poria,\\nErik Cambria,\\nDevamanyu Haz-\\narika, Navonil Mazumder, Amir Zadeh, and Louis-\\nPhilippe Morency. 2017.\\nContext-dependent sen-\\ntiment analysis in user-generated videos.\\nIn ACL,\\npages 873–883.\\nSoujanya Poria, Navonil Majumder, Rada Mihalcea,\\nand Eduard Hovy. 2019.\\nEmotion recognition in\\nconversation: Research challenges, datasets, and re-\\ncent advances. arXiv preprint arXiv:1905.02947.\\nHannah Rashkin, Eric Michael Smith, Margaret Li,\\nand Y-Lan Boureau. 2018.\\nI know the feeling:\\nLearning to converse with empathy. arXiv preprint\\narXiv:1811.00207.\\nBj¨orn Schuller, Michel Valster, Florian Eyben, Roddy\\nCowie, and Maja Pantic. 2012. Avec 2012: the con-\\ntinuous audio/visual emotion challenge. In Proceed-\\nings of the 14th ACM international conference on\\nMultimodal interaction, pages 449–456. ACM.\\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\\nLaurent Charlin, Joelle Pineau, Aaron C Courville,\\nand Yoshua Bengio. 2017.\\nA hierarchical latent\\nvariable encoder-decoder model for generating dia-\\nlogues. In AAAI, pages 3295–3301.\\nMartin Wollmer, Felix Weninger, Timo Knaup, Bjorn\\nSchuller, Congkai Sun, Kenji Sagae, and Louis-\\nPhilippe Morency. 2013.\\nYoutube movie reviews:\\nSentiment analysis in an audio-visual context. IEEE\\nIntelligent Systems, 28(3):46–53.\\nAmir Zadeh, Tadas Baltruˇsaitis, and Louis-Philippe\\nMorency. 2016a.\\nDeep constrained local mod-\\nels for facial landmark detection.\\narXiv preprint\\narXiv:1611.08657.\\nAmir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cam-\\nbria, and Louis-Philippe Morency. 2018.\\nMulti-\\nmodal language analysis in the wild: Cmu-mosei\\ndataset and interpretable dynamic fusion graph. In\\nACL, volume 1, pages 2236–2246.\\nAmir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\\nPhilippe Morency. 2016b. Multimodal sentiment in-\\ntensity analysis in videos: Facial gestures and verbal\\nmessages. IEEE Intelligent Systems, 31(6):82–88.\\nHao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan\\nZhu, and Bing Liu. 2017.\\nEmotional chatting\\nmachine: Emotional conversation generation with\\ninternal and external memory.\\narXiv preprint\\narXiv:1704.01074.'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 0}, page_content='Journal Pre-proofs\\nMASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment\\nAnalysis\\nJie Zhou, Jiabao Zhao, Jimmy Xiangji Huang, Qinmin Vivian Hu, Liang He\\nPII:\\nS0925-2312(21)00793-1\\nDOI:\\nhttps://doi.org/10.1016/j.neucom.2021.05.040\\nReference:\\nNEUCOM 23870\\nTo appear in:\\nNeurocomputing\\nReceived Date:\\n1 May 2020\\nRevised Date:\\n12 May 2021\\nAccepted Date:\\n13 May 2021\\nPlease cite this article as: J. Zhou, J. Zhao, J. Xiangji Huang, Q. Vivian Hu, L. He, MASAD: A Large-Scale\\nDataset for Multimodal Aspect-Based Sentiment Analysis, Neurocomputing (2021), doi: https://doi.org/10.1016/\\nj.neucom.2021.05.040\\nThis is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover\\npage and metadata, and formatting for readability, but it is not yet the definitive version of record. This version\\nwill undergo additional copyediting, typesetting and review before it is published in its final form, but we are\\nproviding this version to give early visibility of the article. Please note that, during the production process, errors\\nmay be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.\\n© 2021 Elsevier B.V. All rights reserved.'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 1}, page_content='MASAD: A Large-Scale Dataset for Multimodal Aspect-Based\\nSentiment Analysis\\nJie Zhoua, Jiabao Zhaoa, Jimmy Xiangji Huangb, Qinmin Vivian Huc, Liang Hea\\naSchool of Computer Science and Technology, East China Normal University,\\nShanghai 200241, China.\\nbInformation Retrieval and Knowledge Management Research Lab, York University,\\nToronto, Ontario M3J 1P3, Canada.\\ncThe School of Computer Science, Ryerson University,\\nToronto, Ontario M5B 2K3, Canada.\\nAbstract\\nAspect-based sentiment analysis has obtained great success in recent years. Most of the existing\\nwork focuses on determining the sentiment polarity of the given aspect according to the given\\ntext, while little attention has been paid to the visual information as well as multimodality con-\\ntent for aspect-based sentiment analysis. Multimodal content is becoming increasingly popular\\nin mainstream online social platforms and can help better extract user sentiments toward a given\\naspect. There are only few studies focusing on this new task: Multimodal Aspect-based Sen-\\ntiment Analysis (MASA), which performs aspect-based sentiment analysis by integrating both\\ntexts and images. In this paper, we propose a mutimodal interaction model for MASA to learn\\nthe relationship among the text, image and aspect via interaction layers and adversarial training.\\nAdditionally, we build a new large-scale dataset for this task, named MASAD, which involves\\nseven domains and 57 aspect categories with 38k image-text pairs. Extensive experiments have\\nbeen conducted on the proposed dataset to provide several baselines for this task. Though our\\nmodels obtain signiﬁcant improvement for this task, empirical results show that MASA is more\\nchallenging than textual aspect-based sentiment analysis, which indicates that MASA remains a\\nchallenging open problem and requires further efforts.\\nKeywords: Sentiment Analysis, Multimodal, Aspect-based Sentiment Analysis, Deep Learning.\\n1. Introduction and Motivation\\nNowadays, as a major social media platform for expressing experiences and sharing the opin-\\nion about service, products, and travel, the Internet provides extensive contents of users’ opinion\\nand sentiment about rich topics [1]. This information is expressed in multiple formats, such as\\nreviews, tags, browser behavior, and shared media objects. The analysis of such information\\nplays an essential role in the area of opinion mining, affective computing, and sentiment anal-\\nysis. It can predict human decision making and enables some applications like public opinion\\nEmail address: jzhou@ica.stc.sh.cn (Jie Zhou)\\nPreprint submitted to Neurocomputing\\nMay 12, 2021'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 2}, page_content=\"5XQQLQJ\\x03DQG\\x03FKDVLQJ\\x03EDOOV\\x03KDV\\x03EHFRPH\\x03\\nMXVW\\x03D\\x03ZHHNHQG\\x03DFWLYLW\\\\\\x03ODWHO\\\\\\x11\\x03+H\\nV\\x03WKH\\x03\\nEHVW\\x03GRJ\\x03HYHU\\x11\\x03,\\x03ORYH\\x03KLP\\x03VR\\x03PXFK\\x11\\n$\\x03YHU\\\\\\x03PXFN\\\\\\x03SXS\\x03\\x04\\n$\\x03IURQW\\x03ORDGHU\\x03EDUUHOV\\x03GRZQ\\x03RQ\\x03D\\x03\\nFUXVKHG\\x03FDU\\x03LQ\\x03ZLQGVRU\\x03IULGD\\\\\\x03PRUQLQJ\\x11\\n\\x14\\x1c\\x17\\x13\\x03SDFNDUG\\x03RQH\\x03WHQ\\x03FRQYHUWLEOH\\x03\\nFRXS\\n$VSHFW\\x1d\\x03&DU\\n$VSHFW\\x1d\\x03'RJ\\n6HQWLPHQW\\x1d\\x033RVLWLYH\\n6HQWLPHQW\\x1d\\x031HJDWLYH\\n6HQWLPHQW\\x1d\\x031HJDWLYH\\n6HQWLPHQW\\x1d\\x033RVLWLYH\\nFigure 1: An example of multimodal aspect-based sentiment analysis. This task includes two subtasks: Aspect Extraction\\n(AE) and Aspect Polarity Prediction (AP). First, we extract the aspect (e.g., car or dog) in the image-text pair. Then, we\\njudge the sentiment (e.g., positive or negative) with respect to the extracted aspect.\\nanalysis, brand monitoring, and political voting forecast [2]. As a fundamental subtask of senti-\\nment analysis, aspect-based sentiment analysis provides valuable insights to both consumers and\\nbusinesses. It helps companies to measure satisfaction and improve their products or services\\n[3, 4]. So far, the computational analysis of aspect-based sentiment mostly concentrates on opin-\\nionated texts (e.g., comments, tweets, and reviews) [3, 5]. However, limited efforts have been\\nconducted to analyze sentiments from visual content such as images, which is becoming a key\\nmedia type on the web. Recently, social media users are increasingly using additional images\\nto express their experiences and opinion. Such rich-source textual and visual content can help\\nbetter extract user sentiments toward different aspects.\\nMotivated by the needs to use large-scale social multimodal content for aspect-based senti-\\nment analysis, we focus on the new task Multimodal Aspect-based Sentiment Analysis (MASA),\\nwhere the input is in the form of an image plus some text that describes the input image. MASA\\nconsists of two subtasks: aspect extraction (AE) and aspect polarity prediction (AP). The goal of\\nAE is to extract the aspect of the samples via the image-text pair and the AP aims to predict the\\nsentiment polarity with respect to the aspect. Taking Figure 1 as an example, in the above one,\\ntwo samples both contain a car and the sentiment polarities of them are negative and positive\\nsince the left car is crushed while the right car is new and cool with a clear sky. In the blow one,\\nthe sentiments of the aspect “dog” are positive and negative, respectively. We ﬁrst need to extract\\nthe aspects (e.g., car, dog and so on) from the image-text pair and then we need to determine their\\n2\"),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 3}, page_content='Year\\n#Samples\\n#Aspects\\n#Domain\\nText\\nImage\\nMitchellEN [6]\\n2013\\n10,000\\n10\\n1\\n✓\\n✕\\nSemEval-2014 [5]\\n2014\\n7,686\\n5\\n2\\n✓\\n✕\\nSemEval-2015 [7]\\n2015\\n4,766\\n26\\n3\\n✓\\n✕\\nSemEval-2016EN [8]\\n2016\\n5,984\\n28\\n2\\n✓\\n✕\\nSentiHood [9]\\n2016\\n5,215\\n2\\n1\\n✓\\n✕\\nMulti-ZOL [10]\\n2019\\n5,228\\n40\\n1\\n✓\\n✓\\nTwitter-15/17 [11]\\n2019\\n3,179/3,562\\n-\\n1\\n✓\\n✓\\nMASAD (our)\\n2019\\n38,532\\n57\\n7\\n✓\\n✓\\nTable 1: The comparison of the public aspect-level sentiment analysis.\\nsentiment polarities (e.g., positive or negative) according to the image and text.\\nTo better explore the MASA task, we build and release a new large-scale dataset for mul-\\ntimodal aspect-level sentiment analysis, named MASAD. Table 1 shows the comparison of the\\npublic aspect-level sentiment analysis with our proposed MASAD. One problem of these ex-\\nisting datasets is that most of these datasets focus on the textual information, while the visual\\ninformation is ignored. Another problem is that the limited number of domains does not guar-\\nantee the generalization capability required in real applications. Besides, the limited number of\\ndata size largely limits the performance of this task. To address previous drawbacks, we propose\\nthis MASAD dataset, which consists of 38k samples within seven domains (e.g., Food, Goods,\\nBuildings, Animal, Human, Plant, and Scenery) and 57 aspects. Our dataset is larger than the\\nexisting available datasets of this ﬁeld and is the only one that contains both text and image for\\nthis task.\\nMoreover, we propose a multimodal interaction model to assess the challenges of MASA\\nand provide strong baselines for this task. To be speciﬁc, we ﬁrst use image and text encoders to\\nlearn the representation of the image-based and text-based on state-of-the-art visual and textual\\ntechnology. Then we design a multimodal interaction layer to learn the relationships among\\nthe aspect, image, and text. Effectively fusing this diverse textual and visual information is\\nnon-trivial and poses several challenges to the underlying problem. Our model combines the\\nstrengths of the text and the image representations by extracting interactive information between\\nthem. Furthermore, we design an adversarial training strategy to align text and image features\\ninto one common space. A series of experiments also show the great advantages of our models.\\nThe main contributions of this work can be summarized as follows.\\n• We propose a multimodal interaction model for the new multimodal aspect-based senti-\\nment analysis (MASA) task. Different from the existing aspect-based sentiment analysis\\ntask, which judges the sentiment polarity of the aspect based on textual information, this\\nnew task infers the sentiment for the given aspect based on both texts and images. Our\\nmodel can learn the interactive relation among the aspect, text, and image effectively.\\n• Since the multimodal data (e.g., text and image) is not in a common space, we design\\nan adversarial training to align the feature representations of image and text into a shared\\nspace.\\n• We present and release a large-scale dataset for multimodal aspect-based sentiment anal-\\nysis, namely MASAD, which involves 57 categories in seven domains and contains 38k\\nsamples with image-text pair. This dataset provides a new perspective for aspect-based\\nsentiment classiﬁcation and a new bench-mark for MASA.\\n3'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 4}, page_content='• The extensive experimental results indicate the great advantage of our model. However,\\ndetailed analysis on the experiments shows that MASA is more challenging than textual\\naspect-based sentiment analysis and intensive efforts are needed to improve the perfor-\\nmance.\\n2. Related Work\\nWe divide the relevant work into two groups: aspect-based sentiment analysis and multi-\\nmodal sentiment analysis. There are a large number of studies on the topic of aspect-based sen-\\ntiment classiﬁcation [4, 5, 7, 8, 12] and multimodal sentiment analysis [2, 13]. Here we mainly\\nreview the work that is most related to our research.\\n2.1. Aspect-based Sentiment Analysis\\nAspect-based sentiment analysis plays a signiﬁcant role in sentiment analysis [3, 14]. Early\\nwork mainly used traditional machine learning algorithms, which highly depended on the qual-\\nity of extensive hand-craft features [15, 16]. With the advances of neural networks, various deep\\nlearning models are of growing interest in aspect-based sentiment analysis for their ability to\\nlearn the representation of text automatically [17, 18, 19, 20, 21, 22, 23, 24]. Attention mech-\\nanisms [25] have been adopted to capture the important parts with respect to the given aspect\\n[26, 27, 28, 29, 30, 31]. Sentiment commonsense knowledge are integrated into this task to\\nimprove the performance [32, 33]. In addition, pre-trained models (such as BERT, ELMo) are\\nutilized for aspect-based sentiment analysis [34, 35].\\nIn recent years, aspect-based sentiment analysis task has been organized by some workshops\\nand conferences [5, 7, 8]. These competitions provided training datasets and the opportunity for a\\ncomparison of different methods on the test set. Dong et al. [36] presented a manually annotated\\ndataset for target-dependent twitter sentiment analysis. And Fan et al. [37] labeled the opinion\\nwords with respect to the given aspect for these datasets. SentiHood [9] was a benchmark dataset\\nthat was annotated for the targeted aspect-based sentiment analysis task in the domain of urban\\nneighborhoods. Michell et al. [6] released a dataset1 that includes about 30k Spanish tweets\\nand 10k English tweets labeled for named entities with the sentiments. Recently, Jiang et al.\\n[38] proposed a new datasets, where the samples contain more than one aspects with different\\nsentiments.\\nDespite these advances of methods and datasets in aspect-based sentiment analysis, almost\\nall of them focus on how to perform classiﬁcation based on the textual context, while the visual\\ninformation as well as multimodal content is ignored. More recently, Xu et al. and Yu et al.\\n[10, 11] proposed the task of multimodal aspect-level sentiment analysis. Different from their\\nresearches, we aim to explore the effectiveness of the interaction among the aspect, text and\\nimage in this paper. We propose a multimodal interaction model for this new task. We also\\npresent and release a large-scale dataset for this new task. The dataset we provided facilitates\\nstudying MASA relative to conceptually different and diverse aspects.\\n1http://www.m-mitchell.com/code/index.html\\n4'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 5}, page_content='2.2. Multimodal Sentiment Analysis\\nMultimodal sentiment analysis has recently attached attention due to the tremendous growth\\nof many social media platforms such as Twitter, Flickr, Facebook, and so on [39, 40]. It depends\\non the information obtained from more than one modality (i.e., text and image) for the analysis.\\nMultimodal sentiment analysis has become a very challenging problem for the researchers [41].\\nA survey of the literature showed that multimodal sentiment analysis is relatively a new area com-\\npared to textual sentiment analysis [40]. Ghosal et al. [42] developed an RNN-based multimodal\\nattention model to utilize the contextual information for utterance-level sentiment analysis. Poria\\net al. [13] introduced an LSTM-based method that leveraged the contextual information to cap-\\nture the inter-dependencies between the utterances. In another work, a user opinion based model\\nis proposed to combine the three modality inputs (i.e., text, visual and acoustic) by applying a\\nmulti-kernel learning based method [40]. Zadeh et al. [43] developed multi-attention blocks\\n(MAB) to capture information across text, visual and acoustic.\\nHowever, almost all of them focus on the multimodal sentiment analysis for the whole sam-\\nple rather than aspect level. Xu et al. [10] integrated the image information into the attention\\nmechanism for aspect-based multi-modal sentiment analysis task. However, the interaction and\\nalignment between the text and image information are ignored by this work. In this paper, we\\npropose to learn the interaction between the aspect and multi-modal, and the interaction among\\nmulti-modal information. Moreover, we design an adversarial training strategy to align the text\\nand image representations into the same space.\\n3. Task Description\\nIn this paper, we focus on the new task, namely multimodal aspect-level sentiment analysis\\n(MASA), which aims to perform the aspect-based sentiment analysis [3, 5] based the textual\\nand visual information. This task consists of two subtasks: Aspect Extraction (AE) and Aspect\\nPolarity Prediction (AP). The goal of AE is to extract the aspect (e.g., dog, car) in the text-image\\npair. Then AP aims to judge the sentiment polarity for the given aspect via the text-image pair.\\nFormally, the deﬁnitions of these two subtasks are given as follows.\\nAspect Extraction (AE): Given a predeﬁned set of aspects A and a text-image pair P(T, I),\\nthe aim of this task is to identify the aspect a ∈A expressed in the text-image pair P(T, I). The\\nnumber of the aspects is |A|. The text T = {w1, w2, ..., wn} consists of n words. For example,\\nin Figure 1, the above two samples contain aspect “car” and the below samples contain aspect\\n“dog”.\\nAspect Polarity Prediction (AP): For this subtask, aspect a for each text and image pair\\nP(T, I) is provided. The goal is to determine the sentiment polarity c ∈{N, P} of the aspect a\\ndiscussed in each text and image pair, where N and P denote the “negative” and “positive” sen-\\ntiment polarities respectively. For instance, the user expresses negative and positive sentiments\\nover aspect “car” in the above samples, respectively (Figure 1). And the sentiments for aspect\\n“dog” in the below samples are positive and negative, respectively.\\n5'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 6}, page_content=\",W\\x03ZDV\\x03D\\x03FRORUIXO\\x03DXWXPQ\\x03D\\x03IHZ\\x03\\n\\\\HDUV\\x03DJR\\x03DURXQG\\x03RXU\\x03DUHD\\x03ZKLFK\\x03\\nSURYLGHG\\x03D\\x03IHZ\\x03RSSRUWXQLWLHV\\x03IRU\\x03\\nVRPH\\x03FRORUIXO\\x03VKRWV\\x11\\n$VVRUWHG\\x03FRORUV\\n,\\nP\\x03QRW\\x03D\\x03JDUGHQHU\\x03DQG\\x03ILQG\\x03\\nWKHVH\\x03SODQWV\\x03HYHU\\\\ZKHUH\\x0f\\x03,\\x03\\nORRNHG\\x03IRU\\x03P\\\\\\x03IDYRULWH\\x03RQHV\\x03\\nDQG\\x03WRRN\\x03D\\x03IHZ\\x03SLFWXUHV\\x11\\x03\\n:H\\nUH\\x03VXSSRVHG\\x03WR\\x03EH\\x03XQGHU\\x03\\nZLOGILUH\\x03ZDWFK\\x11\\x03,\\x03KRSH\\x03ZH\\x03\\nZRQ\\nW\\x03KDYH\\x03WR\\x03JR\\x03WKURXJK\\x03\\nWKDW\\x03DJDLQ\\x11\\n'ULHG\\x03OHDI\\n7UDQVOXFHQW\\x03GU\\\\LQJ\\x03OHDYHV\\n7KH\\x03PDJLFDO\\x03OLJKW\\x03RI\\x03VXQVHW\\x03\\nEULQJV\\x03OLIH\\x03DJDLQ\\x03WR\\x03GHDG\\x03\\nOHDYHV\\x03GULHG\\x03RQ\\x03WKH\\x03EUDQFK\\x11\\x03\\nGU\\\\\\x03OHDI\\x03RQ\\x03FUXVKHG\\x03JUDYHO\\n$VSHFW\\x1d\\x03/HDYHV\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x036HQWLPHQW\\x1d\\x031HJDWLYH\\n$VSHFW\\x1d\\x03/HDYHV\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x036HQWLPHQW\\x1d\\x033RVLWLYH\\n(a) Examples of aspect “Leaves” with different sentiments.\\n)RU\\x03WKH\\x03EOXH\\x03VNLHV\\x03DQG\\x03WKH\\x03\\nVPLOH\\x03MXVW\\x03RQH\\x03PRUH\\x03\\x10\\x03QRW\\x03\\nDPELWLRXV\\x03EXW\\x03KDSS\\\\\\x03\\x1e\\x0c\\x03\\nKDSS\\\\\\x03IDFH\\x03\\x10\\x03$QD\\x103DXOD\\x03DQG\\x03\\n/HLOD\\x03%DUURV\\x03IURP\\x03%UD]LO\\n+DSS\\\\\\x03+DSS\\\\\\x03)DFH\\x11\\x11\\x11\\x11\\n(OOLVLI\\x031R\\x03PDNH\\x10XS\\x03QHHGHG\\x03\\nIRU\\x03WKLV\\x03EHDXW\\\\\\x11\\n3RZHUIXO\\x03/RZ\\x03.H\\\\\\x036KRW\\x03RI\\x03D\\x03\\n<RXQJ\\x03&KLOG\\x03/RRNLQJ\\x036DG\\n<RX\\x03ZLWK\\x03WKH\\x03VDG\\x03IDFH\\n'RQ\\nW\\x03EH\\x03VFDUHG\\x03WR\\x03ZDON\\x03\\nDORQH\\x11\\x11\\x11\\n6KH\\x03VFDUHV\\x03PH\\x0f\\x03PDNHV\\x03PH\\x03\\nZRUULHG\\x03EXW\\x03DW\\x03HQG\\x03RI\\x03LW\\x03DOO\\x03VKH\\x03\\nJLYHV\\x03PH\\x03LV\\x03SULGH\\x03DQG\\x03MR\\\\\\x11\\x11\\x11\\x11\\n$VSHFW\\x1d\\x03)DFH\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x036HQWLPHQW\\x1d\\x031HJDWLYH\\n$VSHFW\\x1d\\x03)DFH\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x036HQWLPHQW\\x1d\\x033RVLWLYH\\n(b) Examples of aspect “Face” with different sentiments.\\nFigure 2: Several examples of two typical aspects with different sentiment polarities in our MASAD dataset.\\n6\"),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 7}, page_content='Train\\nTest\\nTotal\\nPositive\\nNegative\\nTotal\\nPositive\\nNegative\\nTotal\\nPositive\\nNegative\\nTotal\\nFood\\n2360\\n433\\n2793\\n592\\n109\\n701\\n2952\\n542\\n3494\\nGoods\\n2671\\n1674\\n4345\\n743\\n512\\n1255\\n3414\\n2186\\n5600\\nBuildings\\n1450\\n970\\n2420\\n367\\n245\\n612\\n1817\\n1215\\n3032\\nAnimal\\n3023\\n2208\\n5231\\n1126\\n670\\n1796\\n4149\\n2878\\n7027\\nHuman\\n1999\\n1838\\n3837\\n503\\n464\\n967\\n2502\\n2302\\n4804\\nPlant\\n2819\\n2607\\n5426\\n1269\\n947\\n2216\\n4088\\n3554\\n7642\\nScenery\\n3600\\n1936\\n5536\\n907\\n490\\n1397\\n4507\\n2426\\n6933\\nTotal\\n17922\\n11666\\n29588\\n5507\\n3437\\n8944\\n23429\\n15103\\n38532\\nTable 2: Statistical information of our proposed MASAD dataset.\\n4. Dataset\\n4.1. Data Collection\\nWe collect and label our dataset based on the publicly available Visual Sentiment Ontology\\n(VSO) dataset 2 [44] and Multilingual Visual Sentiment Ontology (MVSO) dataset3 [2], which\\nare the largest available datasets for visual sentiment analysis. VSO dataset was collected from\\nFlickr4. We select Flickr since there is some existing work of multimedia research using it\\n[44, 2], and to be speciﬁc, Jin et al. [45] presented that Flickr has two advantages popularity and\\navailability for utilizing the “wisdom of the social multimedia”. However, the VSO dataset does\\nnot provide the text and sentiment polarities towards the given objects. Moreover, it contains a\\nlot of noise and many images in it have no clear sentiments. We select the samples which can\\nexpress obvious sentiments (about 38k samples) from parts of VSO dataset (about 120k samples)\\nand summarize them into seven domains. Then we crawl the descriptions (text information)\\nof images and clean the data for each aspect to ensure the high-quality of each sample with\\nimage-text pair. To obtain the given aspect’s sentiment polarity, we label the datasets through\\ncrowdsourcing with three annotators. In particular, we ask three workers to label the results via\\nAli Crowdsourcing Platform5. We present the image-text pair to the annotators and ask them to\\nlabel the sentiment polarities towards the given aspect. Then, we obtain the label through voting.\\nWe calculate the Krippendorff’s alpha coefﬁcient [46] to measure the inter-annotator agreement\\nof the manual annotation. The value is 0.850, which indicates the high agreement of the labeled\\ndata. We updated the dataset in github with a readme6.\\n4.2. Dataset Description\\nWith the widespread application of aspect-based sentiment analysis in e-commerce, the datasets\\nprovided by SemEval [5, 7, 8] are widely used in this ﬁeld. Most of the existing aspect-based\\nsentiment analysis datasets only contain text, while the visual information is ignored. To address\\nthis problem, we present and release a large-scale multimodal aspect-based sentiment analysis\\ndataset (MASAD), which contains both texts and images. The main statistics of our proposed\\n2https://visual-sentiment-ontology.appspot.com/\\n3http://mvso.cs.columbia.edu/\\n4https://www.ﬂickr.com/\\n5https://newjob.taobao.com/#!/\\n6https://github.com/12190143/MASAD\\n7'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 8}, page_content='MASAD is provided in Table 2. This dataset consists of seven domains, including food, goods,\\nbuildings, animal, human, plant, and scenery. Each domain contains several aspects with posi-\\ntive and negative samples. There are a total of 57 predeﬁned aspects, such as dog, cat, car, etc.\\nAs shown in Figure 2, several typical examples of two aspects “Leaves” and “Face” are given.\\nNotably, all the samples have an obvious sentiment polarity and the image and text are both\\ncritical for the classiﬁcation. In particular, our dataset consists of 38,532 text and image pairs.\\nThe number of samples with positive and negative sentiments is 23,429 and 15,103, respectively.\\nThe dataset is split into two sets: training and testing. We have 29,588 and 8,944 samples for its\\ncorresponding set.\\n5. Methods\\nIn this section, we aim to ﬁnd a straightforward architecture that provides good performance\\nfor the MASA task. In particular, we propose a multimodal aspect extraction (MMAE) method\\nand a multimodal aspect polarity prediction (MMAP) method for aspect extraction (AE) and\\naspect polarity prediction (AP) respectively. The frameworks of these models are shown in\\nFigure 3. We ﬁrst use text encoder and image encoder to extract the text and image representation\\nvia the state-of-the-art textual and visual methods. We also propose an aspect embedding the\\nobtain the representation of the aspect. To learn the relationships among the text, image, and\\naspect, we design three interaction mechanisms to learn the interaction between the text and\\nimage, the text and aspect, and the image and aspect. Additionally, we design an adversarial\\ntraining strategy to align the feature representation of text and image into a common space. The\\ndetails are showing in the following sections.\\n5.1. Multimodal Aspect Extraction (MMAE)\\nIn this section, we propose our multimodal aspect extraction (MMAE) method for AE. Given\\nthe text-image pair {T, I}, the goal of this task is to identify the aspect expressed in it. The frame-\\nwork of the MMAE model is shown in Figure 3. Our model consists of four parts: Image En-\\ncoder, Text Encoder, Multimodal Interaction Layer, and Aspect Classiﬁcation. Image Encoder\\nand Text Encoder are used to extract the feature representations of image and text. Then we de-\\nvelop a multimodal interaction layer to learn the interaction between the text and image. Finally,\\nthe text-image pair representation is fed to an aspect classiﬁcation layer. Details are presented as\\nfollows.\\nText Encoder. Each word wi in the text is mapped into a low-dimensional continuous vector\\nspace xi ∈Rdw, which is calculated by looking up the word embedding Ew ∈Rdw×|V|. Here\\ndw denotes the dimension of the word embedding and |V| is the size of vocabulary. We use the\\npre-trained 300-dimensional GloVe [47] embedding to represent words.\\nBi-directional long short-term memory (Bi-LSTM) [48] model is employed to accumulate\\nthe context information from word embedding. The Bi-LSTM contains a forward −−−−−→\\nLS T M and a\\nbackward ←−−−−−\\nLS T M which read the text from w1 to wn and wn to w1 respectively. The contextualized\\nrepresentation for each word is computed as follows:\\n−→hi = −−−−−→\\nLS T M(xi), i ∈[1, n]\\n(1)\\n←−hi = ←−−−−−\\nLS T M(xi), i ∈[n, 1]\\n(2)\\n8'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 9}, page_content=\"'LVFULPLQDWRU\\n0XOWLPRGDO\\x03\\n,QWHUDFWLRQ\\x03/D\\\\HU\\n7$\\n,$\\n,7\\n&ODVVLILHU\\n6HQWLPHQW\\x1d3RVLWLYH\\n&ODVVLILHU\\n$VSHFW\\x1d'RJ\\n00$(\\n00$3\\n0RGDO\\n%L/670\\n%L/670\\n%L/670\\n5XQQLQJ\\nDQG\\nFKDVLQJ\\nEDOOV\\n«\\n,\\nORYH\\nKLP\\n%L/670\\n«\\n\\x18\\x13\\x10OD\\\\HU\\n5HVLGXDO\\x03\\n1HWZRUN\\n%L/670\\n%L/670\\n%L/670\\n,PDJH\\n7H[W\\n7H[W\\x03(QFRGHU\\n,PDJH\\x03(QFRGHU\\n'RJ\\n$VSHFW\\x03\\n(PEHGGLQJ\\n$GYHUVDULDO\\x03\\n7UDLQLQJ\\nFigure 3: The framework of Multimodal Aspect Extraction (MMAE) and Multimodal Aspect Polarity Polarity (MMAP).\\nFor MMAE, we ﬁrst learn the representation of the image and text via image and text encoders. To align the representa-\\ntions of the text and image in the same space, we design an adversarial training strategy by predicting the modal based\\non the input (text representation or image representation). Then, we design a multimodal interaction layer to learn the\\nrelationships between the text and image. Finally, the interactive representation of the image-text pair is fed to aspect\\nclassiﬁer for predicting the aspect. Similar to MMAE, for MMAP, we utilize image and text encoders to extract the\\nrepresentation of the image and text respectively. In addition, we design an aspect embedding to learn the representation\\nof the aspect. Then, we adopt three interaction parts to learning the relationships between the image and aspect, the\\naspect and text, and the image and text. Finally, we concatenate these three interactive representations and feed them into\\nthe sentiment classiﬁer to predict the sentiment for the given aspect.\\nFinally, the last hidden state of the Bi-LSTM is used as the feature representation of the text\\nRT:\\nRT = [−→\\nhn; ←−\\nhn]\\n(3)\\nwhere RT ∈RdT is obtained by concatenating the hidden states −→hi ∈Rdh and ←−hi ∈Rdh. And ;\\ndonates the concatenate operator, dh is the dimension of the hidden states and dT = 2dh is the\\ndimension of the text features.\\nImage Encoder. In this part, we adopt an Image Encoder to learn the feature representation of\\nthe image. Recently, the residual network (ResNet) [49] achieved great success on a wide range\\nof vision tasks. Gajarla and Gupta [50] applied pre-trained ResNet for visual sentiment analysis\\nand achieved good performance. More speciﬁcally, we adopt the ResNet-50 [49] as the image\\nencoder to extract the feature representation of the image. The ResNet-50 is a 50 layer Residual\\nNetwork, which obtains excellent results by extremely deep residual nets. Note that we use the\\nparameters pre-trained on the ImageNet7 as the initialization parameters and ﬁne-tune the model\\n7https://pytorch.org/docs/stable/torchvision/models.html\\n9\"),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 10}, page_content='on our dataset. Finally, the feature representation of the image I can be calculated as:\\nRI = ImageEncoder(I)\\n(4)\\nwhere RI ∈RdI is the feature representation of the image, dI is the dimension of the image\\nfeatures and ImageEncoder is a ResNet-50 model here.\\nMultimodal Interaction Layer. The main challenge in multimodal sentiment analysis lies in\\nproperly using the information extracted from multiple modalities, such as text and image. Al-\\nthough it is often argued that the modalities are always beneﬁcial for enhanced performance, it\\nis notable that not the relationships among the modalities are vital. To obtain a better representa-\\ntion of the text-image pair, we design a multimodal interaction layer to learn the inter-modality\\nrelationship between the text and image. As shown in Figure 3, we propose an InterIT to learn\\nthe interaction between the image and text.\\nRIT = InterIT = RI × A × RT + b\\n(5)\\nwhere A ∈RdT ×dIT ×dI is the learnable weights and b is the bias. RIT ∈RdIT is the interactive\\nfeature representation of text-image pair and dIT is dimension of RIT.\\nAspect Classiﬁcation. The ﬁnal step is to feed the representation of text-image pair RIT to a\\nmultilayer perceptron (MLP) and softmax layer for aspect distribution prediction:\\npAE = Softmax(WAERIT + bAE)\\n(6)\\nwhere WAE ∈Rdh×|A| and bAE ∈R|A| are the learnable parameters, A is the number of aspect\\nclasses.\\nThe loss function is deﬁned by the cross-entropy of the predicted and true label distributions\\nfor training:\\nLAE = −1\\nN\\nN\\nX\\ni\\nlog(pAE\\na )\\n(7)\\nwhere N is the number of instances in the dataset, a is the true class of the ith instance, and pAE\\nk\\nindicates the kth value of the vector pAE.\\n5.2. Adversarial Training for Alignment\\nAlthough interaction representation of text and image are learned by the multimodal interac-\\ntion layer, there is no guarantee that features of image and text are in the same space. Inspired by\\n[51], we utilize the adversarial training to align the space of image and text into the same space\\n(Figure 3). In particular, we adopt a discriminator to judge the representation comes from text\\n(yT) or image (yI). It minimizes the cross-entropy of the predicted label distribution p(yR|R) and\\nthe true label (yT or yI):\\nmin\\nθd Ld(θd) = −1\\nN\\nN\\nX\\ni=1\\nlog(p(yR = yI|RI)) + log(p(yR = yT|RT))\\n(8)\\nwhere yI and yT represent the label of the feature representation belong to image or text, respec-\\ntively. θd is the parameters of discriminator. Speciﬁcally, we use an MLP and a softmax layer as\\nthe modal discriminator.\\np(yR|R) = Softmax(WdR + bd)\\n(9)\\n10'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 11}, page_content='where R is the vector representations of the text/image (RT/RI) and θd = {Wd, bd}.\\nOur generator is quite different from the traditional generator in multi-criteria tasks [51]. To\\nbe speciﬁc, the traditional generator always plays against the discriminator to obtain the invariant\\nrepresentations of text and image that the discriminator can not distinguish. In particular, the\\ngenerator (image encoder and text encoder) plays an adversarial game with the discriminator,\\nmaking it difﬁcult to discriminate the feature label (text (yT) or image (yI)). Thus, our generator\\naims to maximize the feature label prediction of the generated feature according to text or image\\nas follows:\\nmax\\nθg Lg(θg) = −1\\nN\\nN\\nX\\ni=1\\nlog(p(yR = yI|RI)) + log(p(yR = yT|RT))\\n(10)\\nwhere θg is the parameters of the generator (e.g., image encoder and text encoder).\\n5.3. Multimodal Aspect Polarity Prediction (MMAP)\\nWe develop a multimodal aspect polarity prediction (MMAP) model for task AP, which aims\\nto determine the sentiment polarity of the given aspect in the text-aspect pair. Figure 3 shows the\\nframework of our MMAP model. The same as MMAE, we use Image Encoder and Text Encoder\\nto obtain the image’s feature representation RI via Equation 3 and text’s feature representation\\nRT via Equation 4 respectively. Also, we design an Aspect Embedding to learn the representation\\nof the given aspect. For the Interaction Layer, we propose three strategies to model the interac-\\ntion between image and aspect, image and text, and aspect and text to capture the relationships\\nbetween them more effectively. We give details in the following sections.\\nAspect Representation. To make full use of aspect information, we propose to learn an embed-\\nding vector for each aspect. Inspired by [26], we obtain the corresponding aspect representation\\nRA by looking up an aspect embedding matrix EA ∈RdA, which is initialized by GloVe, and\\nupdated during the training process. dA is the dimension of the aspect embedding.\\nMultimodal Interaction Layer. How to model the relationships among the text, image, and aspect\\nplays a signiﬁcant role in this task. Thus, we design our multimodal interaction layer, which\\nconsists of three parts, namely InterIA, InterIT, InterAT, which model the interaction between\\nimage and aspect, image and text, and aspect and text, respectively.\\nRIA = InterIA = RI × AIA × RA + bIA\\n(11)\\nRIT = InterIT = RI × AIT × RT + bIT\\n(12)\\nRAT = InterAT = RA × AAT × RT + bAT\\n(13)\\nwhere AIA ∈RdT ×dIA×dA, AIT ∈RdI×dIT ×dT and AAT ∈RdA×dAT ×dT are the learnable weights and\\nbIA, bIT and bAT are the bias. RIAIA, RIT IT and RAT AT are the interactive feature representation\\nof image and aspect, image and text, and aspect and text respectively. And dIA, dIT and dAT are\\nthe dimension of RIA, RIT and RAT respectively.\\nThe ﬁnal representation of the text-image pair towards the aspect is calculated as follows:\\nRITA = [RIT; RIA; RAT]\\n(14)\\nwhere ; denotes the concatenate operator, RITARdIT +dIA+dAT indicates the ﬁnal representation of\\nthe aspect in the text-image pair.\\n11'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 12}, page_content='Noun\\nAbbreviation\\nAspect Extraction\\nAE\\nAspect Polarity Prediction\\nAP\\nMultimodal Aspect Extraction\\nMMAE\\nMultimodal Aspect Polarity Predition\\nMMAP\\nMultimodal Asepct-based Sentiment Analysis\\nMASA\\nMultimodal Asepct-based Sentiment Analysis Dataset\\nMASAD\\nTable 3: The abbreviation of the speciﬁc noun.\\nSentiment Classiﬁcation. The ﬁnal step is to feed RITA to a softmax layer for sentiment distribu-\\ntion prediction:\\npAP = Softmax(WAPRITA + bAP)\\n(15)\\nwhere WAP ∈Rdh×|C| and bAP ∈R|C| are the learnable parameters, C is the number of sentiment\\nclasses.\\nThe loss function is deﬁned by the cross-entropy of the predicted and true label distributions\\nfor training:\\nLAP = −1\\nN\\nN\\nX\\ni\\nlog(pAP\\nc )\\n(16)\\nwhere N is the number of instances in the dataset, c is the true class of the ith instance, and pAP\\nk\\nindicates the kth value of the vector pAP.\\n5.4. Joint Training\\nFinally, we combine the our two subtask and adversarial objective functions for joint training.\\nFor aspect extraction, the ﬁnal loss function can be calculated as follows:\\nL = LAE + Ld(θd) −Lg(θg)\\n(17)\\nFor aspect polarity prediction, the ﬁnal loss function can be computed as follows:\\nL = LAP + Ld(θd) −Lg(θg)\\n(18)\\n6. Experimental Results and Analysis\\nIn this section, we conduct extensive experimental results on two subtasks, AE and AP on our\\nMASAD dataset to evaluate our proposed methods and provide several baselines for this task.\\nWe ﬁrst present the implementation details of our models. Then we discuss the experimental\\nresults of AE and AP. To be speciﬁc, we compare the results using text only, visual only, and\\ntheir combination over seven domains of MASAD with various metrics. In addition, to verify the\\neffectiveness of our model, we compare our interaction model with concatenation, which simply\\nconcatenates the representations of the image, text, and aspect. To make the abbreviations easier\\nto understand, we list the speciﬁc nouns with the corresponding abbreviations (Table 3). These\\nresults also provide extensive benchmarks for this task.\\n12'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 13}, page_content='Acc.\\nF1\\nText\\nBiLSTM\\n62.43\\n56.72\\nImage\\nResNet50\\n30.43\\n17.39\\nMMAE\\nConcatenation\\n64.07\\n56.28\\nInteraction\\n70.32\\n68.92\\nMMAE (w/o Adv)\\nConcatenation\\n62.11\\n55.32\\nInteraction\\n69.23\\n68.42\\nTable 4: The experimental results of aspect extraction over the whole dataset. “(w/o) Adv” means the corresponding\\nmodel without adversarial training.\\nText\\nMultimodal\\nATAE-LSTM\\nIAN\\nRAM\\nTNet\\nMIMN\\nTomBERT\\nMMAP (Ours)\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nFood\\n93.06\\n86.13\\n93.56 88.30 93.64 89.25 94.34 90.18 94.72 91.39 95.56 91.80 95.75\\n92.89\\nGoods\\n95.31\\n94.85\\n95.77 95.30 95.59 95.12 95.88 95.34 95.93 95.87 96.05 96.10 96.55\\n96.44\\nBuildings 94.32\\n94.64\\n94.78 94.98 94.57 94.71 95.04 94.90 96.26 95.80 96.53 96.04 96.86\\n96.85\\nAnimal\\n93.56\\n92.85\\n94.05 93.32 94.29 93.55 94.76 93.87 95.03 94.06 95.62 94.78 95.92\\n95.62\\nHuman\\n91.55\\n91.34\\n92.04 92.13 91.75 91.87 92.27 92.16 92.54 92.31 92.67 92.53 92.74\\n92.74\\nPlant\\n93.05\\n93.40\\n93.85 93.71 93.50 93.37 94.38 94.21 95.04 94.97 95.67 95.30 97.02\\n96.97\\nScenery\\n91.87\\n91.03\\n92.33 91.87 92.15 91.39 92.76 91.98 93.17 92.38 93.63 92.94 94.57\\n94.15\\nAverage\\n93.25\\n92.03\\n93.77 92.80 93.64 92.75 94.02 93.23 94.67 93.83 95.10 94.21 95.63\\n95.09\\nTable 5: The experimental results of subtask aspect polarity prediction over seven domains over our model and strong\\nbaselines.\\n6.1. Implementation Details\\nIn our experiments, word embedding vectors and aspect embedding vectors are initialized\\nwith 300-dimension GloVe [47] vectors and ﬁne-tuned during the training, the same as [52]. The\\ndimension of hidden state vectors dh, and image feature dI are 300. Words out of vocabulary\\nGloVe, position embedding and weight matrices are initialized with the uniform distribution\\nU(−0.1, 0.1), and the biases are initialized to zero. Adam [53] is adopted as the optimizer with a\\nlearning rate of 0.001 and min-batch size 64. We implement our neural networks with Pytorch8.\\nWe split 10% from training data as the development set and keep the optimal parameters based\\non the best performance on the development set. We adopt Accuracy (Acc.) and Macro-Average\\nF1 (F1) to evaluate the model performance, which are the primary metrics used in aspect-based\\nsentiment analysis [28, 18].\\n6.2. Main Results\\nTo verify the effectiveness of our model, we compare our model with several strong baselines.\\nWe split the baselines into two parts: textual and multi-modality aspect-based sentiment analysis.\\nThe details are shown as follows.\\nFirst, we will introduce some textual aspect-based sentiment analysis baselines, which are\\nwidely used in this task.\\n• ATAE-LSTM takes aspect information into account via attention mechanism to capture\\nthe salient parts of the sentence [26].\\n8https://pytorch.org/\\n13'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 14}, page_content='• IAN designs an interaction attention mechanism to model the relationships between the\\naspect and sentence [27].\\n• RAM proposes a weighted memory mechanism to capture the sentiment information of\\nthe given aspect [28].\\n• TNet employs a CNN layer to extract important features from the transformed word rep-\\nresentations originated from a Bi-RNN layer [54].\\nSince all the above methods are based on text, we also provide some methods about multi-\\nmodal aspect-based sentiment analysis.\\n• MIMN adopts two interactive memory networks to model the textual and visual informa-\\ntion with the given aspect by learning the interaction between cross-modality data [10].\\n• TomBERT proposes a multimodal BERT model to obtain aspect-aware textual represen-\\ntations via multimodal interaction [11].\\nDifferent from the existing models, we ﬁrst learn the inter- and intra- interaction to learn the\\nrelationships among the aspect, text and image. Then, to map the image and text representation\\ninto a common space, we design an adversarial training strategy.\\nTable 5 reports the experimental results of our method and the baselines. From this table, we\\ncan obtain the following observations. First, multimodal based models perform better than text-\\nbased models. To be speciﬁc, MIMN and TomBERT perform better than all the textual aspect-\\nbased baselines. Second, our multimodal aspect-based sentiment classiﬁcation model performs\\nbetter than the state-of-the-art methods (e.g., TomBERT). It indicates that our model can learn\\nthe interaction between aspect, text and image well. In addition, our adversarial training strategy\\ncan align the representation of image and text into a common space effectively.\\nFor Food domain, the accuracy is relatively higher than the Macro-F1 score of the baselines\\n(about 5 points) since the distribution of samples in the this domain is unbalanced. Moreover, in-\\ntegrating multimodal information into aspect-based sentiment information can enhance the sam-\\nple representations. The interaction and alignment among multimodal information can further\\nimprove the performance. The difference between accuracy and Marco-F1 is less than 3 points\\nfor our MMAP model, which indicates that our model can reduce the inﬂuence of unbalance, to\\na certain extent.\\n6.3. Performance of Aspect Extraction (AE)\\nWe present the results of aspect extraction on our dataset in this section. Table 4 shows\\nthe accuracy and F1 of aspect extraction over the whole dataset. In addition, Table 6 reports the\\nexperimental results over seven domains with text only, image only, and their combination. From\\nthese tables, we ﬁnd the following observations.\\n• The model with text performs much better than the one with an image. We ﬁnd that the\\ndifferences between some aspects in images are relatively similar, which largely limits the\\nperformance of the model. It is worth exploring how to extract the aspect from the image\\nfor this task more effectively.\\n• The model with the interaction layer outperforms the model with concatenation, which\\nindicates that our interaction layer can capture the relationships between the text and image\\nmore effectively.\\n14'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 15}, page_content='Text\\nImage\\nMultimodal\\nBiLSTM\\nResNet50\\nConcatenation\\nInteraction\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nFood\\n91.02 89.95 45.14\\n3.74\\n89.24\\n88.51\\n90.72 91.11\\nGoods\\n83.24 81.27 59.02\\n7.63\\n85.24\\n82.57\\n85.68 82.79\\nBuildings 91.96 92.13 47.74\\n4.14\\n91.52\\n91.75\\n91.95 92.00\\nAnimal\\n86.70 85.05 56.39\\n5.06\\n88.28\\n86.17\\n91.81 89.46\\nHuman\\n77.04 68.39 41.19\\n4.92\\n77.38\\n71.27\\n78.52 69.49\\nPlant\\n85.36 85.75 55.36\\n5.49\\n88.43\\n88.49\\n96.07 95.49\\nScenery\\n78.47 78.44 57.88 10.89 79.75\\n79.51\\n81.66 80.80\\nAverage\\n84.83 83.00 51.82\\n5.98\\n85.69\\n84.04\\n88.06 85.88\\nTable 6: The experimental results of subtask aspect extraction over seven domains.\\n• However, we ﬁnd that simply concatenating the representation of the image and text some-\\ntimes even performs worse than the single-modal model (text-based models). It denotes\\nthat designing an effective interaction model is important for this task.\\n• Our adversarial training can signiﬁcantly improve performance, which indicates that this\\nmodule can align the representation of the text and image effectively.\\n• Moreover, we ﬁnd that our model obtains 70.32% in terms of accuracy with 57 classes,\\nwhich indicates the high quality of our MASAD dataset.\\n6.4. Performance of Aspect Polarity Prediction (AP)\\nIn this section, we present the results of the aspect polarity prediction (AP) task. Table 7\\nshows the experimental results of AP over the whole dataset. Table 8 reports the Accuracy and\\nF1 of AP with text only, image only and their combination respectively. For this task, we need\\nto model the relationships among the aspect, text, and image. From the experimental results, we\\nobserve that the model with our multimodal interaction layer outperforms the one with the text\\nonly, image only, and combination of them, which verify the great advantage of our interaction\\nmodel. Similarly, the text-based models perform better than image-based models for this task.\\nInferring the sentiment polarity of the image is more challenging than the text. It is observed\\nthat our MMAP model obtain 90.14% in terms of F1, which indicates the high quality of our\\nMASAD dataset. Moreover, the performance over the seven domains differs signiﬁcantly. Thus,\\nintegrating extra knowledge or transfer knowledge from other domains is an interesting point to\\ninvestigate.\\n6.5. Discussions\\nFirst, we do some ablation studies to verify the effectiveness of each parts of our models.\\n• Effectiveness of Interaction. 1) the inter-interaction between the aspect and the multi-\\nmodal information is important for this task. To be speciﬁc, we can ﬁnd that the perfor-\\nmance of the models with interaction is better than the ones with concatenation (Table 4).\\nFor example, the aspect “dog” can help the model to ﬁnd the object “dog” in the image and\\nﬁnd the word “dog” in the text (Figure 1). The expression of the aspect and the words near\\nthe aspect (e.g., best, love) are important for sentiment prediction. 2) the intra-interaction\\n15'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 16}, page_content='Acc.\\nF1\\nImage\\nConcatenation\\n78.36\\n77.01\\nInteraction\\n78.80\\n77.23\\nText\\nConcatenation\\n88.47\\n87.46\\nInteraction\\n89.48\\n88.78\\nMMAP\\nConcatenation\\n89.85\\n89.28\\nInteraction\\n90.64\\n90.14\\nMMAP (w/o Adv)\\nConcatenation\\n88.24\\n87.95\\nInteraction\\n89.43\\n89.73\\nTable 7: The experimental results of aspect polarity prediction over the whole dataset. “(w/o) Adv” means the corre-\\nsponding model without adversarial training.\\nText\\nImage\\nMultimodal\\nConcatenationTA InteractionTA ConcatenationIA InteractionIA ConcatenationITA InteractionITA\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nFood\\n92.50\\n85.65\\n95.31 91.93 87.14\\n78.60\\n87.29 78.29 95.60\\n92.69\\n95.75\\n92.89\\nGoods\\n94.88\\n94.74\\n96.64 96.53 88.07\\n87.61\\n88.78 88.45 95.06\\n94.88\\n96.55\\n96.44\\nBuildings 93.88\\n93.81\\n96.01 96.00 83.53\\n83.53\\n85.85 85.83 96.01\\n95.99\\n96.86\\n96.85\\nAnimal\\n93.43\\n92.90\\n93.55 92.90 79.42\\n77.06\\n79.59 77.52 94.25\\n93.75\\n95.92\\n95.62\\nHuman\\n92.40\\n92.39\\n93.54 93.53 85.66\\n85.65\\n85.97 85.95 91.72\\n91.70\\n92.74\\n92.74\\nPlant\\n92.85\\n92.69\\n92.93 92.84 83.16\\n82.85\\n84.38 84.23 94.83\\n94.73\\n97.02\\n96.97\\nScenery\\n91.82\\n90.87\\n93.96 93.43 82.95\\n81.25\\n82.95 81.46 94.11\\n93.64\\n94.57\\n94.15\\nAverage\\n93.11\\n91.86\\n94.56 93.88 84.28\\n82.36\\n84.97 83.10 94.51\\n93.91\\n95.63\\n95.09\\nTable 8: The experimental results of subtask aspect polarity prediction over seven domains with text only, image only\\nand combination of them. The mask TA and IA denote the concatenation/interaction for text and aspect, image and aspect\\nrespectively, ITA indicates the concatenation/interaction for image, text and aspect.\\nbetween the text and image also plays a key role in this task. It is observed that our models\\nwith InteractionITA perform better than ConcatenationITA (Table 8). For instance, the word\\n“running” in the text expresses the dog’s action in the image.\\n• Effectiveness of Adversarial Training. From Table 4 and Table 7, we observe that the\\nmodels with our adversarial training strategy performs better than the corresponding one\\nwithout adversarial training. All these observations show that our adversarial training\\nstrategy can align the feature representation of the text and image effectively.\\nSecond, we investigate the complexity and convergence of our proposed MMAP model (Ta-\\nble 9).\\n• Convergence Analysis. From Table 9, we ﬁnd that our proposed MMAP model converges\\nfaster than the MIMN model. In addition, our model obtains the better performance than\\nComplexity\\nConvergence (Acc.)\\nParameters\\nEpoch 1\\nEpoch 2\\nEpoch 3\\nEpoch 4\\nEpoch 5\\nMIMN\\n26M\\n84.51\\n85.71\\n86.34\\n86.18\\n87.62\\nMMAP (Ours)\\n25M\\n86.53\\n87.09\\n88.73\\n89.33\\n89.87\\nTable 9: The complexity and convergence of our MMAP model and MIMN. We report the accuracy of test set over ﬁrst\\nﬁve epoches and the parameters of the models.\\n16'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 17}, page_content='MIMN with the same epoch number. Also, these indicate that our model has a better\\nconvergence rate than the MIMN model.\\n• Complexity Analysis. We also calculate the parameters of these two models. The param-\\neters of these two models are similar, which denotes that our MMAP model has a similar\\nspace complexity with the MIMN model. Since our MMAP model converges faster than\\nMIMN, the time complexity of MMAP is smaller than it.\\n7. Conclusion and Future Work\\nIn this paper, we focus on the new task named multimodal aspect-based sentiment analysis\\n(MASA), which performs aspect-based sentiment analysis based on the multimodal content on\\nsocial media platforms. We propose a multimodal interaction model for this task, which learns\\nthe interaction between the aspect, text, and image effectively. Then we present and release a\\nlarge-scale multimodal aspect-based sentiment analysis dataset named MASAD. The MASAD\\ninvolves 57 aspects with seven domains and contains 38k samples with image-text pair. This\\ndataset provides a new point of view for aspect-based sentiment classiﬁcation, and also a new\\nbenchmark for MASA. We believe this dataset will push the state-of-the-art in MASA. We also\\npropose several strong baselines for this task. Extensive experiments have been conducted on\\nthe MASAD dataset, showing high generalization capability of models trained on the proposed\\ndataset and the beneﬁt of using multiple visual modalities.\\nWe can conclude from the experimental results and analysis that MASA is more challeng-\\ning than textual aspect-based sentiment analysis. We believe the following research directions\\nare worth studying: (1) Designing more expressive model architectures for learning the inter-\\nmultimodal information; (2) Exploring how to extract the aspect and predict the sentiment jointly;\\n(3) Leveraging extra knowledge or transferring the knowledge from similar domains to improve\\nthe performance of MASA; (4) Expanding the dataset with multiple linguistics is an under-\\nexplored area where more work is expected.\\nAcknowledgments.\\nWe greatly appreciate anonymous reviewers and the associate editor for their valuable and\\nhigh quality comments that greatly helped to improve the quality of this article. This research is\\nfunded by the Science and Technology Commission of Shanghai Municipality (19511120200).\\nThis research is also supported by the Natural Sciences and Engineering Research Council\\n(NSERC) of Canada, an NSERC CREATE award in ADERSIM9, the York Research Chairs\\n(YRC) program and an ORF-RE (Ontario Research Fund-Research Excellence) award in BRAIN\\nAlliance10.\\nReferences\\n[1] Y. Lu, C. Zhai, Opinion integration through semi-supervised topic modeling, in: Proceedings of WWW, 2008, pp.\\n121–130.\\n9http://www.yorku.ca/adersim\\n10http://brainalliance.ca\\n17'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 18}, page_content='[2] B. Jou, T. Chen, N. Pappas, M. Redi, M. Topkara, S.-F. Chang, Visual affect around the world: A large-scale\\nmultilingual visual sentiment ontology, in: Proceedings of ACMMM, ACM, 2015, pp. 159–168.\\n[3] B. Liu, Sentiment analysis and opinion mining, Synthesis lectures on human language technologies 5 (2012)\\n1–167.\\n[4] J. Zhou, J. X. Huang, Q. Chen, Q. V. Hu, T. Wang, L. He, Deep learning for aspect-level sentiment classiﬁcation:\\nSurvey, vision and challenges, IEEE Access (2019) 78454–78483.\\n[5] S. Manandhar, Semeval-2014 task 4: Aspect based sentiment analysis, in: Proceedings of SemEval, 2014.\\n[6] M. Mitchell, J. Aguilar, T. Wilson, B. Van Durme, Open domain targeted sentiment, in: Proceedings of EMNLP,\\n2013, pp. 1643–1654.\\n[7] M. Pontiki, D. Galanis, H. Papageorgiou, S. Manandhar, I. Androutsopoulos, Semeval-2015 task 12: Aspect based\\nsentiment analysis, in: Proceedings of SemEval, 2015, pp. 486–495.\\n[8] M. Pontiki, D. Galanis, H. Papageorgiou, I. Androutsopoulos, S. Manandhar, A.-S. Mohammad, M. Al-Ayyoub,\\nY. Zhao, B. Qin, O. De Clercq, et al., Semeval-2016 task 5: Aspect based sentiment analysis, in: Proceedings of\\nSemEval, 2016, pp. 19–30.\\n[9] M. Saeidi, G. Bouchard, M. Liakata, S. Riedel, Sentihood: Targeted aspect based sentiment analysis dataset for\\nurban neighbourhoods, in: Proceedings of COLING, 2016, pp. 1546–1556.\\n[10] N. Xu, W. Mao, G. Chen, Multi-interactive memory network for aspect based multimodal sentiment analysis (2019)\\n371–378.\\n[11] J. Yu, J. Jiang, Adapting BERT for target-oriented multimodal sentiment classiﬁcation (2019) 5408–5414.\\n[12] J. Zhou, J. X. Huang, Q. V. Hu, L. He, Modeling multi-aspect relationship with joint learning for aspect-level\\nsentiment classiﬁcation, in: International Conference on Database Systems for Advanced Applications, Springer,\\n2020, pp. 786–802.\\n[13] S. Poria, E. Cambria, D. Hazarika, N. Majumder, A. Zadeh, L.-P. Morency, Context-dependent sentiment analysis\\nin user-generated videos, in: Proceedings of ACL, 2017, pp. 873–883.\\n[14] J. Zhou, J. Tian, R. Wang, Y. Wu, W. Xiao, L. He, Sentix: A sentiment-aware pre-trained model for cross-domain\\nsentiment analysis, in: Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp.\\n568–579.\\n[15] S. Kiritchenko, X. Zhu, C. Cherry, S. Mohammad, Nrc-canada-2014: Detecting aspects and sentiment in customer\\nreviews, in: Proceedings of SemEval, 2014, pp. 437–442.\\n[16] D.-T. Vo, Y. Zhang, Target-dependent twitter sentiment classiﬁcation with rich automatic features., in: Proceedings\\nof IJCAI, 2015, pp. 1347–1353.\\n[17] D. Tang, B. Qin, X. Feng, T. Liu, Effective lstms for target-dependent sentiment classiﬁcation, in: Proceedings of\\nCOLING, 2016, pp. 3298–3307.\\n[18] R. He, W. S. Lee, H. T. Ng, D. Dahlmeier, Exploiting document knowledge for aspect-level sentiment classiﬁcation,\\nin: Proceedings of ACL, 2018, pp. 579–585.\\n[19] X. Li, W. Lam, Deep multi-task learning for aspect term extraction with memory interaction, in: Proceedings of\\nEMNLP, 2017, pp. 2886–2892.\\n[20] C. Fan, Q. Gao, J. Du, L. Gui, R. Xu, K.-F. Wong, Convolution-based memory network for aspect-based sentiment\\nanalysis, in: Proceedings of SIGIR, ACM, 2018, pp. 1161–1164.\\n[21] J. Liu, Y. Zhang, Attention modeling for targeted sentiment, in: Proceedings of EACL, volume 2, 2017, pp.\\n572–577.\\n[22] S. Wang, S. Mazumder, B. Liu, M. Zhou, Y. Chang,\\nTarget-sensitive memory networks for aspect sentiment\\nclassiﬁcation, in: Proceedings of ACL, volume 1, 2018, pp. 957–967.\\n[23] W. Xue, T. Li, Aspect based sentiment analysis with gated convolutional networks, in: Proceedings of ACL,\\nAssociation for Computational Linguistics, 2018, pp. 2514–2523.\\n[24] J. Zhou, Q. Chen, J. X. Huang, Q. V. Hu, L. He,\\nPosition-aware hierarchical transfer model for aspect-level\\nsentiment classiﬁcation, Information Sciences 513 (2020) 1–16.\\n[25] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning to align and translate, arXiv\\npreprint arXiv:1409.0473 (2014).\\n[26] Y. Wang, M. Huang, L. Zhao, et al., Attention-based lstm for aspect-level sentiment classiﬁcation, in: Proceedings\\nof EMNLP, 2016, pp. 606–615.\\n[27] D. Ma, S. Li, X. Zhang, H. Wang, Interactive attention networks for aspect-level sentiment classiﬁcation, in:\\nProceedings of IJCAI, 2017, pp. 4068–4074.\\n[28] P. Chen, Z. Sun, L. Bing, W. Yang, Recurrent attention network on memory for aspect sentiment analysis, in:\\nProceedings of EMNLP, 2017, pp. 452–461.\\n[29] Q. Liu, H. Zhang, Y. Zeng, Z. Huang, Z. Wu, Content attention model for aspect based sentiment analysis, in:\\nProceedings of WWW, International World Wide Web Conferences Steering Committee, 2018, pp. 1023–1032.\\n[30] J. Wang, J. Li, S. Li, Y. Kang, M. Zhang, L. Si, G. Zhou, Aspect sentiment classiﬁcation with both word-level and\\nclause-level attention networks., in: Proceedings of IJCAI, 2018, pp. 4439–4445.\\n18'),\n",
              " Document(metadata={'producer': '', 'creator': 'Elsevier', 'creationdate': '2021-05-18T06:12:04+05:30', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/MASAD sataset.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'MASAD: A Large-Scale Dataset for Multimodal Aspect-Based Sentiment Analysis', 'author': 'Jie Zhou', 'subject': 'Neurocomputing, Journal Pre-proof. doi:10.1016/j.neucom.2021.05.040', 'keywords': '', 'moddate': '2021-05-18T06:12:52+05:30', 'trapped': '', 'modDate': \"D:20210518061252+05'30'\", 'creationDate': \"D:20210518061204+05'30'\", 'page': 19}, page_content='[31] F. Zhao, Z. Wu, X. Dai, Attention transfer network for aspect-level sentiment classiﬁcation, in: Proceedings of the\\n28th International Conference on Computational Linguistics, 2020, pp. 811–821.\\n[32] J. Zhou, J. X. Huang, Q. V. Hu, L. He, Sk-gcn: Modeling syntax and knowledge via graph convolutional network\\nfor aspect-level sentiment classiﬁcation, Knowledge-Based Systems 205 (2020) 106292.\\n[33] Y. Ma, H. Peng, E. Cambria, Targeted aspect-based sentiment analysis via embedding commonsense knowledge\\ninto an attentive lstm, in: Proceedings of AAAI, 2018, pp. 5876–5883.\\n[34] H. Xu, B. Liu, L. Shu, S. Y. Philip, Bert post-training for review reading comprehension and aspect-based sen-\\ntiment analysis, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 2324–\\n2335.\\n[35] M. H. Phan, P. O. Ogunbona, Modelling context and syntactical features for aspect-based sentiment analysis, in:\\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 3211–3220.\\n[36] L. Dong, F. Wei, C. Tan, D. Tang, M. Zhou, K. Xu, Adaptive recursive neural network for target-dependent twitter\\nsentiment classiﬁcation, in: Proceedings of ACL, volume 2, 2014, pp. 49–54.\\n[37] Z. Fan, Z. Wu, X. Dai, S. Huang, J. Chen, Target-oriented opinion words extraction with target-fused neural\\nsequence labeling, in: Proceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp.\\n2509–2518.\\n[38] Q. Jiang, L. Chen, R. Xu, X. Ao, M. Yang, A challenge dataset and effective models for aspect-based sentiment\\nanalysis, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the\\n9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 6281–6286.\\n[39] M. Chen, S. Wang, P. P. Liang, T. Baltruˇsaitis, A. Zadeh, L.-P. Morency, Multimodal sentiment analysis with\\nword-level fusion and reinforcement learning, in: Proceedings of ICMI, ACM, 2017, pp. 163–171.\\n[40] S. Poria, H. Peng, A. Hussain, N. Howard, E. Cambria, Ensemble application of convolutional neural networks\\nand multiple kernel learning for multimodal sentiment analysis, Neurocomputing 261 (2017) 217–230.\\n[41] R. Kaur, S. Kautish, Multimodal sentiment analysis: A survey and comparison, IJSSMET 10 (2019) 38–58.\\n[42] D. Ghosal, M. S. Akhtar, D. Chauhan, S. Poria, A. Ekbal, P. Bhattacharyya, Contextual inter-modal attention for\\nmulti-modal sentiment analysis, in: Proceedings of EMNLP, 2018, pp. 3454–3466.\\n[43] A. Zadeh, P. P. Liang, S. Poria, P. Vij, E. Cambria, L.-P. Morency, Multi-attention recurrent network for human\\ncommunication comprehension, in: Proceedings of AAAI, 2018.\\n[44] D. Borth, R. Ji, T. Chen, T. Breuel, S.-F. Chang, Large-scale visual sentiment ontology and detectors using adjective\\nnoun pairs, in: Proceedings of ACMMM, ACM, 2013, pp. 223–232.\\n[45] X. Jin, A. Gallagher, L. Cao, J. Luo, J. Han, The wisdom of social multimedia: using ﬂickr for prediction and\\nforecast, in: Proceedings of ACMMM, ACM, 2010, pp. 1235–1244.\\n[46] K. Krippendorff, Computing krippendorff’s alpha-reliability (2011).\\n[47] J. Pennington, R. Socher, C. Manning, Glove: Global vectors for word representation, in: Proceedings of EMNLP,\\n2014, pp. 1532–1543.\\n[48] A. Graves, J. Schmidhuber, Framewise phoneme classiﬁcation with bidirectional lstm and other neural network\\narchitectures, Neural Networks 18 (2005) 602–610.\\n[49] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of CVPR, 2016,\\npp. 770–778.\\n[50] V. Gajarla, A. Gupta, Emotion detection and sentiment analysis of images, Georgia Institute of Technology (2015).\\n[51] X. Chen, Z. Shi, X. Qiu, X.-J. Huang, Adversarial multi-criteria learning for chinese word segmentation, in: Pro-\\nceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\n2017, pp. 1193–1203.\\n[52] D. Tang, B. Qin, T. Liu, Aspect level sentiment classiﬁcation with deep memory network, in: Proceedings of\\nEMNLP, 2016, pp. 214–224.\\n[53] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in: Proceedings of ICLR, volume 5, 2015.\\n[54] X. Li, L. Bing, W. Lam, B. Shi, Transformation networks for target-oriented sentiment classiﬁcation, in: Proceed-\\nings of ACL, 2018, pp. 946–956.\\n19'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 0}, page_content='A Novel Energy Based Model Mechanism for Multi-Modal Aspect-Based\\nSentiment Analysis\\nTianshuo Peng1,†, Zuchao Li1,†,*, Ping Wang3,4, Lefei Zhang1,2, and Hai Zhao5\\n1School of Computer Science, Wuhan University, Wuhan, 430072, China\\n2Hubei Luojia Laboratory, Wuhan 430072, P. R. China,\\n3Center for the Studies of Information Resources, Wuhan University, Wuhan 430072, China\\n4School of Information Management, Wuhan University, Wuhan 430072, China\\n5Department of Computer Science and Engineering, Shanghai Jiao Tong University\\n{pengts,zcli-charlie,wangping,zhanglefei}@whu.edu.cn, zhaohai@cs.sjtu.edu.cn\\nAbstract\\nMulti-modal aspect-based sentiment analysis (MABSA) has\\nrecently attracted increasing attention. The span-based ex-\\ntraction methods, such as FSUIE, demonstrate strong perfor-\\nmance in sentiment analysis due to their joint modeling of in-\\nput sequences and target labels. However, previous methods\\nstill have certain limitations: (i) They ignore the difference\\nin the focus of visual information between different analysis\\ntargets (aspect or sentiment). (ii) Combining features from\\nuni-modal encoders directly may not be sufficient to elim-\\ninate the modal gap and can cause difficulties in capturing\\nthe image-text pairwise relevance. (iii) Existing span-based\\nmethods for MABSA ignore the pairwise relevance of tar-\\nget span boundaries. To tackle these limitations, we propose\\na novel framework called DQPSA for multi-modal sentiment\\nanalysis. Specifically, our model contains a Prompt as Dual\\nQuery (PDQ) module that uses the prompt as both a visual\\nquery and a language query to extract prompt-aware visual\\ninformation and strengthen the pairwise relevance between\\nvisual information and the analysis target. Additionally, we\\nintroduce an Energy-based Pairwise Expert (EPE) module\\nthat models the boundaries pairing of the analysis target from\\nthe perspective of an Energy-based Model. This expert pre-\\ndicts aspect or sentiment span based on pairwise stability.\\nExperiments on three widely used benchmarks demonstrate\\nthat DQPSA outperforms previous approaches and achieves a\\nnew state-of-the-art performance. Furthermore, we conducted\\na fair comparison with relevant large-scale models such as\\nChatGPT-3.5 and VisualGLM. We found that our model has\\n* Corresponding author. † Equal contribution. This work was\\nsupported by the National Natural Science Foundation of China\\n(No. 62306216), the Natural Science Foundation of Hubei Province\\nof China (No. 2023AFB816), the Fundamental Research Funds for\\nthe Central Universities (No. 2042023kf0133), the Special Fund of\\nHubei Luojia Laboratory (No. 220100014), National Natural Sci-\\nence Foundation of China [No. 72074171] [No. 72374161].\\nCopyright © 2024, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\nsignificant advantages in terms of performance though with\\nrelatively fewer parameters. A large amount of complemen-\\ntary experiments and ablation studies further demonstrate the\\neffectiveness of the components we proposed. The code will\\nbe released at https://github.com/pengts/DQPSA.\\nIntroduction\\nAs one of the most important tasks that examines a model’s\\nsemantic comprehension and sentiment perception, Multi-\\nmodal Aspect-Based Sentiment Analysis (MABSA) is an\\nchallenging and fine-grained task in the Sentiment Analy-\\nsis field and has attracted increasing attention. The MABSA\\ntask consists of three main tasks: given an image-text pair,\\nMulti-modal Aspect Term Extraction (MATE) focuses on\\nextracting all aspect terms with sentiment polarity in the\\nsentence (Zhao et al. 2022; Lu et al. 2018; Wu et al.\\n2020a), Multi-modal Aspect-oriented Sentiment Classifica-\\ntion (MASC) aims to determine the sentiment polarity of\\neach given aspect (Xu, Mao, and Chen 2019; Yu and Jiang\\n2019; Yu, Jiang, and Xia 2020), Joint Multi-modal Aspect-\\nSentiment Analysis (JMASA), on the other hand, requires\\nthe model to extract aspect-sentiment pairs jointly (Ju et al.\\n2021; Ling, Yu, and Xia 2022; Zhou et al. 2023). Among\\nall the methods of previous work, the span-based extrac-\\ntion methods, such as FSUIE (Peng et al. 2023), demon-\\nstrate strong performance in sentiment analysis due to their\\njoint modeling of input sequences and target labels. Besides,\\nit avoids complex structures for sequence labelling or se-\\nquence generation with a more concise structure.\\nIn this scenario of fine-grained MABSA task, three main\\nchallenges are worth emphasizing: First, image contains a\\nlarge amount of semantic information, and there is a dif-\\nference in the focus of visual information between differ-\\nent analysis targets. Take figure 1 as an example: (1) The\\nfocus of visual information is different between MATE and\\narXiv:2312.08084v2  [cs.AI]  15 Dec 2023'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 1}, page_content='MASC tasks: the MATE task should focus on all the po-\\ntential entities across the image while MASC concentrates\\non the details of specific aspect which is fine-grained. (2) In\\nthe MASC task, different image regions may imply differ-\\nent sentimental tendencies, resulting the difference of focus\\namong aspects. Previous work only focused on the general\\ninformation in image features, while the visual information\\nrepresenting positive emotions, such as the person ponder-\\ning with his chin resting on their hand, and the visual infor-\\nmation representing negative emotions, such as the person\\nwith his head lowered would influence each other, introduc-\\ning a significant amount of misleading information into their\\nsentiment analysis.\\nSecond, most of the aforementioned studies focus on ex-\\ntracting modal features using pre-trained unimodal models\\nand fusing them directly. However, separate training of im-\\nage feature and text feature ignores the semantic alignment\\nand modal gap between text and image, leading to the dif-\\nficulty of the model in capturing the pairwise relevance be-\\ntween image and text. Therefore, it is crucial to design spe-\\ncific structures to mitigate modal gap and strengthen image-\\ntext pairwise relevance\\nBesides, existing span-based models (Peng et al. 2023)\\nconsider independently the possibility of certain position as\\na start or end boundary while ignoring the pairwise relevance\\nbetween span boundaries, i.e., the a priori knowledge that\\n”the boundaries of spans should be semantically related”.\\nBased on the challenges above, we proposed the DQPSA\\nframework for Multi-modal Aspect-Based Sentiment Anal-\\nysis, which unifies the MABSA tasks under one framework.\\nSpecifically, inspired by BLIP-2 (Li et al. 2023a) that trains\\nan adapter to filter features from visual encoder, we designed\\nthe Prompt as Dual Query module to address the issue that\\ndifferent analysis targets pay different attention to visual in-\\nformation and strengthen the pairwise relevance between\\nimage and text. Prompt as Dual Query uses prompt as both\\nvisual query and language query. The visual query interacts\\nwith image features from frozen pre-trained image encoder\\nin alternating self-attention layers and cross-attention lay-\\ners, from which the prompt-aware visual information with\\nthe highest semantic relevance to the analysis target is ex-\\ntracted. The language query will act as one of the input of\\ntext encoder, guiding model to output prediction based on\\nthe current analysis target. Considering the pairwise rele-\\nvance between target span boundaries, we introduce the idea\\nof Energy Based Model (LeCun et al. 2006) to give better\\nspan scores and proposed the novel Energy based Pairwise\\nExpert that predicts span based on pairing stability. Experi-\\nments on three widely used benchmarks verify that DQPSA\\noutperforms previous approaches and achieves the state-of-\\nthe-art performance. A large amount of complementary ex-\\nperiments and ablation studies further demonstrate the effec-\\ntiveness of components we proposed.\\nIn summary, our contributions are as follows:\\n• We proposed Prompt as Dual Query module that satisfy\\ndiverse focus of different analysis targets on visual informa-\\ntion, and strengthen the pairwise relevance between visual\\ninformation and analysis target.\\n• Inspired by the Energy Based Model (EBM) that quantify-\\nText: Chess World Championship: \\ncontemplating Liren Ding and frustrated \\nNebo just now decided the winner!\\nAspect\\ncontemplating \\nLiren Ding\\nfrustrated Nebo\\nSentiment\\nPositive\\nNegative\\nFigure 1: Example of the variability in the focus of different\\nanalysis tasks\\ning compatibility between variables using an energy scalar,\\nwe proposed a novel Energy based Pairwise Expert that\\nmodels the boundaries pairwise stability of target span.\\n• Experiments on three widely used benchmarks Twit-\\nter2015, Twitter2017 and Political Twitter shows that\\nDQPSA outperforms previous approaches and achieves\\nSOTA performance. DQPSA also significantly outperforms\\nMulti-modal Large Language Model (LLM) VisualGLM-6B\\nand Uni-modal LLM ChatGPT-3.5 under fair comparison.\\nRelated Work\\nMulti-modal Aspect-Based Sentiment Analysis\\nThe sentiment analysis task, due to its strong correlation\\nwith human sentiment traits, has been regarded as a chal-\\nlenging benchmark to verify the model’s semantic compre-\\nhension and analytical ability, has attracted many research\\nefforts ( (Peng, Li, and Zhao 2021), (Jing et al. 2021)). With\\nthe proliferation of multi-modal data disseminated on social\\nmedia, images are considered to be an important comple-\\nmentary information for sentiment analysis. Thus, MABSA\\nbegan to receive increasing attention.\\n(Wang et al. 2022b) aligns the image into regional ob-\\nject tags, image-level captions and optical characters as vi-\\nsual contexts for better interactions. (Wang et al. 2022a) in-\\njects knowledge-aware information through multi-modal re-\\ntrieval. (Cai, Cai, and Wan 2019) treats image attribute fea-\\ntures as supplementary modalities to bridge the gap between\\ntexts and images. (Ju et al. 2021) first proposes JMASA\\ntask that jointly extract aspects and corresponding sentiment\\npolarity to better satisfy the practical applications, (Ling,\\nYu, and Xia 2022) performs various vision-language pre-\\ntraining tasks to capture crossmodal alignment, and (Zhou\\net al. 2023) designs an aspect-aware attention module to se-\\nlect textual tokens and image blocks that are semantically\\nrelated to the aspects. The above approaches for JMASA\\neither treat MATE and MASC as sequence labelling and bi-\\nnary classification, or require an additional decoding module\\nfor sequence generating, and do not emphasize the variabil-\\nity of image focus across analysis targets. Unlike previous\\nworks, our proposed DQPSA address MATE and MASC\\nunder a unified framework as span recognition, dispens-\\ning with the complex sequence generation structure. Mean-\\nwhile, we design the Prompt as Dual Query module that uses\\nprompt as both visual query and language query, in order to\\ndifferentially extract prompt-aware visual information and\\nstrengthen image-text pairwise relevance.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 2}, page_content='Energy Based Model\\nThe concept of Energy Based Model was first proposed\\nby (LeCun et al. 2006). The core idea is to establish a map-\\nping between different variable configurations and a scalar\\nenergy that will be able to measure compatibility, thus cap-\\nturing the dependence between different variable configura-\\ntions. The target of learning is to find an efficient energy\\nfunction that maps correct variable configurations to low\\nenergy values while mapping incorrect variable configura-\\ntions to high energy values. The goal of inference is to find\\nvariable configurations that minimize the energy. The loss\\nfunction selected during training can be used to measure\\nthe effectiveness of the energy function. (Zou, Yang, and\\nWu 2021) introduce the concept of EBM to text-only adver-\\nsarial domain adaptation to construct an autoencoder dur-\\ning the training phase. This autoencoder maps the source-\\ndomain data to a low-energy space and imposes constraints\\non the target-domain data distribution to align the source-\\ntarget features. However, due to the fact that the code and\\ndataset used in (Zou, Yang, and Wu 2021) have not been\\npublicly released, and considering the differences in dataset\\nand task, it’s hard for us to make a fair comparison with it.\\nInspired by the idea of Energy Based Model, we quantify\\nboundary pairing stability of potential span with scalar ener-\\ngies and design the Energy based Pairwise Expert to predict\\nspan based on pairwise stability. To the best of our knowl-\\nedge, this is the first attempt that adapting Energy Based\\nModel into MABSA task.\\nMethod\\nIn this section, we first introduce the general framework of\\nour proposed DQPSA. Then we introduce the specific struc-\\nture of Prompt as Dual Query, followed by a detailed de-\\nscription of Energy based Pairwise Expert . Figure 2 demon-\\nstrates the framework of our proposed DQPSA, which con-\\nsists of four main components: a frozen image encoder, a\\nPrompt as Dual Query module, a text encoder and an En-\\nergy based Pairwise Expert. To strengthen the correlation\\nbetween visual information and analysis target, we design\\nthe Prompt as Dual Query that allows prompt to interacting\\nwith both image and text. Besides, in order to consider the\\nboundary positions of the target span more comprehensively,\\nwe design the Energy based Pairwise Expert to extract span\\nconsidering both start and end boundaries simultaneously.\\nIn the following subsections, we will illustrate the details of\\nour proposed model.\\nPrompt as Dual Query (PDQ)\\nWe propose the Prompt as Dual Query (PDQ) module,\\nwhich leverage prompt as both visual query and language\\nquery, guiding model to focus on different perspectives of\\nvisual information and text information according to con-\\ncrete analysis targets.\\nFigure 3 shows the specific structure of PDQ, which\\nis mainly composed of two kinds of blocks stacked alter-\\nnately: cross-attention block and self-attention block. we as-\\nsume that the image feature obtained from the frozen im-\\nage encoder is FI = {I0, I1, I2 · · · ILI}. For each image,\\nwe constructed a description for its content, the features\\nof the constructed description and the prompt are F i\\nD =\\n\\x08\\nIi\\n0, Ii\\n1, Ii\\n2 · · · Ii\\nLD\\n\\t\\nand F i\\nP =\\n\\x08\\nIi\\n0, Ii\\n1, Ii\\n2 · · · Ii\\nLP\\n\\t\\n, where\\nLI, LD, LP are the lengths of the corresponding sequences.\\ni ∈[-1, N] represents the index of the block in which FD\\nand FP are located and i = −1 represents the original word\\nembedding. The PDQ receives three types of text sequences\\nbelonging to S = {F −1\\nD , F −1\\nP\\n, [F −1\\nP\\n: F −1\\nD ]} as inputs, with\\nonly sequences belonging to ˆS = {F −1\\nP\\n, [F −1\\nP\\n: F −1\\nD ]} in-\\nteracting with image features in cross-attention layer.\\nThe basic formula for attention can be expressed as:\\nATTN(Q, K, V ) = Softmax\\n \\nQ K⊤\\n√dk\\n!\\nV.\\n(1)\\nSuppose the hidden state Hi−1 of the previous layer, for in-\\nput ∈S, the result of self-attention layer can be represented\\nas follows, where WQS, WKS, WV S, WQC, WKC, WV C\\nare the parameters to be optimized::\\nSELF-ATTN(Hi−1)\\n= ATTN(WQSHi−1, WKSHi−1, WVSHi−1).\\n(2)\\nAs for input ∈ˆS, it will go through an additional cross-\\nattention layer in cross-attention block that can be repre-\\nsented as:\\nI2T-ATTN(Hi−1)\\n= ATTN(WQCHi−1[: LP], WKCFI, WVCFI),\\nCROSS-ATTN(Hi−1)\\n= CAT[I2T-ATTN(Hi−1) : Hi−1[LP :]].\\n(3)\\nwhere Hi−1[: LP] represents the sub-sequence of Hi−1\\nup to the LP-th token and Hi−1[LP :] represents the sub-\\nsequence of Hi−1 from and include the LP-th token. In\\nthe inference process, we use F −1\\nP\\nas input. Through mul-\\ntiple fusion with FI, prompt guides the model to filter the\\nprompt-aware visual information that semantically related\\nto the analysis target.\\nTo further strength the pairwise relevance between visual\\ninformation and analysis targets, we introduce image-text\\nmatching task and in-batch image-text contrastive learning,\\nthe specific algorithmic process is as follows:\\nFor image-text matching task, we have\\nLOSSITM = −1\\n2\\n1\\nX\\ni=0\\npilog(qi),\\np = MEAN((WITM[F N\\nVQ : F N\\nD ])[: LVQ]).\\n(4)\\nwhere the hidden state in the last block of PDQ will go\\nthough an linear projection and we select the average of to-\\nkens corresponding to visual query as model prediction p. q\\nis the label that identifies whether the image and description\\nmatch or not. WITM is the parameter to be optimized.\\nFor in-batch image-text comparative learning, we firstly\\nuse the first token of F N\\nVQ and F N\\nD as [CLS] token to con-\\nstruct the similarity matrix between different F N\\nVQ and F N\\nD'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 3}, page_content='��(��, ��)\\nChess World Championship: contemplating Liren \\nDing and frustrated Nebo just now decided the winner! \\n⋯\\nFrozen Image Encoder\\nSentiment of  “contemplating Liren \\nDing” is [ positive, neutral, negative ].\\nToken Embedding\\n[Prompt]\\n[Text]\\nSelf Attention Block\\nCross Attention Block\\nPDQ Module\\n× �\\nVisual Query\\nText Query\\nPrompt Tokens\\n��0\\n[���]\\n��1\\n⋯\\n[���]\\nImage Feature of Interest\\n��\\n0\\n��\\n1\\n⋯\\n[���]\\n��\\n2\\nText tokens\\n��\\n0\\n[���]\\n��\\n1\\n⋯\\n[���]\\n��\\n2\\n��\\n3\\n:\\n:\\nText Encoder\\nEPE Module\\n[Positive]\\n[Image]\\n[\\nPositive\\n,\\nNeutral\\n,\\nNegative\\n]\\n[\\nPositive\\n,\\nNeutral\\n,\\nNegative\\n]\\nComponent Level \\nEnergy Distribution\\nSpan Distribution Candidates\\n�1\\n�2\\n�3\\nSystem Energy  ��(��, ��)\\nSequence Output ��\\n �= ���������(��, ��)\\nFigure 2: The overview of our proposed DQPSA.\\nPDQ\\nImage-Text \\nrepresentations\\nImage-Text Matching\\nSelf Attention\\nCross Attention\\nMulti-modal FFN\\n[Visual Query][Description]\\nImage Features of \\nInterest\\n[Visual Query]\\nImage-Text Contrastive Learning\\nText features\\n[Description]\\nImage \\nfeatures\\n⋯\\nPDQ\\nSelf Attention\\nCross Attention\\nMulti-modal FFN\\nPDQ\\nSelf Attention\\nText FFN\\nFigure 3: Demonstration of Prompt as Dual Query\\nwithin the same batch:\\nIITC = [IITC\\n1\\n, IITC\\n2\\n· · · IITC\\nB\\n],\\nT ITC = [T ITC\\n1\\n, T ITC\\n2\\n· · · T ITC\\nB\\n].\\n(5)\\nwhere B is the batch size, IITC\\ni\\nand T ITC\\ni\\nis the [CLS] token\\nof the i-th F N\\nVQ and F N\\nD in batch. And then we construct the\\nsimilarity vector pi2d pd2i, and corresponding label qi2d qd2i,\\nfor the i-th F N\\nVQ and F N\\nD in the batch:\\npi2d = T ITC⊤IITC\\ni\\n, pd2i = IITC⊤T ITC\\ni\\n,\\nqi2d, qd2i ∈RB×1, qi2d\\nj , qd2i\\nj\\n= 1 if j == i else 0.\\n(6)\\n���\\n���\\ns1\\ns2\\ns3\\n⋮\\n⋮\\nsL−1\\nsL\\ns1\\xa0\\xa0\\xa0s2\\xa0\\xa0\\xa0\\xa0s3\\xa0\\xa0⋯\\xa0\\xa0\\xa0⋯\\xa0\\xa0sL−1\\xa0sL\\ns1\\ns2\\ns3\\n⋮\\n⋮\\nsL−1\\nsL\\ns1\\xa0\\xa0\\xa0s2\\xa0\\xa0\\xa0\\xa0s3\\xa0\\xa0⋯\\xa0\\xa0\\xa0⋯\\xa0\\xa0sL−1\\xa0sL\\nPush down\\nPush up\\nText Encoder\\nSequence Output ��\\nComponent Level Energy Distribution\\nComponent Level Energy Distribution\\nFigure 4: Demonstration of Energy based Pairwise Expert\\nfinally, the LOSSITC can be represented as:\\nLOSSITC = −1\\nB(\\nB\\nX\\nj=1\\npi2d\\nj log(qi2d\\ni ) +\\nB\\nX\\nj=1\\npd2i\\nj log(qd2i\\ni )).\\n(7)\\nOptimizing LOSSITM and LOSSITC enables model to capture\\nthe pairwise relevance of image-text pairs thus obtain the\\ncapability of prompt-aware visual information extraction.\\nEnergy based Pairwise Expert (EPE)\\nTo capture the semantical relevance of start and end bound-\\nary within a span, we introduce the idea of Energy Based\\nModel (LeCun et al. 2006) to give better span scores and\\nproposed the novel Energy based Pairwise Expert that pre-\\ndicts span based on pairwise stability.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 4}, page_content='EBM captures the correlation between variables by creat-\\ning a mapping of variable combinations to a scalar energy. In\\nthe inference phase, the model aims to find the combination\\nof variables that minimizes the energy of the system, in the\\ntraining phase, the model is forced to find an energy function\\nthat assigns lower energy values to paired combinations of\\nvariables while assigning higher energy values to unpaired\\ncombinations of variables.\\nThe core theory of EBM is learning an energy function\\nthat assigns lower energy values to paired combinations of\\nvariables while assigning higher energy values to unpaired\\ncombinations of variables.\\nFollowing the theory of EBM, we try to map the boundary\\npairing relations of a target span to a scalar energy, which in\\nturn quantitatively describe the intensity of the pairing rela-\\ntions at the boundary of a given span. Figure 4 demonstrates\\nthe learning process of EPE, in which we lower the energy of\\npositions with pairwise relevance while raising the energy of\\nothers.Specifically, we design the Component-level energy\\nfunction EC and System-level energy function ES. Let the\\noutput sequence of the text encoder be SO = {s1, s2 · · · sL},\\nL is the length of SO. We use EC to denote the pairing en-\\nergy of the span whose boundaries are the i-th and j-th to-\\nken, denoted as\\nEC(WS, WE, si, sj) = −(s⊤\\ni W ⊤\\nSi(WEjsj)).\\n(8)\\nwhere WS and WE are the parameters to be optimized. The\\nsmaller EC represents the more stable pairing relation, i.e.,\\nthe higher the probability that SO[i : j +1] is the target span.\\nFor the distribution of predicted span over the entire se-\\nquence, we use ES to measure the energy of the entire sys-\\ntem. Let the span distribution matrix Y ∈RL×L be the la-\\nbel of the target span, where yij ∈{0, 1} indicates whether\\nSO[i : j + 1] is the target span or not. The system level en-\\nergy can be expressed as\\nES(SO, Y ) =\\nL\\nX\\ni=0\\nL\\nX\\nj=i\\n(EC(si, sj)yij).\\n(9)\\nIn the inference phase, we select the span distribution ma-\\ntrix that minimizes the energy of the system. In the training\\nphase, we construct the loss function directly using EC as\\nLOSSEPE = −\\n1\\nL(L+1)\\n2\\nL\\nX\\ni=0\\nL\\nX\\nj=i\\n\\x12\\nyijlog(xij)\\n+ (1 −yij)log(1 −xij)\\n\\x13\\n,\\nxij = Sigmoid(−EC(si, sj)).\\n(10)\\nby optimizing LOSSEPE, the model will learn the energy\\nfunction EC that allocates lower energy to the paired sys-\\ntems (SO, Y ) while allocating higher energy to the unpaired\\nones.\\nBased on all the above narratives, the final training loss of\\nthe model is represented as\\nLOSS = λ1LOSSITM + λ2LOSSITC + λ3LOSSEPE.\\n(11)\\nwhere λ1, λ2, λ3 are predefined hyper-parameters\\nExperiments\\nExperimental settings\\nDataset\\nFollowing previous works, we adopt two widely\\nused benchmarks: Twitter2015 and Twitter2017 (Yu and\\nJiang 2019) to evaluate our proposed DQPSA. Besides,\\nwe employ another Political Twitter dataset1 from (Yang\\net al. 2021) for JMASA task. In pre-training stage, we use\\nCOCO2017 dataset and ImageNet dataset.\\nImplementation Details\\nIn our proposed model, we\\nchoose CLIP-ViT-bigG-14-laion2B-39B-b160k (Radford\\net al. 2021; Ilharco et al. 2021) as the frozen image en-\\ncoder, FSUIE-base (Peng et al. 2023) as the text encoder,\\nPrompt as Dual Query module is based on BERT-base (De-\\nvlin et al. 2019) architecture and pre-training parameter\\nand randomly initialized cross-attention layers. Refer to Ap-\\npendix A for detailed information of our backbone and se-\\nlections of hyper-parameters.\\nWe first employ a two-stage pre-training with 5 epochs\\nfor each stage. Then We trained model for 50 epochs with\\nan AdamW optimizer on the datasets of each task, and se-\\nlected the final model based on the performance on the de-\\nvelopment set. Associated code and pre-trained models will\\nbe made publicly available upon receipt.\\nEvaluation Metrics\\nFollowing previous work, we evalu-\\nate the performance of our model on the MATE and JMASA\\ntasks with Precision (P), Recall (R) and Micro-F1 (F1) score,\\nwhile on MASC task, we report Accuracy (Acc) and Micro-\\nF1 (F1) score for comparison.\\nPre-training\\nIn order to equip the PDQ with initial capa-\\nbility of prompt-controlled image comprehending, we em-\\nployed a two-stage pre-training before adapting to the spe-\\ncific MABSA task. For ImageNet data, we train model to\\npredict the entity class contained in the image under the\\nguidance of prompt, which helps the model to capture the\\nword level pairwise relevance between images and entities.\\nFor COCO data, the model is trained to predict the descrip-\\ntions that are relevant to the content of image under the guid-\\nance of prompt, from which the model will learn the sen-\\ntence level pairwise relevance between image and text. See\\nAppendix B for specific constructs of prompt, description\\nand text.\\nDuring phase 1, to prevent the initialized PDQ from de-\\ntracting the semantic comprehension of text encoder, we\\nfreeze all parameters except PDQ and EPE. While for subse-\\nquent training, we train all model parameters except image\\nencoder.\\nMain Results\\nResults of JMASA Task\\nTable 1 and\\n2 show the re-\\nsults of JMASA task. It can be seen that, by introducing the\\nPrompt as Dual Query module and the Energy based Pair-\\nwise Expert module, DQPSA significantly outperforms the\\nsub-optimal models (3.3 on Twitter2015 and 0.9 on Twit-\\nter2017) and achieves SOTA results. This demonstrates the\\n1Political Twitter for evaluaton is consistence with dataset re-\\nleased by (Yang, Na, and Yu 2022)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 5}, page_content='Methods\\nTwitter2015\\nTwitter2017\\nP\\nR\\nF1\\nP\\nR\\nF1\\nText-based\\nSPAN (Hu et al. 2019)\\n53.7\\n53.9\\n53.8\\n59.6\\n61.7\\n60.6\\nD-GCN (Chen, Tian, and Song 2020)\\n58.3\\n58.8\\n59.4\\n64.2\\n64.1\\n64.1\\nBART (Yan et al. 2021)\\n62.9\\n65.0\\n63.9\\n65.2\\n65.6\\n65.4\\nMulti-modal\\nUMT+TomBERT (Yu et al. 2020; Yu and Jiang 2019)\\n58.4\\n61.3\\n59.8\\n62.3\\n62.4\\n62.4\\nOSCGA+TomBERT (Wu et al. 2020b; Yu and Jiang 2019)\\n61.7\\n63.4\\n62.5\\n63.4\\n64.0\\n63.7\\nOSCGA-collapse (Wu et al. 2020b)\\n63.1\\n63.7\\n63.2\\n63.5\\n63.5\\n63.5\\nRpBERT-collapse (Sun et al. 2021)\\n49.3\\n46.9\\n48.0\\n57.0\\n55.4\\n56.2\\nUMT-collapse (Yu et al. 2020)\\n61.0\\n60.4\\n61.6\\n60.8\\n60.0\\n61.7\\nJML (Ju et al. 2021)\\n65.0\\n63.2\\n64.1\\n66.5\\n65.5\\n66.0\\nVLP-MABSA (Ling, Yu, and Xia 2022)\\n65.1\\n68.3\\n66.6\\n66.9\\n69.2\\n68.0\\nCMMT (Yang, Na, and Yu 2022)\\n64.6\\n68.7\\n66.5\\n67.6\\n69.4\\n68.5\\nAoM (Zhou et al. 2023)\\n67.9\\n69.3\\n68.6\\n68.4\\n71.0\\n69.7\\nDQPSA (ours)\\n71.7\\n72.0\\n71.9\\n71.1\\n70.2\\n70.6\\nTable 1: Results of Twitter2015 and Twitter2017, JMASA task. The best results are bold-typed.\\nMethods\\nPolitical-Twitter\\nP\\nR\\nF1\\nRoBERTa (Liu et al. 2019)\\n63.1\\n62.1\\n62.6\\nUMT+collapse (Yu et al. 2020)\\n54.9\\n54.7\\n54.8\\nJML (Ju et al. 2021)\\n63.6\\n59.4\\n61.4\\nUMT-RoBERTa (Yu et al. 2020; Liu et al. 2019)\\n63.8\\n63.4\\n63.6\\nJML-PoBERTa (Ju et al. 2021; Liu et al. 2019)\\n63.0\\n60.2\\n61.6\\nCMMT (Yang, Na, and Yu 2022)\\n65.3\\n65.7\\n65.5\\nDQPSA (ours)\\n68.3\\n65.5\\n66.9\\nTable 2: Results of Political Twitter, JMASA task. The best\\nresults are bold-typed.\\nMethods\\nTwitter2015\\nTwitter2017\\nP\\nR\\nF1\\nP\\nR\\nF1\\nRAN (Wu et al. 2020a)\\n80.5\\n81.5\\n81.0\\n90.7\\n90.7\\n90.0\\nUMT (Yu et al. 2020)\\n77.8\\n81.7\\n79.7\\n86.7\\n86.8\\n86.7\\nOSCGA (Wu et al. 2020b)\\n81.7\\n82.1\\n81.9\\n90.2\\n90.7\\n90.4\\nJML (Ju et al. 2021)\\n83.6\\n81.2\\n82.4\\n92.0\\n90.7\\n91.4\\nVLP-MABSA (Ling, Yu, and Xia 2022)\\n83.6\\n87.9\\n85.7\\n90.8\\n92.6\\n91.7\\nCMMT (Yang, Na, and Yu 2022)\\n83.9\\n88.1\\n85.9\\n92.2\\n93.9\\n93.1\\nAoM (Zhou et al. 2023)\\n84.6\\n87.9\\n86.2\\n91.8\\n92.8\\n92.3\\nDQPSA (ours)\\n88.3\\n87.1\\n87.7\\n95.1\\n93.5\\n94.3\\nTable 3: Results of Twitter2015 and Twitter2017, MATE\\ntask. The best results are bold-typed.\\neffectiveness of differentially leveraging visual information\\naccording to different analytical targets and focusing on the\\npairwise relevance of the target span.\\nCompared to the text-based models, the multi-modal\\nmodels perform better in general and DQPSA far exceeds\\nall text-based models. This verifies that image modal intro-\\nduced by our method does provide important supplementary\\ninformation for sentiment analysis.\\nCompare to the span based methods, EPE improves the\\nspan boundary recognition process. Instead of separately\\npredicting the start and end boundaries of span, EPE cap-\\nture the pairwise relevance of span boundaries, which pro-\\nvides a better understanding of the distribution of target\\nspans. Unlike directly using visual features as a prefix to\\ntextual inputs, PDQ module filters out visual noise and se-\\nlects visual information beneficial to the current task. It also\\nbridge the modality gap between visual and textual fea-\\ntures through stacked attention operation. Furthermore, our\\nmethod focuses on task-specific visual information rather\\nthan token-specific visual information, allowing for a more\\nmacro-level utilization of visual information. Compare to\\nthe span based methods, EPE improves the span boundary\\nrecognition process. Instead of separately predicting the start\\nand end boundaries of span, EPE capture the pairwise rele-\\nvance of span boundaries, which provides a better under-\\nstanding of the distribution of target spans. Unlike directly\\nusing visual features as a prefix to textual inputs, PDQ mod-\\nule filters out visual noise and selects visual information\\nbeneficial to the current task. It also bridge the modality\\ngap between visual and textual features through stacked at-\\ntention operation. Furthermore, our method focuses on task-\\nspecific visual information rather than token-specific visual\\ninformation, allowing for a more macro-level utilization of\\nvisual information.\\nIn contrast to approaches that use collapse labels or\\nwork with pipelines using different models, our approach\\nuses a unified framework for the MASC and MATE tasks,\\ncompletely eliminating the formal differences between two\\ntasks, helping the model to learn the interactions between\\nthe two sub-tasks in terms of semantic information and sen-\\ntiment with a more concise structure, resulting in better\\nperformance. Compared with methods that focus on token-\\nvarious visual information, our approach focuses on target-\\nvarious visual information to filter and leverage image fea-\\ntures from a more macroscopic perspective, achieving better\\nperformance while reducing computational effort.\\nResults of MATE and MASC Task\\nTable 3 and 4 show\\nthe results of the MATE and MASC task. Consistent with\\nthe results of JMASA task, DQPSA also achieves a signif-\\nicant improvement or competitive performance in the two\\nsub-tasks performance.\\nSpecifically,\\nDQPSA\\noutperforms\\nthe\\nsub-optimal\\nmethod in the MATE task by 1.5 on Twitter2015 and 2.0 on\\nTwitter2017, which suggests that our method helps model\\nto focus on the image information related to the aspects and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 6}, page_content='Methods\\nTwitter2015\\nTwitter2017\\nACC\\nF1\\nACC\\nF1\\nESAFN (Yu, Jiang, and Xia 2020)\\n73.4\\n67.4\\n67.8\\n64.2\\nTomBERT (Yu and Jiang 2019)\\n77.2\\n71.8\\n70.5\\n68.0\\nCapTrBERT (Khan and Fu 2021)\\n78.0\\n73.2\\n72.3\\n70.2\\nJML (Ju et al. 2021)\\n78.7\\n-\\n72.7\\n-\\nVLP-MABSA (Ling, Yu, and Xia 2022)\\n78.6\\n73.8\\n73.8\\n71.8\\nCMMT (Yang, Na, and Yu 2022)\\n77.9\\n-\\n73.8\\n-\\nAoM (Zhou et al. 2023)\\n80.2\\n75.9\\n76.4\\n75.0\\nDQPSA (ours)\\n81.1\\n81.1\\n75.0\\n75.0\\nTable 4: Results of different methods for MASC.\\nMethods\\nTwitter2015\\nTwitter2017\\nMATE\\nMASC\\nJMASA\\nMATE\\nMASC\\nJMASA\\nDQPSA\\n87.7\\n81.1\\n71.9\\n94.3\\n75.0\\n70.6\\nw/o EPE\\n86.3\\n80.9\\n69.1\\n92.5\\n73.1\\n66.8\\nw/o PDQ\\n87.4\\n78.4\\n69.9\\n93.8\\n69.7\\n65.6\\nw/o PDQ&EPE\\n84.4\\n76.4\\n63.3\\n90.8\\n68.6\\n64.1\\nTable 5: Results of Ablation Study\\nfilter out irrelevant information.\\nAs for MASC, we note that the model made a huge im-\\nprovement on Twitter2015 (5.2 over previous SOTA), but\\nonly achieved competitive results on Twitter2017. On the\\none hand, we think it is because the sentiment analysis data\\nin Twitter2017 is more challenging for it contains a signif-\\nicant number of unresolvable and unidentifiable symbols,\\nincluding emojis commonly used on Twitter, which posed\\na relative hard challenge for DQPSA, on the other hand,\\nmodel with large scale may better capture the correlation\\nbetween aspect and sentiment in difficult cases. However,\\nconsidering the difference in the number of trainable param-\\neters, the results of MASC are still convincing in proving the\\neffectiveness of our approach.\\nAblation Study\\nTo further investigate the contribution of each component\\nto model performance improvement, we conducted ablation\\nstudies on Twitter2015 and Twitter2017 for MATE, MASC,\\nand JMASA tasks, and the results of F1 scores are illustrated\\nin table 5. To examine the effect of Energy based Pairwise\\nExpert module, we follow the original setting of text en-\\ncoder to predict positions of start and end boundaries inde-\\npendently. To examine the effect of Prompt as Dual Query\\nmodule, we replace the visual query with a set of optimiz-\\nable tokens.\\nIt can be seen that removing both the PDQ and the EPE\\ndetracts somewhat from the model’s performance. Specifi-\\ncally, the model without PDQ achieves competitive perfor-\\nmance on the MATE task, but performance on the MASC\\nand JMASA tasks drops significantly. This is because the\\nprompt we constructed for the MASC task is different\\namong potential aspects, and optimizable tokens are not suf-\\nficient to capture this target-variability. However, the prompt\\nused by MATE is relatively fixed, so optimizable tokens can\\nfit the target requirements of MATE to a certain extent and\\nacts as soft prompt. The JMASA task, as a prolongation of\\nthe two subtasks, naturally shows a decrease in performance\\ndue to decrease on MATE and MASC. And this verifies that\\nMethods\\n14lap\\n14res\\n15res\\n16res\\nFSUIE-base\\n65.6\\n74.1\\n70.6\\n75.8\\nPSA (ours)\\n69.8\\n78.5\\n78.3\\n79.2\\nTable 6: Results on ASTE-DATA-V2 datasets (14lap, 14res,\\n15res, and 16res)\\nMethods\\nTwitter2015\\nTwitter2017\\nMATE\\nMASC\\nJMASA\\nMATE\\nMASC\\nJMASA\\nDQPSA\\n87.7\\n81.1\\n71.9\\n94.3\\n75.0\\n70.6\\nPSA\\n80.8\\n77.7\\n62.6\\n91.2\\n68.6\\n61.6\\nTable 7: Results of models w & w/o image modal\\nour Prompt as Dual Query can satisfy the differential fo-\\ncus on visual information of different targets in the MABSA\\ntopic.\\nFor model without EPE, we can see that the model shows\\na significant performance degradation on all three tasks. This\\nis due to the fact that EPE, as a module for the model to make\\nspan decisions, has an auxiliary effect on all the tasks per-\\nformed by the model. By introducing EPE, the model does\\nnot consider the boundaries of the target span in isolation,\\nbut makes full use of the pairwise relevance between the\\nboundaries of the spans, which further enriches the knowl-\\nedge learnt by the model, and thus improves the effect on\\nmultiple tasks.\\nConsidering the whole table, the introduction of PDQ and\\nEPE alone can significantly improve the model performance,\\nin which EPE leads to comprehensive performance improve-\\nment by capture the span pairwise relevance, while PDQ fo-\\ncuses more on guiding the model to filter visual information\\naccording to different target requirements thus is more effec-\\ntive in enhancing fine-grained MASC tasks. Simultaneous\\napplication of the two will further boost the model perfor-\\nmance.\\nCase Study\\nFor in-depth analysis, we also trained a PSA model based on\\nFSUIE-base without using Prompt as Dual Query module\\nand only receiving text as input. We apply PSA model to\\ndifferent case study as follows:\\nEPE on Sentiment Analysis of Plain Text\\nTo verify the\\nrobustness of Energy based Pairwise Expert, we apply PSA\\nto the ASTE-Data-V2 (Xu et al. 2020) of Aspect Sentiment\\nTriplet Extraction (ASTE) task, and compare it with the best\\nresult of the existing work FSUIE-base (Peng et al. 2023).\\nTable 6 shows the F1 scores of PSA on the ASTE task, it\\nshows that EPE delivers a huge performance improvement\\non all the four datasets and achieves the up-to-date SOTA\\nresults. This demonstrates the extensibility and robustness\\nof our proposed EPE, as well as its strong performance un-\\nder span recognition especially in cases involving sentiment\\nanalysis.\\nW/o Information from Image Modal\\nTo investigate\\nwhether our approach helps the model utilize visual in-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 7}, page_content='Models\\nTwitter2015\\nTwitter2017\\nP\\nR\\nF1\\nP\\nR\\nF1\\nDQPSA (ours)\\n81.1\\n81.1\\n81.1\\n75.0\\n75.0\\n75.0\\nVisualGLM-6B\\n69.2\\n64.6\\n66.8\\n57.2\\n52.0\\n54.5\\nChatGPT-3.5\\n66.3\\n66.3\\n66.3\\n58.9\\n58.9\\n58.9\\nTable 8: Results of comparison with LLMs on MASC task\\n(a)\\n\"Extract the aspects in the sentence\"\\n(b)\\n\"Sentiment of  \"contemplating Liren Ding (left)\" is [ positive, neutral, negative ]\"\\n(c)\\n\"Sentiment of  \"frustrated Nebo (right)\" is [ positive, neutral, negative ]\"\\n(a)\\n(b)\\n(c)\\nFigure 5: Visualization of Prompt as Dual Query\\nformation more efficiently, we apply PSA comparing to\\nDQPSA on Twitter2015 and Twitter2017 to examine the ef-\\nfects of the introduction of image modal on the model per-\\nformance. Table 7 reports F1 scores for PSA and DQPSA on\\nTwitter2015 and Twitter2017 for the three tasks.\\nIt can be seen from table 7 that: with Prompt as Dual\\nQuery module , our DQPSA significantly outperformed PSA\\nthat without image modal on a variety of tasks. This verifies\\nthat our proposed method is efficient in helping models to\\nexploit the rich information from images.\\nSentiment analysis compared to LLMs\\nRecently, Large\\nLanguage Models ( (Li et al. 2023b), (Du et al. 2022), etc)\\nhave shown impressive performance in a wide range of NLP\\ntasks with their powerful language perception and genera-\\ntion capabilities. In order to verify whether our model has an\\nadvantage over large models on the MABSA task, we com-\\npare the model’s performance with that of common LLMs\\ninclude VisualGLM-6B and ChatGPT-3.5. Since LLMs are\\nnot designed for identifying aspects in text and it is more dif-\\nficult to unify the output structure, we only test them on the\\nMASC task for a fair comparison. Table 8 shows the results\\nof DQPSA and other LLMs on the MASC task. The results\\nshow that our DQOSA using fewer parameters obtains sig-\\nnificantly better performance than LLMs, which also verifies\\nthe superiority and effectiveness of our proposed DQPSA\\nframework. It should be noted that due to the lack of a visual\\nmodule, ChatGPT 3.5 only receives text as input. Therefore,\\ndespite its impressive generation capability, the performance\\nof ChatGPT3.5 is not very outstanding. This also verifies\\nthat there is a great potential and importance for the integra-\\ntion and utilization of information from different modalities\\nin multi-modal tasks.\\nVisualization\\nTo further validate the effectiveness of our proposed method,\\nwe visualize the attention matrices in the last cross-attention\\nlayer to verify if Prompt as Dual Query module guide model\\nto focus on different visual information based on various\\nanalysis targets. Figure 5 presents the visualization results\\nalong with the corresponding prompts. It can be observed\\nthat for the MATE task, model exhibits a relatively uni-\\nform distribution of attention over the image, indicating that\\nmodel analyzes potential aspects by incorporating a larger\\nreceptive field. However, when analyzing different aspects\\nof sentiment, model demonstrates distinct focus towards dif-\\nferent regions of visual information. This result once again\\nconfirms that our proposed PDQ effectively captures the\\nvariations in the focus of visual information across different\\nanalysis targets.\\nConclusion\\nIn this paper, we proposed a novel framework, named\\nDQPSA, for Multi-modal Aspect-Based Sentiment Analysis\\n(MABSA). We use a well-designed Prompt as Dual Query\\nmodule that leveraging prompt as both visual query and lan-\\nguage query to extract the prompt-aware visual information,\\nthus satisfying various focus of different analysis targets on\\nthe visual information. Besides, we capture the boundaries\\npairing of analysis target with the perspective of Energy\\nbased Model and predict span based on pairwise stability\\nwith an Energy base Pairwise Expert module. Performance\\non three widely used benchmark datasets verifies that our\\nmethod outperforms previous methods.\\nReferences\\nCai, Y.; Cai, H.; and Wan, X. 2019. Multi-Modal Sarcasm\\nDetection in Twitter with Hierarchical Fusion Model.\\nIn\\nProceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, 2506–2515. Florence, Italy:\\nAssociation for Computational Linguistics.\\nChen, G.; Tian, Y.; and Song, Y. 2020. Joint Aspect Extrac-\\ntion and Sentiment Analysis with Directional Graph Con-\\nvolutional Networks.\\nIn Proceedings of the 28th Inter-\\nnational Conference on Computational Linguistics, 272–\\n279. Barcelona, Spain (Online): International Committee on\\nComputational Linguistics.\\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\\nBERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\\nneapolis, Minnesota: Association for Computational Lin-\\nguistics.\\nDu, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\\nTang, J. 2022. GLM: General Language Model Pretraining\\nwith Autoregressive Blank Infilling. In Proceedings of the\\n60th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), 320–335.\\nHu, M.; Peng, Y.; Huang, Z.; Li, D.; and Lv, Y. 2019. Open-\\nDomain Targeted Sentiment Analysis via Span-Based Ex-\\ntraction and Classification. In Proceedings of the 57th An-\\nnual Meeting of the Association for Computational Linguis-\\ntics, 537–546. Florence, Italy: Association for Computa-\\ntional Linguistics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 8}, page_content='Ilharco, G.; Wortsman, M.; Wightman, R.; Gordon, C.; Car-\\nlini, N.; Taori, R.; Dave, A.; Shankar, V.; Namkoong, H.;\\nMiller, J.; Hajishirzi, H.; Farhadi, A.; and Schmidt, L. 2021.\\nOpenCLIP. If you use this software, please cite it as below.\\nJing, H.; Li, Z.; Zhao, H.; and Jiang, S. 2021.\\nSeeking\\nCommon but Distinguishing Difference, A Joint Aspect-\\nbased Sentiment Analysis Model. In Moens, M.-F.; Huang,\\nX.; Specia, L.; and Yih, S. W.-t., eds., Proceedings of the\\n2021 Conference on Empirical Methods in Natural Lan-\\nguage Processing, 3910–3922. Online and Punta Cana, Do-\\nminican Republic: Association for Computational Linguis-\\ntics.\\nJu, X.; Zhang, D.; Xiao, R.; Li, J.; Li, S.; Zhang, M.; and\\nZhou, G. 2021. Joint Multi-modal Aspect-Sentiment Anal-\\nysis with Auxiliary Cross-modal Relation Detection.\\nIn\\nProceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, 4395–4405. Online and\\nPunta Cana, Dominican Republic: Association for Compu-\\ntational Linguistics.\\nKhan, Z.; and Fu, Y. 2021. Exploiting BERT for Multimodal\\nTarget Sentiment Classification through Input Space Trans-\\nlation. In Shen, H. T.; Zhuang, Y.; Smith, J. R.; Yang, Y.;\\nC´esar, P.; Metze, F.; and Prabhakaran, B., eds., MM ’21:\\nACM Multimedia Conference, Virtual Event, China, Octo-\\nber 20 - 24, 2021, 3034–3042. ACM.\\nLeCun, Y.; Chopra, S.; Hadsell, R.; Ranzato, M.; and Huang,\\nF. 2006. A tutorial on energy-based learning. Predicting\\nstructured data, 1(0).\\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. C. H. 2023a. BLIP-\\n2: Bootstrapping Language-Image Pre-training with Frozen\\nImage Encoders and Large Language Models.\\nCoRR,\\nabs/2301.12597.\\nLi, Z.; Zhang, S.; Zhao, H.; Yang, Y.; and Yang, D. 2023b.\\nBatGPT: A Bidirectional Autoregessive Talker from Gener-\\native Pre-trained Transformer. CoRR, abs/2307.00360.\\nLing, Y.; Yu, J.; and Xia, R. 2022. Vision-Language Pre-\\nTraining for Multimodal Aspect-Based Sentiment Analysis.\\nIn Proceedings of the 60th Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1: Long Papers),\\n2149–2159. Dublin, Ireland: Association for Computational\\nLinguistics.\\nLiu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.\\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\\nApproach. CoRR, abs/1907.11692.\\nLu, D.; Neves, L.; Carvalho, V.; Zhang, N.; and Ji, H. 2018.\\nVisual Attention Model for Name Tagging in Multimodal\\nSocial Media. In Proceedings of the 56th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1:\\nLong Papers), 1990–1999. Melbourne, Australia: Associa-\\ntion for Computational Linguistics.\\nPeng, L.; Li, Z.; and Zhao, H. 2021. Sparse Fuzzy Attention\\nfor Structured Sentiment Analysis. CoRR, abs/2109.06719.\\nPeng, T.; Li, Z.; Zhang, L.; Du, B.; and Zhao, H. 2023.\\nFSUIE: A Novel Fuzzy Span Mechanism for Universal In-\\nformation Extraction.\\nIn Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), 16318–16333. Toronto, Canada:\\nAssociation for Computational Linguistics.\\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\\nKrueger, G.; and Sutskever, I. 2021.\\nLearning Transfer-\\nable Visual Models From Natural Language Supervision. In\\nMeila, M.; and Zhang, T., eds., Proceedings of the 38th In-\\nternational Conference on Machine Learning, ICML 2021,\\n18-24 July 2021, Virtual Event, volume 139 of Proceedings\\nof Machine Learning Research, 8748–8763. PMLR.\\nSun, L.; Wang, J.; Zhang, K.; Su, Y.; and Weng, F. 2021.\\nRpBERT: A Text-image Relation Propagation-based BERT\\nModel for Multimodal NER.\\nIn Thirty-Fifth AAAI Con-\\nference on Artificial Intelligence, AAAI 2021, Thirty-Third\\nConference on Innovative Applications of Artificial Intel-\\nligence, IAAI 2021, The Eleventh Symposium on Educa-\\ntional Advances in Artificial Intelligence, EAAI 2021, Vir-\\ntual Event, February 2-9, 2021, 13860–13868. AAAI Press.\\nWang, X.; Cai, J.; Jiang, Y.; Xie, P.; Tu, K.; and Lu, W.\\n2022a. Named Entity and Relation Extraction with Multi-\\nModal Retrieval. In Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2022, 5925–5936. Abu Dhabi,\\nUnited Arab Emirates: Association for Computational Lin-\\nguistics.\\nWang, X.; Gui, M.; Jiang, Y.; Jia, Z.; Bach, N.; Wang, T.;\\nHuang, Z.; and Tu, K. 2022b. ITA: Image-Text Alignments\\nfor Multi-Modal Named Entity Recognition.\\nIn Carpuat,\\nM.; de Marneffe, M.; and Ru´ız, I. V. M., eds., Proceed-\\nings of the 2022 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human\\nLanguage Technologies, NAACL 2022, Seattle, WA, United\\nStates, July 10-15, 2022, 3176–3189. Association for Com-\\nputational Linguistics.\\nWu, H.; Cheng, S.; Wang, J.; Li, S.; and Chi, L. 2020a.\\nMultimodal Aspect Extraction with Region-Aware Align-\\nment Network. In Zhu, X.; Zhang, M.; Hong, Y.; and He,\\nR., eds., Natural Language Processing and Chinese Com-\\nputing - 9th CCF International Conference, NLPCC 2020,\\nZhengzhou, China, October 14-18, 2020, Proceedings, Part\\nI, volume 12430 of Lecture Notes in Computer Science,\\n145–156. Springer.\\nWu, Z.; Zheng, C.; Cai, Y.; Chen, J.; Leung, H.; and Li, Q.\\n2020b. Multimodal Representation with Embedded Visual\\nGuiding Objects for Named Entity Recognition in Social\\nMedia Posts. In Chen, C. W.; Cucchiara, R.; Hua, X.; Qi,\\nG.; Ricci, E.; Zhang, Z.; and Zimmermann, R., eds., MM\\n’20: The 28th ACM International Conference on Multime-\\ndia, Virtual Event / Seattle, WA, USA, October 12-16, 2020,\\n1038–1046. ACM.\\nXu, L.; Li, H.; Lu, W.; and Bing, L. 2020. Position-Aware\\nTagging for Aspect Sentiment Triplet Extraction. In Pro-\\nceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP), 2339–2349. On-\\nline: Association for Computational Linguistics.\\nXu, N.; Mao, W.; and Chen, G. 2019.\\nMulti-Interactive\\nMemory Network for Aspect Based Multimodal Sentiment\\nAnalysis. In The Thirty-Third AAAI Conference on Artificial'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2023-12-18T01:36:06+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/DQPSA.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-18T01:36:06+00:00', 'trapped': '', 'modDate': 'D:20231218013606Z', 'creationDate': 'D:20231218013606Z', 'page': 9}, page_content='Intelligence, AAAI 2019, The Thirty-First Innovative Appli-\\ncations of Artificial Intelligence Conference, IAAI 2019, The\\nNinth AAAI Symposium on Educational Advances in Artifi-\\ncial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, Jan-\\nuary 27 - February 1, 2019, 371–378. AAAI Press.\\nYan, H.; Dai, J.; Ji, T.; Qiu, X.; and Zhang, Z. 2021.\\nA\\nUnified Generative Framework for Aspect-based Sentiment\\nAnalysis. In Proceedings of the 59th Annual Meeting of the\\nAssociation for Computational Linguistics and the 11th In-\\nternational Joint Conference on Natural Language Process-\\ning (Volume 1: Long Papers), 2416–2429. Online: Associa-\\ntion for Computational Linguistics.\\nYang, L.; Na, J.; and Yu, J. 2022. Cross-Modal Multitask\\nTransformer for End-to-End Multimodal Aspect-Based Sen-\\ntiment Analysis. Inf. Process. Manag., 59(5): 103038.\\nYang, L.; Yu, J.; Zhang, C.; and Na, J. 2021. Fine-Grained\\nSentiment Analysis of Political Tweets with Entity-Aware\\nMultimodal Network.\\nIn Toeppe, K.; Yan, H.; and Chu,\\nS. K., eds., Diversity, Divergence, Dialogue - 16th Interna-\\ntional Conference, iConference 2021, Beijing, China, March\\n17-31, 2021, Proceedings, Part I, volume 12645 of Lecture\\nNotes in Computer Science, 411–420. Springer.\\nYu, J.; and Jiang, J. 2019.\\nAdapting BERT for Target-\\nOriented Multimodal Sentiment Classification. In Kraus, S.,\\ned., Proceedings of the Twenty-Eighth International Joint\\nConference on Artificial Intelligence, IJCAI 2019, Macao,\\nChina, August 10-16, 2019, 5408–5414. ijcai.org.\\nYu, J.; Jiang, J.; and Xia, R. 2020. Entity-Sensitive Attention\\nand Fusion Network for Entity-Level Multimodal Sentiment\\nClassification. IEEE ACM Trans. Audio Speech Lang. Pro-\\ncess., 28: 429–439.\\nYu, J.; Jiang, J.; Yang, L.; and Xia, R. 2020.\\nImproving\\nMultimodal Named Entity Recognition via Entity Span De-\\ntection with Unified Multimodal Transformer. In Proceed-\\nings of the 58th Annual Meeting of the Association for Com-\\nputational Linguistics, 3342–3352. Online: Association for\\nComputational Linguistics.\\nZhao, G.; Dong, G.; Shi, Y.; Yan, H.; Xu, W.; and Li, S.\\n2022. Entity-level Interaction via Heterogeneous Graph for\\nMultimodal Named Entity Recognition. In Findings of the\\nAssociation for Computational Linguistics: EMNLP 2022,\\n6345–6350. Abu Dhabi, United Arab Emirates: Association\\nfor Computational Linguistics.\\nZhou, R.; Guo, W.; Liu, X.; Yu, S.; Zhang, Y.; and Yuan,\\nX. 2023. AoM: Detecting Aspect-oriented Information for\\nMultimodal Aspect-Based Sentiment Analysis.\\nIn Find-\\nings of the Association for Computational Linguistics: ACL\\n2023, 8184–8196. Toronto, Canada: Association for Com-\\nputational Linguistics.\\nZou, H.; Yang, J.; and Wu, X. 2021. Unsupervised Energy-\\nbased Adversarial Domain Adaptation for Cross-domain\\nText Classification. In Zong, C.; Xia, F.; Li, W.; and Nav-\\nigli, R., eds., Findings of the Association for Computational\\nLinguistics: ACL-IJCNLP 2021, 1208–1218. Online: Asso-\\nciation for Computational Linguistics.\\nAppendix\\nA. Backbone inrtoduction&hyper-parameters\\nselection\\nSpecifically, the frozen image encoder contains 48 layers of\\n104-head Transformer layers with hidden size of 1664, the\\nPrompt as Dual Query module and the text encoder have\\n12 layers of 12-head Transformer layers and a hidden size\\nof 768. Hyper-parameters used during training are shown in\\ntable 9\\nTraining Stage\\nλ1\\nλ2\\nλ3\\nlearing rate\\nPretraing-Stage1\\n2.0\\n2.0\\n1.0\\n5e-5\\nPretraing-Stage2\\n1.0\\n1.0\\n1.0\\n3e-5\\nFinetuning\\n0.1\\n0.1\\n1.0\\n2e-5\\nTable 9: Hyper-parameters used during training\\nB. Data construction for pre-training\\nPrompt\\n“Provide a description for image.”\\nDescription\\nthe label description of image\\nText\\nlabel\\ndescription,\\nIrrelevant\\ndescription1, Irrelevant description2,\\nIrrelevant description3.\\nTable 10: COCO pre-training data construction\\nCOCO dataset\\nFor COCO data, each image has corre-\\nsponding five descriptions. We randomly select one relevant\\ndescription and three irrelevant descriptions to splice as the\\ninput text, the model needs to predict the descriptions that\\nare relevant to the content of the picture under the guidance\\nof prompt. Training data is constructed as table 10.\\nPrompt\\n“What does this image contains.”\\nDescription\\n“It’s an image of {image label}.”\\nText\\nCorrect label, fake label1, fake label2\\n· · · fake label9.\\nTable 11: ImageNet pre-training data construction\\nImageNet dataset\\nFor ImageNet data, which contains\\n1000 classes of images and corresponding labels, we select\\n100 of these classes to construct the pre-training data, and\\ndivide them into 10 groups equally. For the same group of\\ndata, the input text is constructed as a splice of ten class\\nnames, and the model needs to predict the entity class con-\\ntained in the current image under the guidance of prompt.\\nTraining data is constructed as table 11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 0}, page_content='AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1\\nWenya Guo1∗Xumeng Liu1\\nShenglong Yu1\\nYing Zhang1\\nXiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented\\nMethod (AoM) to detect aspect-relevant seman-\\ntic and sentiment information. Specifically, an\\naspect-aware attention module is designed to\\nsimultaneously select textual tokens and im-\\nage blocks that are semantically related to the\\naspects. To accurately aggregate sentiment in-\\nformation, we explicitly introduce sentiment\\nembedding into AoM, and use a graph convo-\\nlutional network to model the vision-text and\\ntext-text interaction. Extensive experiments\\ndemonstrate the superiority of AoM to existing\\nmethods. The source code is publicly released\\nat https://github.com/SilyRab/AoM.\\n1\\nIntroduction\\nAs an important and promising task in the field of\\nsentiment analysis, Multimodal Aspect-Based Sen-\\ntiment Analysis (MABSA) has attracted increasing\\nattention (Lv et al., 2021; Ju et al., 2021). Given an\\nimage and corresponding text, MABSA is defined\\nas jointly extracting all aspect terms from image-\\ntext pairs and predicting their sentiment polarities\\n(Ju et al., 2021).\\nIn this scenario of fine-grained sentiment recog-\\nnition for multimodal information, the input image-\\ntext pairs are always complex. (1) The semantics\\nof sentence is complex which adds sentiment con-\\nfusion among different aspects. Take Figure 1 as an\\n∗Corresponding author.\\nComplain about Kyoto a bit\\nand someone takes you to\\nsee the mayor. Interesting!\\nMayor Kadokawa, thanks\\nfor your time!\\nregion 1\\nregion 2\\nAspect\\nKyoto\\nmayor\\nMayor Kadokawa\\nSentiment\\nNegative\\nNeutral\\nPositive\\nFigure 1: An example of MABSA task, including the as-\\npects, their corresponding descriptions, and sentiments.\\nexample, there are 3 aspects in the sentence with 3\\ndifferent sentiments, The sentiment of “mayor\" can\\nbe easily confused by the keyword, “Interesting”,\\nwhich is of positive sentiment. (2) The images con-\\ntain a large amount of detailed information, and the\\nvisual contents are usually related to only one or\\nseveral of the aspects. For example, as shown in\\nFigure 1, the objects in red boxes are more helpful\\nin analyzing the sentiment of “Mayor Kadokawa”\\nthan the other aspects. The complex input greatly\\nchallenges the recognition of aspect-based senti-\\nment.\\nConsidering the multimodal input, existing meth-\\nods are typically dedicated to associated visual and\\ntextual contents (Ju et al., 2021; Ling et al., 2022;\\nYang et al., 2022). Ju et al. (2021) uses image-\\ntext relation to evaluate the contribution of visual\\ncontents to aspect sentiment, based on which to de-\\ntermine whether the image is involved in sentiment\\nanalysis. Ling et al. (2022) and Yang et al. (2022)\\nalign visual representations of objects and their at-\\ntributes with corresponding textual contents. To\\nsummarize, the whole image is directly associated\\nwith textual content in these methods. Intuitively,\\nwithout aligning image blocks to corresponding as-\\npects, the coarse whole-image-text association can\\nintroduce aspect-irrelevant visual noise, which fur-\\nther hinders aspect sentiment analysis. In addition,\\nthe performance can be further impacted by the\\narXiv:2306.01004v1  [cs.CL]  31 May 2023'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 1}, page_content='textual noise from the confusion among different\\naspects.\\nIn this paper, we propose an Aspect-oriented\\nMethod (AoM) to mitigate aforementioned noises\\nfrom both image and text. AoM can detect aspect-\\nrelevant information from perspectives of both se-\\nmantics and sentiment. There are two key modules\\nin AoM: Aspect-Aware Attention Module (A3M)\\nfor semantically fine-grained image-text alignment\\nand Aspect-Guided Graph Convolutional Network\\n(AG-GCN) for sentiment information aggregation.\\nIn A3M, we first extract aspect features associated\\nwith each visual and textual token. And then aspect-\\nrelevant token representations are computed based\\non their relevance to the corresponding aspect fea-\\ntures. In AG-GCN, we first explicitly add sentiment\\nembeddings to the obtained representations of vi-\\nsual and textual tokens. A multimodal weighted-\\nassociation matrix is constructed containing aspect-\\nto-image-block similarity and word-to-word depen-\\ndency. Then we use a graph convolutional network\\nto aggregate sentiment information according to\\nthe constructed multimodal matrix.\\nThe contributions can be summarized as follows:\\n(1) We propose an aspect-oriented network to\\nmitigate the visual and textual noises from the com-\\nplex image-text interactions.\\n(2) We design an aspect-aware attention mod-\\nule and an aspect-guided graph convolutional net-\\nwork to effectively detect aspect-relevant multi-\\nmodal contents from the perspectives of semantic\\nand sentiment, respectively.\\n(3) Experiments on two benchmark datasets, in-\\ncluding Twitter2015 and Twitter2017, show that\\nour approach generally outperforms the state-of-\\nthe-art methods.\\n2\\nRelated Work\\nIn this section, we review the existing methods for\\nboth ABSA and MABSA.\\n2.1\\nAspect-based Sentiment Analysis\\nIn the past few years, Aspect-Based Sentiment\\nAnalysis (ABSA) in the textual fields has attracted\\nmuch attention and gained mature research (Chen\\nand Qian, 2020; Oh et al., 2021; Xu et al., 2020).\\nOn the one hand, most recent works are based on\\nthe pre-trained language model BERT because of\\nits remarkable performance in many NLP tasks\\n(Liang et al., 2022a). On the other hand, some\\nrecent efforts focus on modeling the dependency\\nrelationship between aspects and their correspond-\\ning descriptions, in which graph convolutional net-\\nworks (GCNs) (Chen et al., 2022; Liang et al.,\\n2022b, 2020; Li et al., 2021a; Pang et al., 2021)\\nor graph attention networks (GATs) (Yuan et al.,\\n2020) over dependency with the syntactic structure\\nof a sentence are fully exploited.\\n2.2\\nMultimodal Aspect-based Sentiment\\nAnalysis\\nWith the enrichment of multimodal users’ posts\\nin social media, researchers find that images of-\\nfer great supplementary information in aspect term\\nextraction (Wu et al., 2020a; Zhang et al., 2018;\\nAsgari-Chenaghlu et al., 2021) and sentiment anal-\\nysis (Wu et al., 2022; Zhang et al., 2022; Li\\net al., 2021b; Hazarika et al., 2020; Cai et al.,\\n2019). Thus, Multimodal Aspect-based Sentiment\\nAnalysis (MABSA) begins to be widely studied.\\nMABSA task can be divided into two independent\\nsub-tasks, i.e., Multimodal Aspect Term Extraction\\n(MATE) and Multimodal Aspect-oriented Senti-\\nment Classification (MASC). The former extracts\\nall aspect terms in the sentence at the prompt of the\\nimage, and the latter predicts the sentiment polari-\\nties for the aspects.\\nJu et al. (2021) first realizes MABSA in a unified\\nframework and designs an auxiliary cross-modal\\nrelation detection to control whether the visual in-\\nformation will be used in prediction. For captur-\\ning cross-modal alignment, Ling et al. (2022) con-\\nstructs a generative multimodal architecture based\\non BART for both vision-language pre-training and\\nthe downstream MABSA tasks. Yang et al. (2022)\\ndynamically controls the contributions of the vi-\\nsual information to different aspects via the trick\\nthat the lower confidence of the results predicted\\nby purely textual is, the more contributions from\\nimages will be considered.\\nOn the one hand, the above methods ignore the\\nalignment of fine-grained visual blocks and the\\ncorresponding aspects, which introduce irrelevant\\nvisual noise. On the other hand, modeling of syntax\\ndependency and sentiment information for aspect\\ndescriptions is absent in these methods, which is\\nproved important in sentiment analysis (Liang et al.,\\n2022a; Kalaivani et al., 2022; Xu et al., 2022).\\nTo tackle the aforementioned issues, we pro-\\npose an aspect-oriented model consisting of Aspect-\\nAware Attention Module and Aspect-Guided Graph\\nConvolutional Network which respectively work'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 2}, page_content='A3M\\nComplain about Kyoto ... time ! \\nSenticNet\\nlinear layer (1 to 768)\\ncosine  similarity\\nSenticNet\\nAG-GCN\\nGCN Layers\\nGCN Layers\\natomic feature\\nsoftmax linear layer\\nsigmoid linear layer\\nℎ𝑡𝑡\\nℎ1\\n𝐶𝐶𝐶𝐶\\nℎ𝑡𝑡\\n...\\nℎ𝑡𝑡\\nℎ𝑡𝑡\\nℎ2\\n𝐶𝐶𝐶𝐶\\nℎ𝑙𝑙\\n𝐶𝐶𝐶𝐶\\n𝛼𝛼𝑡𝑡1\\n𝛼𝛼𝑡𝑡2\\n𝛼𝛼𝑡𝑡𝑙𝑙\\nℎ𝑡𝑡\\n𝐴𝐴\\nℎ𝑡𝑡\\n𝛽𝛽𝑡𝑡\\n1 −𝛽𝛽𝑡𝑡\\n\\u0de1ℎ𝑡𝑡\\n...\\n\\u0de2\\nℎ𝑉𝑉𝑚𝑚\\n...\\n\\u0de2\\nℎ𝑇𝑇𝑛𝑛\\n3\\n3         NEG       13          13        NEU       17          18        POS      <eos>\\nBART decoder\\n<bos>  <AESC> Kyoto Kyoto NEG\\nmayor\\nmayor NEU Mayor Kadokawa POS\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\naspect-guided dependency, D\\nA3M\\nKyoto\\nMayor\\nKadokawa\\nKyoto\\nMayor\\nKadokawa\\nMayor\\nKadokawa\\n\\u0de2\\nℎ𝑉𝑉1\\n\\u0de2\\nℎ𝑉𝑉2\\n\\u0de2\\nℎ𝑉𝑉3\\n\\u0de2\\nℎ𝑉𝑉4\\n\\u0de2\\nℎ𝑉𝑉5\\n\\u0de2\\nℎ𝑉𝑉6\\n\\u0de2\\nℎ𝑉𝑉𝑚𝑚\\nComplain\\nKyoto\\n\\u0de2\\nℎ𝑇𝑇1\\nKyoto\\nKyoto\\n\\u0de2\\nℎ𝑇𝑇3\\nmayor\\nmayor\\n\\u0de2\\nℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n\\u0de2\\nℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n\\u0de2\\nℎ𝑇𝑇18\\nthanks\\nthanks\\n\\u0de2\\nℎ𝑇𝑇20\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos>\\n<img>\\n</img>\\nBART encoder\\nResNet                                        BART embedding\\nmask\\n𝒘𝒘𝒊𝒊\\n...\\n𝑤𝑤𝑆𝑆1\\n𝑤𝑤𝑆𝑆2\\n𝑤𝑤𝑆𝑆3\\n𝑤𝑤𝑆𝑆𝑛𝑛\\nAG-GCN\\nGCN Layers\\nGCN Layers\\nweighted association matrix, A\\nS from SenticNet\\nH\\nweighted association matrix, A\\n𝑺𝑺\\n\\u0de1𝑯𝑯S\\n\\u0de1𝑯𝑯\\n\\u0de1𝑯𝑯S\\n෩𝑯𝑯\\n\\u0de1𝑯𝑯\\nHS\\nFigure 2: The overview of our proposed aspect-oriented model AoM.\\nto capture semantic information by fine-grained\\nimage-text alignment and effectively aggregate\\naspect-relevant sentiment information.\\n3\\nMethodology\\n3.1\\nOverview\\nTask Definition.\\nFormally, given a tweet that\\ncontains an image V and a sentence with n\\nwords S = (w1, w2, ..., wn), our goal is to ac-\\nquire the sequence Y representing all aspects\\nand their associated sentiment polarities.\\nWe\\nformulate the output of MABSA as Y\\n=\\n[as\\n1, ae\\n1, s1, ..., as\\ni, ae\\ni, si, ...as\\nk, ae\\nk, sk], where as\\ni, ae\\ni\\nand si depict the start index, end index of the i-th\\naspect and its sentiment polarity in the tweet, and\\nk is the number of aspects.\\nModel preview.\\nFigure 2 shows the overview\\nof our model architecture, which builds on an\\nencoder-decoder architecture based on BART\\n(Lewis et al., 2019). Between the encoder and\\nthe decoder of BART, we creatively implement\\nthe Aspect-Aware Attention Module (A3M) and\\nAspect-Guided Graph Convolutional Network (AG-\\nGCN) to align the textual aspect to its associated\\nvisual blocks and textual description, simultane-\\nously mitigate interference both from semantics\\nand sentiment among different aspects. In the fol-\\nlowing subsections, we will illustrate the details of\\nthe proposed model.\\nFeature Extractor. The initial word embeddings\\nare obtained from the pre-trained BART due to\\nits excellent ability of textual representation. The\\nembeddings of visual blocks are obtained by pre-\\nprocessing via ResNet (Chen et al., 2014) following\\n(Yu et al., 2019). We consider every feature of a\\nvisual block or word token as an atomic feature. We\\nadd <img> and </img> before and after the visual\\nfeatures, <bos> and <eos> for the textual features.\\nThen, we concatenate the multimodal features as\\nX which is the input of BART encoder.\\nWe can get the multimodal hidden state H =\\n{hV\\n0 , ...hV\\ni , ...hV\\nm, hT\\n0 , ..., hT\\nj , ...hT\\nn} with m visual\\nblocks and n words, where hV\\ni and hT\\nj refer to\\nfeatures of the i-th visual block and the j-th word\\nin the sentence.\\n3.2\\nAspect-Aware Attention Module (A3M)\\nSince aspects are not specially modeled by BART\\nencoder, we creatively design the Aspect-Aware\\nAttention Module (A3M) aiming to capture aspect-\\nrelevant semantic information. For this purpose, we\\nalign the multimodal information of target objects\\nand filter out the semantic noise from images.\\nFirst, as aspects are usually noun phrases from\\nthe sentences, we extract those phrases as the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 3}, page_content='candidate aspects (CA) with the NLP tool Spacy1.\\nAnd from the hidden state H of the BART encoder,\\nwe obtain the features of all candidate aspects de-\\nnoted as HCA = {hCA\\n1\\n, ..., hCA\\ni\\n, ..., hCA\\nl\\n}, where\\nl is the number of noun phrases in the sentence. To\\nget the relationship between candidate aspects and\\natomic features, we implement an attention-based\\nmechanism guided by the candidate aspects. Given\\nthe t-th hidden feature ht, its attention distribution\\nαt over k candidate aspects is obtained by:\\nZt = tanh((WCAHCA + bCA) ⊕(WHht + bH)),\\n(1)\\nαt = softmax(WαZt + bα),\\n(2)\\nwhere Zt ∈R2d×k is the comprehensive feature\\nextracted from both the candidate aspects and the\\nhidden states. HCA ∈Rd×k denotes the features\\nof candidate aspects. WCA ∈Rd×d, WH ∈Rd×d,\\nWα ∈R1×2d, bCA, bH and bα are the learned\\nparameters.⊕is an operator between a matrix and a\\nvector, where the vector is repeated into the appro-\\npriate size to concatenate with the matrix. We then\\nget the aspect-related hidden feature hA\\nt by calcu-\\nlating the weighted sum of all candidate aspects\\nfollowing the below equation:\\nhA\\nt =\\nk\\nX\\ni\\nαt,ihCA\\ni\\n.\\n(3)\\nFor example, if a visual block is strongly associ-\\nated with the j-th aspect, the corresponding αt,j is\\napproximately 1. hA\\nt would be equal to the aspect\\nsemantically. And if the visual block is not related\\nto any specific candidate aspects, both αt and hA\\nt\\nwould be zero-like vectors of no information.\\nConsidering that not every visual block can be\\nused for prediction, βt is learned to add up the\\natomic feature ht and its aspect-related hidden fea-\\nture hA\\nt . Details are as follows:\\nβt = sigmoid(Wβ[W1ht; W2hA\\nt ] + bβ),\\n(4)\\nˆht = βtht + (1 −βt)hA\\nt ,\\n(5)\\nwhere Wβ, W1, W2, bβ are parameters, and [; ]\\ndenotes the concatenation operator for vectors.\\nˆht ∈ˆH is the final output of A3M after the seman-\\ntic alignment and the noise reduction procedure.\\nThus we get the noiseless and aligned information\\nfor every atomic feature.\\nPre-training To align the two modalities and re-\\nduce noise, we conduct a pre-training task in A3M.\\n1Spacy: https://spacy.io/\\naverage\\nrelated or unrelated\\nH\\nimage-text relation classification layer\\nA3M\\nKyoto\\nMayor\\nKadokawa\\nKyoto\\nMayor\\nKadokawa\\nMayor\\nKadokawa\\n\\u0de2\\nℎ𝑉𝑉1\\n\\u0de2\\nℎ𝑉𝑉2\\n\\u0de2\\nℎ𝑉𝑉3\\n\\u0de2\\nℎ𝑉𝑉4\\n\\u0de2\\nℎ𝑉𝑉5\\n\\u0de2\\nℎ𝑉𝑉6\\n\\u0de2\\nℎ𝑉𝑉𝑉𝑉\\nComplain\\nKyoto\\n\\u0de2\\nℎ𝑇𝑇1\\nKyoto\\nKyoto\\n\\u0de2\\nℎ𝑇𝑇3\\nmayor\\nmayor\\n\\u0de2\\nℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n\\u0de2\\nℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n\\u0de2\\nℎ𝑇𝑇18\\nthanks\\nthanks\\n\\u0de2\\nℎ𝑇𝑇20\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos>\\n<img>\\n</img>\\nBART encoder\\nResNet                                        BART embedding\\nFigure 3: The framework of the pre-training task.\\nSpecifically, we detect the image-text relationship\\non the datasets TRC (Vempala and Preo¸tiuc-Pietro,\\n2019) as illustrated by Figure 3. We first obtain the\\naverage feature of image blocks from the output of\\nA3M and then pass it to a fully connected softmax\\nlayer, which outputs a probability distribution over\\nwhether the image is related to the text. Finally, we\\nuse cross entropy loss to train our model.\\n3.3\\nAspect-Guided Graph Convolutional\\nNetwork (AG-GCN)\\nThe aspect-focused interaction between visual\\nmodality and textual modality in A3M concentrates\\non the context semantics, and that is not adequate\\nfor MABSA. Sentiment interference among dif-\\nferent aspects still exists and influences sentiment\\nprediction. Thus, we design the Aspect-Guided\\nGraph Convolutional Network (AG-GCN) module\\nto introduce external sentiment information and\\nmitigate emotional confusion among different as-\\npects to a certain extent.\\nSpecifically, for word wi in the sentence, we gain\\nits affective score wS\\ni from SenticNet (Ma et al.,\\n2018) and project it to the space with the same\\ndimension as hA\\nt , with si obtained. Then we add\\nthe sentiment feature si to the output of A3M:\\nwS\\ni = SenticNet(wi),\\n(6)\\nsi = WSwS\\ni + bS,\\n(7)\\nhS\\ni = ˆhi + si,\\n(8)\\nwhere WS, bS are the learned parameters. hS\\ni is the\\nfeature with affective knowledge.\\nNext, we build a boolean dependency matrix\\nD among visual blocks and words. First, for the\\nword-to-word part, submatrix DTT representing'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 4}, page_content='Complain\\nabout\\ntakes\\nKyoto\\nbit\\na someone you\\n.\\nto\\nthe\\nmayor\\nsee\\nand\\nInteresting\\n!\\nthanks\\n,\\n!\\nKadokawa\\nMayor\\nfor\\ntime\\nyour\\nFigure 4: The dependency tree of the example men-\\ntioned in the introduction.\\nthe dependency tree2 of the input sentence like Fig-\\nure 4. If two words can be associated within two\\ngenerations, the element of DTT would be set to\\n1, otherwise 0 instead. For example, “Kyoto” is as-\\nsociated with “bit” (child),“a” (grandchild),“about”\\n(father) and “Complain” (grandfather). Second, the\\nvisual dependency submatrix DV V is initialized\\nas a diagonal matrix. And as for the word-image-\\nblock dependency, denoted as DTV and equaled\\nto DT\\nV T , we set all the elements in the i-th line of\\nDTV to 1 if the i-th word is an aspect, otherwise 0.\\nAnd the matrix D is defined as:\\nD =\\n\\x14DV V\\nDV T\\nDTV\\nDTT\\n\\x15\\n,\\n(9)\\nConsidering the different importance of differ-\\nent dependencies, we attach weights onto D with\\ncosine similarity among ˆhi as follows:\\nAij = DijFcosine_similarity( ˆhi, ˆhj),\\n(10)\\nwhere both D, A ∈R(m+n)×(m+n), and A is the\\nweighted association matrix.\\nAG-GCN takes HS from Eq.8 as initial node\\nrepresentations in the graph. For the i-th node at\\nthe l-th layer, the hidden state hS\\ni,l is updated by the\\nfollowing equation:\\nhS\\ni,l = ReLU(\\nn\\nX\\nj=1\\nAijWlhS\\ni,l−1 + bl),\\n(11)\\nwhere Wl,bl are learned parameters and we use\\nReLU as activation function. Significantly, hS\\ni,0\\nis equal to hS\\ni . Accordingly, we get the final out-\\nput ˆHS from the last GCN layer which is rich in\\nsentiment information. Every underlying aspect\\naggregates its relevant information from both the\\nimage-text pair. Moreover, sentiment confusion\\nof different aspects is weakened because the as-\\nsociation matrix makes little interference among\\ndifferent aspects.\\n2We use spaCy toolkit to construct the dependency tree\\nreferring from https://spacy.io\\nTwitter2015\\nTwitter2017\\n#sentence\\n3,502\\n2,910\\n#with one aspect\\n2,159 (61.65%)\\n976 (33.54%)\\n#with multiple aspects\\n1,343 (38.35%)\\n1,934 (66.46%)\\n#with multiple sentiments\\n1,257\\n1,690\\nTable 1: Statistics of the two benchmark datasets. Line 1\\nis the number of sentences. #X in the last 3 lines denotes\\nthe number of sentences with such characteristics X.\\n3.4\\nPrediction and Loss Function\\nThe BART decoder takes the combination of ˆH,\\nˆHS, and the previous decoder output Y<t as inputs,\\nand predicts the token probability distribution as\\nfollows:\\n˜H = λ1 ˆH + λ2 ˆHS,\\n(12)\\nhd\\nt = Decoder( ˜H; Y<t)\\n(13)\\nHT = (W + ˜HT )/2\\n(14)\\nP(yt) = softmax([HT ; Cd]hd\\nt )\\n(15)\\nwhere λ1, λ2 are the hyper-parameters to control\\nthe contribution from the two modules. ˜HT is the\\ntextual part of ˜H. W denotes the embeddings of\\ninput tokens. Cd means the embeddings of the [\\npositive, neutral, negative, <eos>]. The loss func-\\ntion is as follows:\\nL = −EX∼D\\nO\\nX\\nt=1\\nlogP(yt|Y<t, X),\\n(16)\\nwhere O = 2M + 2N + 2 is the length of Y, and\\nX denotes the multimodal input.\\n4\\nExperiment\\n4.1\\nExperimental settings\\nDatasets. Our two benchmark datasets are Twit-\\nter2015 and Twitter2017 (Yu and Jiang (2019)). As\\nshown in the statistics of Table 1, sentences with\\nmultiple aspects take up a considerable part of the\\ntwo datasets.\\nImplementation Details. Our model is based on\\nBART (Lewis et al., 2019), and the pre-training\\ntask is trained for 40 epochs with batch size 64,\\nand for 35 epochs with batch size 16 on MABSA.\\nThe learning rates are both 7e-5 and hidden sizes\\nare 768. Hyper-parameters λ1 and λ2 are 1 and 0.5\\nrespectively. Besides, we pre-train A3M on TRC\\ndataset (Vempala and Preo¸tiuc-Pietro, 2019), which\\nis divided into two groups according to whether the\\ntext is represented.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 5}, page_content='Twitter2015\\nTwitter2017\\nMethods\\nP\\nR\\nF1\\nP\\nR\\nF1\\nText-based\\nSPAN* (Hu et al., 2019)\\n53.7\\n53.9\\n53.8\\n59.6\\n61.7\\n60.6\\nD-GCN* (Chen et al., 2020)\\n58.3\\n58.8\\n59.4\\n64.2\\n64.1\\n64.1\\nBART* (Yan et al., 2021)\\n62.9\\n65.0\\n63.9\\n65.2\\n65.6\\n65.4\\nMultimodal\\nUMT+TomBERT* (Yu et al., 2020; Yu and Jiang, 2019)\\n58.4\\n61.3\\n59.8\\n62.3\\n62.4\\n62.4\\nOSCGA+TomBERT* (Wu et al., 2020c; Yu and Jiang, 2019)\\n61.7\\n63.4\\n62.5\\n63.4\\n64.0\\n63.7\\nOSCGA-collapse* (Wu et al., 2020c)\\n63.1\\n63.7\\n63.2\\n63.5\\n63.5\\n63.5\\nRpBERT-collapse* (Sun et al., 2021)\\n49.3\\n46.9\\n48.0\\n57.0\\n55.4\\n56.2\\nUMT-collapse (Yu et al., 2020)\\n61.0\\n60.4\\n61.6\\n60.8\\n60.0\\n61.7\\nJML (Ju et al., 2021)\\n65.0\\n63.2\\n64.1\\n66.5\\n65.5\\n66.0\\nVLP-MABSA* (Ling et al., 2022)\\n65.1\\n68.3\\n66.6\\n66.9\\n69.2\\n68.0\\nCMMT (Yang et al., 2022)\\n64.6\\n68.7\\n66.5\\n67.6\\n69.4\\n68.5\\nAoM (ours)\\n67.9\\n69.3\\n68.6\\n68.4\\n71.0\\n69.7\\nTable 2: Results of different methods for MABSA on the two Twitter datasets. * denotes the results from Ling et al.\\n(2022). The best results are bold-typed and the second best ones are underlined.\\nEvaluation Metrics. We evaluate the performance\\nof our model on MABSA task and MATE task by\\nMicro-F1 score (F1), Precision (P) and Recall (R),\\nwhile on MASC task we use Accuracy (Acc) and\\nF1 following previous studies.\\n4.2\\nBaselines\\nWe compare our proposed model with four types\\nof methods listed below.\\nApproaches for textual ABSA. 1) SPAN (Hu\\net al., 2019) detects opinion targets with their senti-\\nments. 2) D-GCN (Chen et al., 2020) models de-\\npendency relations among words via dependency\\ntree. 3) BART (Yan et al., 2021) solves seven\\nABSA subtasks in an end-to-end framework.\\nApproaches for MATE. 1) RAN (Wu et al.,\\n2020b) focus on alignment of text and object re-\\ngions. 2) UMT (Yu et al., 2020) takes text-based\\nentity span detection as an auxiliary task. 3) OS-\\nCGA (Wu et al., 2020c) foucus on alignments of\\nvisual objects and entities.\\nApproaches for MASC. 1) ESAFN (Yu et al.,\\n2019) is an entity-level sentiment analysis method\\nbased on LSTM. 2) TomBERT (Yu and Jiang,\\n2019) applies BERT to obtain aspect-sensitive tex-\\ntual representations. 3) CapTrBERT (Khan and\\nFu, 2021) translates images into text and construct\\nan auxiliary sentence for fusion.\\nApproaches for MABSA. 1) UMT-collapse\\n(Yu et al., 2020), OSCGA-collapse (Wu et al.,\\n2020c) and RpBERT-collapse (Sun et al., 2021)\\nare adapted from models for MATE by using col-\\nlapsed labels to represent aspect and sentiment\\npairs. 2) UMT+TomBERT, OSCGA+TomBERT\\nare two pipeline approaches by combining UMT\\n(Yu et al., 2020) or OSCGA (Wu et al., 2020c) with\\nTomBERT (Yu and Jiang, 2019). 3) JML (Ju et al.,\\n2021) is the first joint model for MABSA with aux-\\niliary cross-modal relation detection module. 4)\\nCMMT (Yang et al., 2022) implements a gate to\\ncontrol the multimodal information contributions\\nduring inter-modal interactions. 5) VLP-MABSA\\n(Ling et al., 2022) performs five task-specific pre-\\ntraining tasks to model aspects, opinions and align-\\nments.\\n4.3\\nMain Results\\nIn this section, we show the excellent performance\\nof AoM on the two datasets for the three tasks\\ncompared with SOTAs.\\nPerformance on MABSA: The results for\\nMABSA are shown in Table 2. First, our AoM\\nfar exceeds all text-based models, which means\\ndetection of richer visual information and textual\\ninformation in our model is helpful. Second, multi-\\nmodal pipeline methods and adaptive methods are\\ngenerally unsatisfactory, because they ignore the\\ninteraction between the semantic information and\\nsentiment for the two sub-tasks. Last, AoM out-\\nperforms all multimodal methods in every metric.\\nEspecially, AoM achieves the improvement of 2%\\nand 1.2% with respect to F1 in contrast with the sec-\\nond best models on two datasets (VLP-MABSA for\\nTwitter2015 and CMMT for Twitter2017), which\\ndemonstrates the effectiveness of learning aspect-\\nrelevant visual blocks and textual words compared\\nto focusing on all visual and textual inputs.\\nPerformance on MATE: As shown in Table 3,\\nAoM is ahead of most of the current models and\\nperforms the best in Twitter 2015 by 0.3% higher\\nthan the second best CMMT on F1. The perfor-\\nmance of CMMT in Twitter2017 is 0.8% higher'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 6}, page_content='Twitter2015\\nTwitter2017\\nMethods\\nP\\nR\\nF1\\nP\\nR\\nF1\\nRAN*\\n80.5\\n81.5\\n81.0\\n90.7\\n90.7\\n90.0\\nUMT*\\n77.8\\n81.7\\n79.7\\n86.7\\n86.8\\n86.7\\nOSCGA*\\n81.7\\n82.1\\n81.9\\n90.2\\n90.7\\n90.4\\nJML*\\n83.6\\n81.2\\n82.4\\n92.0\\n90.7\\n91.4\\nVLP-MABSA*\\n83.6\\n87.9\\n85.7\\n90.8\\n92.6\\n91.7\\nCMMT\\n83.9\\n88.1\\n85.9\\n92.2\\n93.9\\n93.1\\nAoM (ours)\\n84.6\\n87.9\\n86.2\\n91.8\\n92.8\\n92.3\\nTable 3: Results of different methods for MATE. * de-\\nnotes the results from Ling et al. (2022).\\nTwitter2015\\nTwitter2017\\nMethods\\nACC\\nF1\\nACC\\nF1\\nESAFN\\n73.4\\n67.4\\n67.8\\n64.2\\nTomBERT\\n77.2\\n71.8\\n70.5\\n68.0\\nCapTrBERT\\n78.0\\n73.2\\n72.3\\n70.2\\nJML\\n78.7\\n-\\n72.7\\n-\\nVLP-MABSA\\n78.6\\n73.8\\n73.8\\n71.8\\nCMMT\\n77.9\\n-\\n73.8\\n-\\nAoM (ours)\\n80.2\\n75.9\\n76.4\\n75.0\\nTable 4: Results of different methods for MASC.\\nthan ours, probably due to our model wrongly pre-\\ndicting some noun phrases as aspects. But consid-\\nering the improvement in MASC and MABSA, it\\nis still worthy treating all noun phrases in the sen-\\ntence as candidate aspects when acquiring aspect-\\nrelevant visual information.\\nPerformance on MASC: Table 4 shows the per-\\nformance of MASC. It is exciting that our model\\noutperforms the second-best results by 1.5% and\\n2.6% in accuracy, 2.1% and 3.2% points in F1 score\\non Twitter2015 and Twitter2017. It demonstrates\\nthat AoM has the ability to detect aspect-related\\nsentiment information from both images and text,\\neven disturbed by other noisy aspects.\\n4.4\\nAblation Study\\nIn this section, we research the effectiveness of\\neach component in AoM, the results are shown in\\nTable 5.\\nW/o A3M&AG-GCN shows that after remov-\\ning the two specially designed modules, the per-\\nTwitter2015\\nTwitter2017\\nMethods\\nP\\nR\\nF1\\nP\\nR\\nF1\\nFull\\n67.9\\n69.3\\n68.6\\n68.4\\n71.0\\n69.7\\nw/o A3M&AG-GCN\\n65.7\\n67.3\\n66.5\\n66.5\\n69.0\\n67.8\\nw/o A3M&TRC\\n62.1\\n61.0\\n61.6\\n63.7\\n64.1\\n63.9\\nw/o TRC\\n66.8\\n68.4\\n67.6\\n67.8\\n69.8\\n68.8\\nw/o AG-GCN\\n67.0\\n69.4\\n68.2\\n67.8\\n69.7\\n68.8\\nw/o SenticNet\\n65.7\\n70.5\\n68.0\\n68.1\\n69.4\\n68.7\\nw/o TRC&AG-GCN\\n66.7\\n69.2\\n68.0\\n67.8\\n69.5\\n68.6\\nTable 5: The performance comparison of our full model\\nand its ablated approaches.\\nImage\\nText\\nVLP-MABSA\\nBART+A3M\\nAoM\\n(a) NBA Western Conference Finals: \\nGolden State Warriors shock \\nOklahoma City Thunder,…\\n( NBA, NEU ) (√, √)\\n---\\n( Oklahoma, NEU ) (×, ×)\\n( NBA, NEU ) (√, √)\\n( Golden State Warriors , POS ) (√, √)\\n( Oklahoma City Thunder, NEG ) (√, √)\\n( NBA, NEU ) (√, √)\\n( Golden State Warriors , POS ) (√, √)\\n( Oklahoma City Thunder, NEG ) (√, √)\\n(b) This subtle difference between \\nDaniel Radcliffe and Elijah Wood\\nis pretty unsettling.\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah, NEU ) (×, ×)\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah Wood, NEG ) (√, √)\\n( Daniel Radcliffe, NEG ) (√, √)\\n( Elijah Wood, NEG ) (√, √)\\nFigure 5: Two cases with predictions by VLP-MABSA\\n(Ling et al., 2022), BART+A3M, and our model.\\nformance declines by 2.1% on Twitter2015 and\\n1.9% on Twitter2017. It fully demonstrates their\\ncontributions to learning effective information.\\nW/o A3M&TRC performs worse after remov-\\ning A3M including the pre-training on TRC. It\\nproves the necessity of modeling semantic align-\\nment between visual blocks and aspects in A3M.\\nWith the alignment, AG-GCN can obtain appropri-\\nate aspect-image-block and text-text association.\\nW/o TRC pre-training shows a slight drop after\\nwe remove the TRC pre-training on A3M, which\\nimplies relevant pre-training task is useful for the\\nmodel to learn better parameters.\\nW/o AG-GCN displays the performance with-\\nout AG-GCN, declining by 0.42% on Twitter2015\\nand 0.9% on Twitter2017. It means that AG-GCN\\ndoes make the prediction focus on specific aspects\\nrelated to blocks and words with syntax dependen-\\ncies. In other words, the multimodal interference\\nfrom other aspects can be mitigated.\\nW/o SenticNet is the model without sentiment\\ninformation in AG-GCN. Its performance shows\\nadding external affective knowledge can enhance\\nthe sentiment comprehension of the model.\\nW/o TRC&AG-GCN is the BART model only\\nwith our A3M module. We can see from Table 5\\nthat w/o TRC&AG-GCN improves w/o A3M&AG-\\nGCN by 1.5% and 0.8%. So it is effective to align\\nthe fine-grained visual block to related aspect and\\nreduce irrelevant information.\\n4.5\\nCase Study\\nTo better analyze how the Aspect-Aware Attention\\nModule and Aspect-Guided Graph Convolutional\\nNetwork work, we present the case study as fol-\\nlows.\\nFigure 5 displays two examples with predic-\\ntions from VLP-MABSA (Ling et al., 2022),'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 7}, page_content='A3M\\nKyoto\\nbit\\nsomeone\\nmayor\\nMayor Kadokawa\\nthanks\\ntime\\n(I.a) attention of CAs\\n(I.b) learned visual attention\\nComplain about\\nKyoto a bit and\\nsomeone takes you\\nto see the mayor.\\nInteresting! Mayor\\nKadokawa, thanks\\nfor your time!\\nInputs\\nAspect-Sentiment Pairs\\n<Kyoto, NEG>\\n<mayor, NEU>\\n<Mayor Kadokawa, POS>\\nThe image-related probability \\ndistribution of candidate aspects.\\nAG-GCN\\n(II.a) word-to-word association matrix\\n(II.b) word-to-visual-block association matrix\\n(II.c) aspect-relevant visual attention map\\nMayor Kadokawa\\nMayor\\nKadokawa\\nthanks\\nKyoto\\nMayor Kadokawa\\n(II.d) information relevant to “Mayor Kadokawa”\\n0.8\\n0.7\\n0.6\\n0.1\\n0.0\\n0.2\\n0.3\\n0.4\\n0.5\\nFigure 6: Visualization of the attention maps in A3M and the sub-parts of the weighted-association matrix AG-GCN.\\nBART+A3M and our AoM. In example (a), VLP-\\nMABSA misses the aspect “Golden State War-\\nriors”, gets an incomplete aspect “Oklahoma City\\nThunder” and wrongly predicts the sentiment. It\\nmay be caused by the interference from the vi-\\nsual region which represents pride expression of a\\nperson. However, BART+A3M gets all right pre-\\ndictions due to the ability of aspect-oriented atten-\\ntion. In example (b), compared with our whole\\nmodel, BART+A3M wrongly predicts the senti-\\nment of “Daniel Radcliffe” which should be nega-\\ntive. We attribute the wrong prediction to lacking\\nsyntax association which benefits sentiment trans-\\nmission. In other words, AG-GCN contributes to\\nthe correctness.\\n4.6\\nAttention Visualization\\nTo investigate the effectiveness of detecting aspect-\\nrelevant information, we visualize the attention pro-\\ncess as shown in Figure 6.\\nFor A3M: (i) Figure 6-(I.a) shows the attention\\nweights of candidate aspects computed according\\nto the images. We can see that “Mayor Kadokawa”\\nis the most relevant aspect.\\n(ii) Figure 6-(I.b)\\nshows the proportions of the visual information re-\\nserved at the last step in A3M, where we weighted\\nadd up the representations of visual blocks and\\nthe corresponding aspects. The heat map shows\\nthat the visual information associated with “Mayor\\nKadokawa” is reserved to a great extent, while the\\nhelpless information from other blocks is disre-\\ngarded as noise. It demonstrates that attention in\\nA3M is able to detect aspect-relevant information.\\nFor AG-GCN: (i) Figure 6-(II.a) shows the\\nword-to-word part of the weighted association ma-\\ntrix. The matrix effectively excludes sentiment\\ninterference from other aspects by adding syntax\\ndependency information. For example, the senti-\\nment of “mayor” cannot be influenced by irrelevant\\nkeywords, such as “Complain” and “thanks”. (ii)\\nFigure 6-(II.b) shows the dependencies between\\nvisual blocks and words. (iii) Specifically, we vi-\\nsualize the visual attention of aspects “Kyoto” (see\\nFigure 6-(II.c) left) and “Mayor Kadokawa” (see\\nFigure 6-(II.c) right). We can see that “Kyoto” pays\\nmore attention to the pictures hanging on the wall\\nwhich are full of Japanese elements related to the\\nplace, while “Mayor Kadokawa” focus more on the\\njoyful expressions of the two people. (iv) Figure\\n6-(II.d) shows the words and image blocks “Mayor\\nKadokawa” focused on in sentiment transmission.\\nIt’s obvious that these attentions are helpful for the\\nprediction.\\n5\\nConclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 8}, page_content='we explicitly add sentiment embedding into AoM.\\nThen, a graph convolutional network is used to ag-\\ngregate the semantic and sentiment embedding un-\\nder the guidance of both image-text similarity and\\nsyntax dependency in sentences. The experimental\\nresults on two widely used datasets demonstrate\\nthe effectiveness of our method.\\nLimitations\\nThough our proposed method outperforms cur-\\nrent state-of-the-art methods, there are still many\\nchallenges we should overcome in future research.\\nFirst, for colloquial expression which confuses cur-\\nrent dependency tree parser, we should come up\\nwith new solutions. Second, emotional prediction\\nof tweet posts describing current issues needs exter-\\nnal knowledge, which is absent in existing research.\\nAcknowledgments\\nWe thank anonymous reviewers for their valu-\\nable comments.\\nThis work was supported by\\nthe Natural Science Foundation of Tianjin,\\nChina (No.22JCJQJC00150, 22JCQNJC01580),\\nthe National Natural Science Foundation of\\nChina\\n(No.62272250),\\nTianjin\\nResearch\\nIn-\\nnovation\\nProject\\nfor\\nPostgraduate\\nStudents\\n(No.2022SKYZ232),\\nand\\nthe\\nFundamental\\nResearch Funds for the Central Universities (No.\\n63231149).\\nReferences\\nMeysam Asgari-Chenaghlu, M. Reza Feizi-Derakhshi,\\nLeili Farzinvash, M. A. Balafar, and Cina Motamed.\\n2021. CWI: A multimodal deep learning approach\\nfor named entity recognition from social media us-\\ning character, word and image features.\\nNeural\\nComputing and Applications, 34(3):1905–1922.\\nYitao Cai, Huiyu Cai, and Xiaojun Wan. 2019.\\nMulti-Modal Sarcasm Detection in Twitter with\\nHierarchical Fusion Model.\\nIn Proceedings of\\nthe 57th Annual Meeting of the Association for\\nComputational Linguistics, pages 2506–2515, Flo-\\nrence, Italy. Association for Computational Linguis-\\ntics.\\nGuimin Chen, Yuanhe Tian, and Yan Song. 2020.\\nJoint aspect extraction and sentiment analysis with\\ndirectional graph convolutional networks.\\nIn\\nProceedings of the 28th international conference on\\ncomputational linguistics, pages 272–279.\\nHao Chen, Zepeng Zhai, Fangxiang Feng, Ruifan\\nLi, and Xiaojie Wang. 2022.\\nEnhanced Multi-\\nChannel\\nGraph\\nConvolutional\\nNetwork\\nfor\\nAspect\\nSentiment\\nTriplet\\nExtraction.\\nIn\\nProceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), pages 2974–2985, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nTao Chen, Damian Borth, Trevor Darrell, and Shih-\\nFu Chang. 2014. Deepsentibank: Visual sentiment\\nconcept classification with deep convolutional neural\\nnetworks.\\nZhuang Chen and Tieyun Qian. 2020.\\nRelation-\\nAware Collaborative Learning for Unified Aspect-\\nBased Sentiment Analysis.\\nIn Proceedings of\\nthe 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 3685–3694, On-\\nline. Association for Computational Linguistics.\\nDevamanyu Hazarika, Roger Zimmermann, and Sou-\\njanya Poria. 2020.\\nMisa:\\nModality-invariant\\nand -specific representations for multimodal senti-\\nment analysis.\\nIn Proceedings of the 28th ACM\\nInternational Conference on Multimedia, MM ’20,\\npage 1122–1131, New York, NY, USA. Association\\nfor Computing Machinery.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Dongsheng\\nLi, and Yiwei Lv. 2019. Open-domain targeted senti-\\nment analysis via span-based extraction and classifi-\\ncation. arXiv preprint arXiv:1906.03820.\\nXincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\\nShoushan Li, Min Zhang, and Guodong Zhou.\\n2021. Joint Multi-modal Aspect-Sentiment Anal-\\nysis with Auxiliary Cross-modal Relation Detec-\\ntion.\\nIn Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 4395–4405, Online and Punta Cana, Domini-\\ncan Republic. Association for Computational Lin-\\nguistics.\\nKS Kalaivani, M Rakshana, K Mounika, and D Sindhu.\\n2022. Senticnet-based feature weighting scheme for\\nsentiment classification. In Mobile Computing and\\nSustainable Informatics, pages 839–848. Springer.\\nZaid Khan and Yun Fu. 2021.\\nExploiting bert for\\nmultimodal target sentiment classification through\\ninput space translation.\\nIn Proceedings of the\\n29th ACM International Conference on Multimedia,\\npages 3034–3042.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\\nnoising sequence-to-sequence pre-training for natural\\nlanguage generation, translation, and comprehension.\\narXiv preprint arXiv:1910.13461.\\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu\\nMa,\\nXiaojie\\nWang,\\nand\\nEduard\\nHovy.\\n2021a.\\nDual Graph Convolutional Networks\\nfor\\nAspect-based\\nSentiment\\nAnalysis.\\nIn\\nProceedings\\nof\\nthe\\n59th\\nAnnual Meeting\\nof\\nthe\\nAssociation\\nfor\\nComputational Linguistics'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 9}, page_content='and\\nthe\\n11th\\nInternational Joint Conference\\non\\nNatural Language Processing\\n(Volume\\n1:\\nLong Papers), pages 6319–6329, Online. Associa-\\ntion for Computational Linguistics.\\nYuanqing Li, Ke Zhang, Jingyu Wang, and Xinbo Gao.\\n2021b. A cognitive brain model for multimodal sen-\\ntiment analysis based on attention neural networks.\\nNeurocomputing, 430:159–173.\\nBin Liang, Hang Su, Lin Gui, Erik Cambria, and\\nRuifeng Xu. 2022a. Aspect-based sentiment anal-\\nysis via affective knowledge enhanced graph con-\\nvolutional networks.\\nKnowledge-Based Systems,\\n235:107643.\\nBin Liang, Rongdi Yin, Lin Gui, Jiachen Du, and\\nRuifeng Xu. 2020. Jointly Learning Aspect-Focused\\nand Inter-Aspect Relations with Graph Convolu-\\ntional Networks for Aspect Sentiment Analysis. In\\nProceedings of the 28th International Conference\\non\\nComputational Linguistics,\\npages\\n150–161,\\nBarcelona, Spain (Online). International Committee\\non Computational Linguistics.\\nShuo Liang, Wei Wei, Xian-Ling Mao, Fei Wang, and\\nZhiyong He. 2022b. BiSyn-GAT+: Bi-Syntax Aware\\nGraph Attention Network for Aspect-based Senti-\\nment Analysis. In Findings of the Association for\\nComputational Linguistics: ACL 2022, pages 1835–\\n1848, Dublin, Ireland. Association for Computational\\nLinguistics.\\nYan Ling, Jianfei Yu, and Rui Xia. 2022.\\nVision-\\nLanguage Pre-Training for Multimodal Aspect-\\nBased Sentiment Analysis.\\nIn Proceedings of\\nthe\\n60th\\nAnnual Meeting\\nof\\nthe\\nAssociation\\nfor\\nComputational Linguistics\\n(Volume\\n1:\\nLong Papers),\\npages\\n2149–2159,\\nDublin,\\nIre-\\nland. Association for Computational Linguistics.\\nYanxia Lv, Fangna Wei, Lihong Cao, Sancheng Peng,\\nJianwei Niu, Shui Yu, and Cuirong Wang. 2021.\\nAspect-level sentiment analysis using context and\\naspect memory network. Neurocomputing, 428:195–\\n205.\\nYukun Ma, Haiyun Peng, and Erik Cambria. 2018. Tar-\\ngeted aspect-based sentiment analysis via embed-\\nding commonsense knowledge into an attentive lstm.\\nProceedings of the AAAI Conference on Artificial\\nIntelligence, 32(1).\\nShinhyeok Oh, Dongyub Lee, Taesun Whang, IlNam\\nPark, Seo Gaeun, EungGyun Kim, and Harksoo\\nKim. 2021.\\nDeep Context- and Relation-Aware\\nLearning for Aspect-based Sentiment Analysis.\\nIn Proceedings of the 59th Annual Meeting of\\nthe\\nAssociation\\nfor\\nComputational Linguistics\\nand\\nthe\\n11th\\nInternational Joint Conference\\non\\nNatural Language Processing\\n(Volume\\n2:\\nShort Papers), pages 495–503, Online. Association\\nfor Computational Linguistics.\\nShiguan Pang, Yun Xue, Zehao Yan, Weihao Huang,\\nand Jinhui Feng. 2021. Dynamic and Multi-Channel\\nGraph Convolutional Networks for Aspect-Based\\nSentiment Analysis. In Findings of the Association\\nfor Computational Linguistics: ACL-IJCNLP 2021,\\npages 2627–2636, Online. Association for Computa-\\ntional Linguistics.\\nLin Sun, Jiquan Wang, Kai Zhang, Yindu Su, and Fang-\\nsheng Weng. 2021. Rpbert: A text-image relation\\npropagation-based bert model for multimodal ner.\\nProceedings of the AAAI Conference on Artificial\\nIntelligence, 35(15):13860–13868.\\nAlakananda Vempala and Daniel Preo¸tiuc-Pietro. 2019.\\nCategorizing and inferring the relationship between\\nthe text and image of twitter posts. In Proceedings\\nof the 57th annual meeting of the Association for\\nComputational Linguistics, pages 2830–2840.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi,\\nand Lian Chi. 2020a.\\nMultimodal as-\\npect extraction with region-aware alignment net-\\nwork. In Natural Language Processing and Chinese\\nComputing, pages 145–156, Cham. Springer Interna-\\ntional Publishing.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi, and Lian Chi. 2020b.\\nMultimodal aspect ex-\\ntraction with region-aware alignment network. In\\nCCF International Conference on Natural Language\\nProcessing and Chinese Computing, pages 145–156.\\nSpringer.\\nYang Wu, Yanyan Zhao, Hao Yang, Song Chen,\\nBing Qin, Xiaohuan Cao, and Wenting Zhao.\\n2022.\\nSentiment Word Aware Multimodal Re-\\nfinement for Multimodal Sentiment Analysis with\\nASR Errors.\\nIn Findings of the Association for\\nComputational Linguistics: ACL 2022, pages 1397–\\n1406, Dublin, Ireland. Association for Computational\\nLinguistics.\\nZhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen,\\nHo-fung Leung, and Qing Li. 2020c. Multimodal\\nrepresentation with embedded visual guiding ob-\\njects for named entity recognition in social media\\nposts. In Proceedings of the 28th ACM International\\nConference on Multimedia, pages 1038–1046.\\nJunjie Xu, Shuwen Yang, Luwei Xiao, Zhichao Fu,\\nXingjiao Wu, Tianlong Ma, and Liang He. 2022.\\nGraph convolution over the semantic-syntactic hybrid\\ngraph enhanced by affective knowledge for aspect-\\nlevel sentiment classification. In 2022 International\\nJoint Conference on Neural Networks (IJCNN),\\npages 1–8. IEEE.\\nLu\\nXu,\\nHao\\nLi,\\nWei\\nLu,\\nand\\nLidong\\nBing.\\n2020.\\nPosition-Aware\\nTagging\\nfor\\nAspect\\nSentiment Triplet Extraction.\\nIn Proceedings\\nof the 2020 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP), pages\\n2339–2349, Online. Association for Computational\\nLinguistics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/AoM.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'trapped': '', 'modDate': 'D:20230605010014Z', 'creationDate': 'D:20230605010014Z', 'page': 10}, page_content='Hang Yan, Junqi Dai, Xipeng Qiu, Zheng Zhang,\\net al. 2021.\\nA unified generative framework for\\naspect-based sentiment analysis.\\narXiv preprint\\narXiv:2106.04300.\\nLi Yang,\\nJin-Cheon Na,\\nand Jianfei Yu. 2022.\\nCross-Modal Multitask Transformer for End-to-\\nEnd Multimodal Aspect-Based Sentiment Anal-\\nysis.\\nInformation Processing & Management,\\n59(5):103038.\\nJianfei Yu and Jing Jiang. 2019.\\nAdapting bert\\nfor target-oriented multimodal sentiment classi-\\nfication.\\nIn Proceedings of the Twenty-Eighth\\nInternational\\nJoint\\nConference\\non\\nArtificial\\nIntelligence, IJCAI-19, pages 5408–5414. Interna-\\ntional Joint Conferences on Artificial Intelligence\\nOrganization.\\nJianfei Yu, Jing Jiang, and Rui Xia. 2019.\\nEntity-\\nsensitive attention and fusion network for entity-level\\nmultimodal sentiment classification.\\nIEEE/ACM\\nTransactions on Audio, Speech, and Language\\nProcessing, 28:429–439.\\nJianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020.\\nImproving multimodal named entity recognition via\\nentity span detection with unified multimodal trans-\\nformer. In Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics,\\npages 3342–3352, Online. Association for Computa-\\ntional Linguistics.\\nLi Yuan, Jin Wang, Liang-Chih Yu, and Xuejie Zhang.\\n2020. Graph attention network with memory fusion\\nfor aspect-level sentiment analysis. In Proceedings\\nof the 1st Conference of the Asia-Pacific Chapter of\\nthe Association for Computational Linguistics and\\nthe 10th International Joint Conference on Natural\\nLanguage Processing, pages 27–36, Suzhou, China.\\nAssociation for Computational Linguistics.\\nQi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang.\\n2018. Adaptive co-attention network for named en-\\ntity recognition in tweets. In Thirty-Second AAAI\\nConference on Artificial Intelligence.\\nYuhao Zhang, Ying Zhang, Wenya Guo, Xiangrui Cai,\\nand Xiaojie Yuan. 2022. Learning disentangled rep-\\nresentation for multimodal cross-domain sentiment\\nanalysis.\\nIEEE Transactions on Neural Networks\\nand Learning Systems.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 0}, page_content='Vanessa\\n: Visual Connotation and Aesthetic Attributes\\nUnderstanding Network for Multimodal Aspect-based Sentiment Analysis\\nLuwei Xiao♠, Rui Mao♦∗, Xulang Zhang♦, Liang He♠and Erik Cambria♦\\n♠School of Computer Science and Technology, East China Normal University, Shanghai, China\\n♦College of Computing and Data Science, Nanyang Technological University, Singapore\\nlouisshaw@stu.ecnu.edu.cn, lhe@cs.ecnu.edu.cn, {rui.mao, xulang.zhang, cambria}@ntu.edu.sg\\nAbstract\\nPrevailing research concentrates on superficial\\nfeatures or descriptions of images, revealing a\\nsignificant gap in the systematic exploration\\nof their connotative and aesthetic attributes.\\nFurthermore, the use of cross-modal relation\\ndetection modules to eliminate noise from\\ncomprehensive image representations leads to\\nthe omission of subtle contextual information.\\nWe present Vanessa, a visual connotation and\\naesthetic Attributes understanding network for\\nmultimodal aspect-based sentiment analysis. It\\nincorporates a multi-aesthetic attributes aggre-\\ngation (MA3) module that models intra- and\\ninter-dependencies among bi-modal represen-\\ntations as well as emotion-laden aesthetic at-\\ntributes. Moreover, we devise a self-supervised\\ncontrastive learning framework to explore the\\npairwise relevance between images and text via\\nthe Gaussian distribution of their CLIP scores.\\nBy dynamically clustering and merging mul-\\ntimodal tokens, Vanessa effectively captures\\nboth implicit and explicit sentimental cues. Ex-\\ntensive experiments on two widely adopted\\nbenchmarks verify Vanessa’s effectiveness.\\n1\\nIntroduction\\nMultimodal\\naspect-based\\nsentiment\\nanalysis\\n(MABSA) marks a pivotal advancement in\\nsentiment analysis by enhancing the machine’s\\nability to interpret human emotions, thus attracting\\ngrowing scholarly interest (Susanto et al., 2020;\\nCambria et al., 2013). MABSA aims to identify\\naspect-sentiment pairs within sentences given\\nimage-text pairs. Examples of MABSA are shown\\nin Fig. 1.\\nThe primary challenge of MABSA\\nlies in leveraging image data to enrich textual\\nsentiment analysis. Existing approaches typically\\nfall into two major categories: (i) segmenting the\\nimage into multiple visual regions or extracting\\nprominent visual objects to facilitate inter-dynamic\\nmodeling with textual sequences through tailored\\n∗Corresponding author.\\nImage\\nText\\n(a) What a wonderful [weather]Neg!\\n(b) [Neymar]Pos has come to [saudi arabia]Neu.\\nImpr.\\nAes/CLIP\\n0.35/0.52\\n0.57/0.65\\nI feel a sense of awe and fearful when \\ni look at this image. The dark clouds \\nand the cityscape create a sense of \\nawe and fearful .\\nI believe this image conveys a sense of \\nexcitement. It makes me think about the \\npower of teamwork. \\nAes-Cap\\nThe man\\'s smile and the hands of his head \\nconvey a sense of happiness. The lighting \\nof the room creates a sense of warmth.\\nThe lighting of the image is very \\npowerful and the city is very powerful.\\nFigure 1: Examples for MABSA, with aspect-sentiment\\npairs highlighted in the text. \"Aes\" and \"CLIP\" repre-\\nsent the aesthetic and CLIP scores (ranging from 0 to\\n1). \"Impr\" and \"Aes-Cap\" denote the impression and\\naesthetic caption generated by our fine-tuned BLIP.\\nfusion mechanisms (Xu et al., 2019; Yu and\\nJiang; Yu et al., 2019, 2020, 2022a,b; Zhang\\net al., 2021; Ling et al., 2022; Yang et al., 2022b;\\nZhou et al., 2023); (ii) translating the image\\ninto textual space and subsequently establishing\\nlinkages between primary text sequences and\\nsupplementary sentences (Khan and Fu, 2021;\\nYang et al., 2022a; Liu et al., 2022; Xiao et al.,\\n2023; Wang et al., 2023).\\nDespite promising outcomes, the majority of\\nstudies confront two challenges. Firstly, they ne-\\nglect the implicit emotions evoked by connotation\\nand aesthetic elements of visual imagery. Psycho-\\nlogically, images serve as powerful stimuli that\\nactivate cognitive and perceptual pathways, elicit-\\ning affective responses through their portrayal of\\ncontextual, symbolic, and aesthetic elements (Lang\\nand Bradley, 2007; Barrett and Bar, 2009). For\\nexample, a beautifully composed photograph with\\nbalanced colors and pleasing symmetry is likely to\\nevoke positive emotions such as joy and admiration.\\nSecond, prevalent approaches utilize cross-modal\\nrelation detection modules to filter noise from holis-\\ntic image representations, which can inadvertently\\neliminate subtle contextual cues.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 1}, page_content='To address the aforementioned issues, we in-\\ntroduce Vanessa, a model crafted to decipher\\nthe sentimental expressions conveyed through vi-\\nsual connotations and aesthetics.\\nAdditionally,\\nVanessa explores the semantic correlations be-\\ntween images and their associated textual content.\\nThe model comprises three primary components:\\nthe Multi-Aesthetic Attributes Aggregation (MA3)\\nmodule, the Self-supervised Contrastive Learning\\nfor Image-Text Relevance (SSL-ITR), and the Dy-\\nnamic Token Merge (DTM) module. Initially, the\\nMA3 generates emotionally rich multimodal repre-\\nsentations and constructs a task-specific, aesthetic-\\naware multimodal dependency matrix. These are\\nthen processed through graph convolutional net-\\nworks (GCNs) to adaptively model the intra- and\\ninter-dynamics of aesthetic-aware emotions across\\nmodalities. Subsequently, SSL-ITR samples pos-\\nitive and negative image-text pairs based on the\\nGaussian distribution of their CLIP scores, thus\\nenabling the model to selectively focus on both\\nvisual and textual information or primarily on tex-\\ntual content. Lastly, DTM dynamically models\\nthe aesthetic-aware multimodal features at both ex-\\nplicit and implicit levels. Experimental results indi-\\ncate that Vanessa outperforms the state-of-the-art\\nbaseline by 1.2% and 0.9% in averaged F1 scores\\non two widely used Twitter datasets.\\nIn a nutshell, we contribute the following: (1) To\\nthe best of our knowledge, this is the first study to\\nexplore the utilization of implicit emotions evoked\\nby the connotation and aesthetic attributes of im-\\nages to model complex intermodal relationships,\\nwhile simultaneously learning sentimental cues at\\nboth explicit and implicit levels within MABSA;\\n(2) We tailor a self-supervised contrastive learning\\nframework to enable the model to grasp the seman-\\ntic pairwise relevance of image-text pairs based\\non their CLIP score and Gaussian distribution; (3)\\nWe conducted comprehensive experiments and rig-\\norous analyses on two widely recognized public\\ndatasets. The experimental results indicate that\\nVanessa achieves state-of-the-art performance.\\n2\\nRelated Work\\nMultimodal Aspect-based Sentiment Analysis.\\nSentiment analysis is a widely studied field that\\naims to understand and quantify human emotions\\nand opinions across various contexts (Zhang et al.,\\n2023; Lu et al., 2023; Liu et al., 2023a; Mao et al.,\\n2023; Cambria et al., 2024; Du et al., 2024).\\nWith the exponential growth of multimodal con-\\ntent on social media (Zhang et al., 2024b), MABSA\\nhas gained significant attention (Liu et al., 2022;\\nMao and Li, 2021; Yue et al., 2023; Fan et al.,\\n2024; Yang et al., 2024). The MABSA task con-\\nsists of two sub-tasks: Multimodal Aspect Term\\nExtraction (MATE) and Multimodal Aspect-based\\nSentiment Classification (MASC). MATE (Yang\\net al., 2023) aims at extracting all relevant aspect\\nterms from the textual content given an image-text\\npair, while MASC (Zhou et al., 2021; Zhang et al.,\\n2022) focuses on predicting the sentiment polarities\\nassociated with these extracted aspects. Recently,\\na group of studies successfully integrated these\\ntwo sub-tasks into a unified framework, effectively\\nstreamlining the process of achieving MABSA (Ju\\net al., 2021; Yang et al., 2022b; Ling et al., 2022;\\nMu et al., 2023; Zhao et al., 2023a; Xiao et al.,\\n2024; Cambria et al., 2023). However, most ma-\\nchine learning-based methods do not pay enough\\nattention to the implicit emotions evoked by the\\nconnotation and aesthetic elements of visual im-\\nagery. Moreover, employing various cross-modal\\nrelation detection modules to filter noise from holis-\\ntic image representations may inadvertently result\\nin the loss of subtle contextual cues (Hu et al., 2022;\\nYan et al., 2023).\\nMultimodal Representation Learning.\\nMulti-\\nmodal representation learning has emerged as a crit-\\nical research area (Liu et al., 2023b). Recent years\\nhave witnessed the development and widespread ap-\\nplication of sophisticated multimodal learning tech-\\nniques across multiple domains (Guo et al., 2023;\\nZhang et al., 2024a; Guo et al., 2024). A prominent\\nexample is CLIP (Radford et al., 2021), which is\\npre-trained on the WIT (WebImageText) dataset.\\nDistinct from conventional vision models, CLIP\\nconcurrently trains an image encoder and a text en-\\ncoder, thereby learning rich semantic relationships\\nbetween linguistic and visual modalities. The CLIP\\nscore (Hessel et al., 2021) quantifies the semantic\\nalignment between images and captions by comput-\\ning the cosine similarity between the image embed-\\nding and the caption embedding using a pre-trained\\nCLIP model. Similarly, BLIP (Li et al., 2022), a\\ncomprehensive vision-language framework, lever-\\nages knowledge distillation on captions to augment\\nits performance. It achieves state-of-the-art results\\nacross various tasks and demonstrates exceptional\\nzero-shot performance.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 2}, page_content='Building upon these advancements, BLIP-2 (Li\\net al., 2023), an enhanced vision-language model\\ndeveloped through an extensive pre-training strat-\\negy, exhibits a wide array of zero-shot image-to-\\ntext capabilities. In this study, we leverage the\\nrobust semantic alignment capabilities of CLIP to\\nmodel the pairwise relationships between text and\\nimages. Furthermore, we employ BLIP for fine-\\ntuning purposes to generate aesthetic captions im-\\nbued with rich emotional connotations.\\nVisual Connotation & Image Aesthetic Analysis.\\nVisual connotation involves emotive and aesthetic\\nmeanings an image conveys beyond its explicit\\ncontent, engaging viewers on deeper interpretative\\nlevels (Arnheim, 1954; Berger, 1972). The aesthet-\\nics of an image relate to its subjective evaluation\\nor the admiration of its beauty (Ramachandran and\\nHirstein, 1999). Previous research has concentrated\\non the aesthetic score (see Fig. 1), a quantitative\\nmetric that evaluates the visual attractiveness of an\\nimage (Zeng et al., 2019). A higher aesthetic score\\nis indicative of enhanced aesthetic quality. Recent\\nscholarly efforts emphasized encouraging vision\\nmodels to engage in generating visual metaphors\\nand aesthetic-related captions (Akula et al., 2023;\\nChakrabarty et al., 2023; Ke et al., 2023). More\\nrecently, Kruk et al. (2023) presented a connotation-\\nrich dataset termed Impressions, which enables the\\nexploration of emotions, thoughts, and beliefs that\\nimages invoke, as well as an analysis of the aes-\\nthetic elements that trigger these responses. In this\\nstudy, we employ visual connotation and aesthetic\\nattributes to comprehensively capture the sentimen-\\ntal cues within visual content for MABSA. To the\\nbest of our knowledge, this is the inaugural effort to\\nintegrate visual connotation and aesthetic attributes\\ninto the MABSA framework.\\n3\\nMethod\\nTask Definition.\\nGiven a image-text pair contain-\\ning image V and sentence S = (w1, w2, . . . , wn),\\nour objective is to predict the corresponding aspect-\\nsentiment sequence Y = (y1, y2, . . . , yn). Here,\\nyi ∈{B−POS, I−POS, B−NEG, I−NEG, B−\\nNEU, I −NEU} ∪{O}. In this case, B denotes\\nthe beginning token of an aspect term; I refers to\\ntokens that are part of the aspect term; O denotes\\ntokens that are outside any specific aspect. POS,\\nNEU, and NEG are the abbreviations of positive,\\nneutral, and negative sentiment associated with as-\\npect terms (Valdivia et al., 2018).\\nModel Overview.\\nFig. 2 illustrates the overall\\narchitecture of our proposed Vanessa, which com-\\nprises three main modules: the Multi-Aesthetic\\nAttributes Aggregation module (MA3), the Self-\\nSupervised Contrastive Learning for Image-Text\\nRelevance module (SSL-ITR), and the Dynamic\\nToken Merge module (DTM). Firstly, we fine-tune\\nthe BLIP on the Impression dataset to generate\\nimpression and aesthetic captions for the images.\\nThe image-text pairs and these auxiliary sentences\\nare then fed into the MA3 module, combined with\\naesthetic and CLIP scores, to construct an aesthetic-\\naware multimodal graph for modeling multimodal\\nand textual features. Subsequently, multimodal\\nfeatures are passed into the SSL-ITR to learn the\\nsemantic pairwise image-text relationship, based\\non the CLIP score and its Gaussian distribution.\\nFinally, the DTM module clusters and merges mul-\\ntimodal and textual features using a KNN-based\\nalgorithm and self-attention, capturing implicit and\\nexplicit sentimental cues for MABSA.\\nAuxiliary Sentence Generation.\\nInitially, we\\nfine-tuned a pre-trained BLIP (Li et al., 2022) using\\nthe Impression dataset (Kruk et al., 2023) to enable\\nit to generate impression and aesthetic captions.\\nFor a given image V ∈R3×H×W , we then input\\nit into the fine-tuned BLIP model to produce its\\ncorresponding impression and aesthetic captions,\\nresulting in two auxiliary, emotion-rich sentences.\\n3.1\\nMulti-Aesthetic Attributes Aggregation\\nmodule (MA3)\\nMA3 is crucial for capturing complex senti-\\nmental relationships in multimodal data. Fig. 3\\ndisplays details of MA3.\\nIt unifies visual and\\ntextual\\nfeatures,\\nimpressions,\\nand\\naesthetic\\nattributes into a cohesive graph, allowing for\\nprecise modeling of sentimental expressions.\\nThe visual features of the image Vf ∈Rd are\\nobtained using CLIP (Radford et al., 2021),\\nand the hidden features of the input sentence\\nHs =\\n\\x00hs\\n1, hs\\n2, . . . , hs\\nNs\\n\\x01\\n∈RNs×d, impression\\nHr =\\n\\x00hr\\n1, hr\\n2, . . . , hr\\nNr\\n\\x01\\n∈RNr×d, and aesthetic\\ncaption Ha =\\n\\x00ha\\n1, ha\\n2, . . . , ha\\nNa\\n\\x01\\n∈RNa×d are\\nderived using RoBERTa (Liu et al., 2019). Graphs\\noffer a unified and consistent framework for\\nrepresenting and integrating diverse data types.\\nThen, we developed a task-specific aesthetic-aware\\nmultimodal graph (AMG) for each sample. The\\nnodes Hg of the AMG comprise the concatenated\\nhidden representations of the input sentence, visual'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 3}, page_content='Figure 2: Overview of the Vanessa framework, covering a three-stage process: (1) multi-aesthetic attributes\\naggregation, (2) self-supervised contrastive learning for image-text relevance, and (3) dynamic token merge.\\ncontent, impression, and aesthetic caption: Hg =\\n\\x10\\nhg\\n1, hg\\n2, . . . , hg\\nNg\\n\\x11\\n= (hs\\n1, . . . , hs\\nNs; Vf; hr\\n1, . . . ,\\nhr\\nNr; ha\\n1, . . . , ha\\nNa). Ng = Ns + 1 + Nr + Na\\ndenotes the length of the hidden representations.\\nWe define A ∈RNg×Ng as the adjacency matrix of\\nthe AMG, with its elements initially set to zero. To\\nclarify the construction of the AMG, we divide the\\nprocedure into two steps: 1) setting edges to model\\nintra-dependency and 2) setting edges to model\\ninter-dependency.\\n3.1.1\\nModel Intra-dependency\\nThis sub-module improves the understanding of\\nintra-dependencies within text, which is essential\\nfor accurately capturing the relationships among en-\\ntities and their opinion words. Specifically, we em-\\nploy the syntactic dependency tree1 for text, com-\\nbined with a self-attention mechanism, to assign\\nweights to the edges between words tagged with\\nspecific part-of-speech (POS) (Xiao et al., 2022)\\nfor the sentence subgraph AS ∈RNs×Ns as:\\nAS\\ni,j =\\nn\\natt(hs\\ni, hs\\nj),\\nif Di,j, (hs(p)\\ni\\n, hs(p)\\nj\\n)∈POS , (1)\\nwhere att denotes the self-attention mecha-\\nnism (Vaswani et al., 2017). Di,j indicates that\\nthere is a syntactic dependency between words\\nhs\\ni and hs\\nj.\\nhs(p)\\ni\\n, hs(p)\\nj\\nare the POS tags for\\nthe i-th and j-th words, respectively. POS =\\n[nouns, adj, vb, cc, rb].\\n1spaCy toolkit (https://spacy.io).\\nThe subgraphs for impression AR ∈RNr×Nr\\nand aesthetic caption AC ∈RNa×Na are derived\\nvia the similar operation. Since the visual feature\\nVf is a feature vector, we set the intra-dependency\\nas 1 to it.\\n3.1.2\\nModel Inter-dependency\\nTo model the inter-dependency and capture ex-\\nplicit/implicit sentiment cues across different\\nmodalities, it is essential to: (1) track the se-\\nmantic correlations between these modalities and\\n(2) infer the sentiment expressions within the as-\\nsociated textual content.\\nWe define six inter-\\ndependencies: visual-sentence, visual-impression,\\nvisual-aesthetic, sentence-impression, sentence-\\naesthetic, and impression-aesthetic.\\n Multi-Aesthetic Attributes Aggregation (MA3)\\nAesthetic-aware Multimodal \\nDependency Matrix\\nGraph Convolution Network\\nGraph Convolution Network\\nSelf-loop\\nVisual-sent. dep.\\nVisual-impr. dep.\\nVisual-aes. dep.\\nSent-syntax dep.\\nSent-impr dep.\\nSent-aes dep.\\nImpr-syntax dep.\\nNone dep.\\nAes-syntax dep.\\nSent. feat\\nVisual feat\\nImpr. feat\\nAes. feat\\nFigure 3: Details of the Multi-Aesthetic Attributes Ag-\\ngregation (MA3). \"Sent\" is the input sentence, \"Impr\"\\nrefers to the impression, and \"Aes\" indicates the aes-\\nthetic caption.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 4}, page_content='We model cross-modal visual-sentence depen-\\ndencies subgraph AV S ∈R1×Ns by first obtaining\\na skew-symmetric matrix S = S0 −S⊤\\n0 and map\\nit to the special orthogonal group Rf = P∞\\nn=0\\nSn\\nn! ,\\na Lie group (Humphreys, 2012). S0 is a randomly\\ninitialized matrix. Rf is the rotation matrix. Mean-\\nwhile, given a random matrix B ∈Rd×d, the k-th\\ncolumn of the orthogonal matrix Q, denoted qk,\\nis obtained via the Gram-Schmidt process (Leon\\net al., 2013):\\nqk =\\nbk −Pk−1\\nj=1\\n\\x10\\nqj·bk\\nqj·qj\\n\\x11\\nqj\\n\\r\\r\\rbk −Pk−1\\nj=1\\n\\x10\\nqj·bk\\nqj·qj\\n\\x11\\nqj\\n\\r\\r\\r\\n,\\n(2)\\nwhere bk is k-th column of the matrix B ∈Rd×d.\\nThen, we form the composite transformation matrix\\nC = Q−1RfQ to rotate and align features from\\nthe input sentence Hs tagged with specific POS\\nand visual V f modalities while preserving their\\ninherent data structure and characteristics:\\nV ′ = CV f, H′ = Chs\\ni, hs\\ni ∈POSvs,\\n(3)\\nwhere V ′ and H′ are transformed feature represen-\\ntations for vision and text, respectively. Then, we\\ncalculate the alignment loss Lalign = ∥V ′ −H′∥2\\nF\\nbetween them. ∥·∥F denotes the Frobenius norm.\\nFinally, the value assigned to this edge is deter-\\nmined by the product of the CLIP score for the\\nimage-text pair and the Gaussian similarity be-\\ntween the transformed features as follows:\\nGaussian = exp\\n \\n−ρ\\nX\\ni,j\\n\\x00V ′\\nij −H′\\nij\\n\\x012\\n!\\n,\\n(4)\\nAV S\\ni\\n=\\nn\\nclip ∗Sim(V f, hs\\ni),\\nif hs\\ni ∈POSvs ,\\n(5)\\nwhere clip denotes the corresponding CLIP score\\nof the image-text pair. Gaussian is the calculation\\nof Gaussian similarity and −ρ serves as the decay\\nparameter within the Gaussian function. Sim in-\\ndicates the whole calculation process of Gaussian\\nsimilarity from skew-symmetric matrix to equation\\n(4). POSvs ∈[nouns]. The visual-impression\\nsubgraph AV I ∈R1×Nr and the visual-aesthetic\\nsubgraph AV A ∈R1×Na are constructed via simi-\\nlar process:\\nAV I\\ni\\n=\\nn\\nclip ∗Sim(V f, hr\\ni ),\\nif hr(p)\\ni\\n∈POSvi , (6)\\nAV A\\ni\\n=\\nn\\naes ∗Sim(V f, ha\\ni ),\\nif ha(p)\\ni\\n∈POSva , (7)\\nwhere POSvi ∈[adj, rb, verbs] and POSva ∈\\n[nouns, adj, verbs, rb]. aes is the aesthetic score\\nof the image.\\nFor the uni-modal inter-dependency sentence-\\nimpression subgraph ASI\\n∈RNr×Ns and the\\nsentence-aesthetic subgraph ASA ∈RNa×Ns, we\\ncalculate the attention score between the corre-\\nsponding textual representations, and multiply it\\nby the CLIP score of the image-text pair ASI =\\nclip ∗att(Hs, Hr) and the aesthetic score of the\\nimage ASA = aes∗att(Hs, Ha). The impression-\\naesthetic dependency is set to zero, as the corre-\\nlation between these two auxiliary sentences pro-\\nvides limited information for this task. Finally, we\\nestablish a self-loop for each node, Ai,i = 1, in\\nthe AMG, resulting in the complete AMG A ∈\\nRNg×Ng as an undirected graph:\\nA =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nAS\\nAV S\\nASI\\nASA\\n(AV S)T\\n1\\nAV I\\nAV A\\n(ASI)T\\n(AV I)T\\nAR\\n0\\n(ASA)T\\n(AV A)T\\n0\\nAC\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n(8)\\n3.1.3\\nMultimodal Graph Convolution\\nThis sub-module is vital for capturing and mod-\\neling the intra- and inter-dynamics of aesthetic-\\naware sentimental features across different modali-\\nties. We feed the task-specific AMG A ∈RNg×Ng\\nand the corresponding node representations Hg ∈\\nRNg×d into multi-layer GCNs to adaptively model\\nthe intra- and inter-dynamics of aesthetic-aware\\nsentimental features across modalities:\\nGl = ReLU\\n\\x10\\nˆAGl−1W l + bl\\x11\\n,\\n(9)\\nwhere ˆA = D−1\\n2 AD−1\\n2 , D denotes the degree\\nmatrix of A with Dii = P\\nj Aij. Gl−1 represents\\nthe hidden features from the preceding GCN layer.\\nW l and bl are the trainable parameters in the l-th\\nGCN layer. The input for the first GCN layer is the\\nconcatenated multimodal hidden representations,\\ndenoted as G0 = Hg. The multimodal feature\\nGL = {gi}Ng\\ni=1 is derived from this module. Mean-\\nwhile, the hidden features of input sentence Hs\\nare fed into the Transformer encoder to model the\\ntextual features Ht =\\n\\x08\\nht\\ni\\n\\tNs\\ni=1.\\n3.2\\nSSL for Image-text Relevance\\nWe propose a Self-supervised Contrastive Learn-\\ning for Image-Text Relevance (SSL-ITR) module,\\nwhich models the semantic pairwise image-text re-\\nlationship by utilizing the CLIP score and its Gaus-\\nsian distribution. Conventional contrastive learning\\nhelps to distinguish the hidden states of positive\\nand negative samples (Liang et al., 2024; Mao et al.,\\n2024).'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 5}, page_content='SSL-ITR dynamically prioritizes visual or tex-\\ntual modalities, improving the model’s ability to\\ndiscern and utilize relevant multimodal features for\\nbetter performance. The CLIP score (see examples\\nin Fig. 1) is a quantitative metric that evaluates the\\nsemantic alignment between an image and its corre-\\nsponding sentence. Initially, we use CLIP (Radford\\net al., 2021) to obtain CLIP scores for all image-text\\npairs in the dataset and calculate their mean and\\nstandard deviation. Based on the mean value, stan-\\ndard deviation, and twice the standard deviation,\\nwe categorize these CLIP scores into six relevance\\nlevel labels R ∈{r0, r1, r2, r3, r4, r5} , tagging\\nthe image-text pairs with their corresponding rel-\\nevance levels. For multimodal features {gi}Nb\\ni=1\\nwithin each mini-batch B (Nb being the size of the\\nmini-batch), the anchor gi is the sample with the\\nhighest CLIP score. If the relevance level Rj of gj\\nexceeds a specified threshold (e.g., Rj ≥r3), then\\nthe sample is considered a positive pair; otherwise,\\nit is a negative pair. The contrastive loss for all\\npositive pairs is computed as follows:\\nLcon = −1\\nNb\\nX\\ngi∈B\\nlog\\nP\\nj∈B\\\\i I[Rj≥R] exp\\n\\x00f\\n\\x00gi, gj\\n\\x01\\n/τ\\n\\x01\\nP\\nj∈B\\\\i exp\\n\\x00f\\n\\x00gi, gj\\n\\x01\\n/τ\\n\\x01\\n(10)\\nwhere I[Rj≥R] ∈0, 1 is an indicator that evaluates\\nto 1, if Rj is higher than the specified relevance\\nlevel. f\\n\\x00gi, gj\\n\\x01\\n= gi⊤gj/ ∥gi∥\\n\\r\\rgj\\n\\r\\r denotes the\\ncosine similarity between gi and gj. τ indicates the\\ntemperature parameter.\\n3.3\\nDynamic Token Merge\\nThis module is essential for adeptly selecting and\\nmerging aesthetic-aware and emotionally-rich fea-\\ntures at both implicit and explicit levels.\\nWe\\nemploy DPC-KNN (Du et al., 2016; Jin et al.,\\n2023), a KNN-based density peaks clustering al-\\ngorithm, to dynamically select aesthetic-aware and\\nemotionally-rich features by clustering the mixed\\nrepresentations M\\n=\\n(m1, m2, . . . , mNm)\\n=\\n(g1, g2, . . . , gNg, ht\\n1, ht\\n2, . . . , ht\\nNs) of multimodal\\nand textual features (Nm = Ng+Ns). We first pass\\nthe mixed representations to a one-dimensional\\nconvolutional layer, and compute the local den-\\nsity ψi = exp(−1\\nK\\nP\\nmk∈KNN(mi) ∥mk −mi∥2)\\nof each token based on its K-nearest neighbors.\\nKNN (mi) indicates the K-nearest neighbors of\\nmi. Then the distance index γi of each token mi is\\ngiven by:\\nDataset\\nTwitter-2015\\nTwitter-2017\\nTrain\\nDev\\nTest\\nTrain\\nDev\\nTest\\n# POS\\n928\\n303\\n317\\n1508\\n515\\n493\\n# NEU\\n1883\\n670\\n607\\n1638\\n517\\n573\\n# NEG\\n368\\n149\\n113\\n416\\n144\\n168\\n# Total\\n3179\\n1122\\n1037\\n3562\\n176\\n1234\\nTable 1: The statistics of two Twitter datasets. Pos:\\nPositive, Neg: Negative, Neu: Neutral.\\nγi =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nmin\\nj:ψj>ψi∥mk −mi∥2,\\nif ∃j s.t. ψj > ψi\\nmax\\nj\\n∥mk −mi∥2,\\notherwise,\\n(11)\\nwhere ψ refers to the local density of tokens, and\\nγ is the distance from other high-density tokens.\\nSubsequently, tokens with relatively high ψi × γi\\nvalues are identified as cluster centers. The remain-\\ning tokens are assigned to the nearest cluster center\\naccording to Euclidean distances. We represent\\neach cluster by the weighted average of its tokens.\\nThe textual features are then used as Q, and the\\nweighted average tokens are used as K and V in\\na multi-head attention module to generate the fi-\\nnal feature representation Hf ∈RNs×d. Finally,\\nthe Hf is passed into a CRF layer to predict the\\naspect-sentiment sequence Y :\\np(Y ) =\\nexp(s(Hf, Y ))\\nP\\nˆY ∈YHf exp(s(Hf, ˆY ))\\n,\\n(12)\\ns(Hf, Y ) =\\nNs\\nX\\ni=0\\nTyi,yi+1 +\\nNs\\nX\\ni=1\\nhf\\ni · W yi,\\n(13)\\nwhere T is the transition matrix and YHf denotes\\nall possible label sequences for the input sample.\\nThe trainable matrix W yi is utilized to compute the\\nemission score from the token hf\\ni to the label yi.\\n3.4\\nModel Training\\nThe overall loss is the combination of task loss,\\nalignment loss, and contrastive loss:\\nLtotal = −logp(Y ) + αLalign + βLcon,\\n(14)\\nwhere α and β are tradeoff hyper-parameters.\\n4\\nExperiments\\n4.1\\nExperimental Settings\\nDatasets and Evaluation Metrics.\\nWe opt for\\ntwo public multimodal datasets Twitter2015 and\\nTwitter2017 (Yu et al., 2019) to evaluate the per-\\nformance of our Vanessa. An overview of both'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 6}, page_content='datasets is shown in Table 1. Moreover, We eval-\\nuate the performance of our proposed Vanessa on\\nthis task using three standard evaluation metrics:\\nMicro-F1 score (F1), Precision (P), and Recall (R).\\nImplementation\\nDetails.\\nWe\\nemploy\\nRoBERTa (Liu et al., 2019) to initialize the\\nword representations and use CLIP (Radford et al.,\\n2021) to extract visual features and generate the\\nCLIP score. The model is trained for 40 epochs\\nwith a batch size of 16 on the MABSA dataset.\\nBoth learning rates are set to 3 × 10−5, and the\\nhidden sizes are set to 768. The hyper-parameters\\nα and β are set to 1 and 0.5, respectively. Addi-\\ntionally, we stack two layers in the GCNs. The\\naesthetic score of the image is generated using\\nVILA (Ke et al., 2023).\\nCompared Baselines.\\n(1) Text-based baselines:\\nRoBERTa (Liu et al., 2019), BART (Yan et al.,\\n2021), and D-GCN (Chen et al., 2020). (2) Multi-\\nmodal baselines: UMT+TomBERT (Yu and Jiang;\\nYu et al., 2020), OSCGA+TomBERT (Yu and Jiang;\\nWu et al., 2020), OSCGA-collapse (Wu et al.,\\n2020), RpBERT-collapse (Sun et al., 2021), UMT-\\ncollapse (Yu et al., 2020), JML (Ju et al., 2021),\\nVLP-MABSA (Ling et al., 2022), CMMT (Yang\\net al., 2022b), MOCOLNet (Mu et al., 2023),\\nVLP-MABSA-M2DF (Zhao et al., 2023a), At-\\nlantis (Xiao et al., 2024), and AoM (Zhou et al.,\\n2023).\\n4.2\\nMain Results\\nThe main experimental results are presented in Ta-\\nble 2. Firstly, we observe that pre-trained language\\nmodels RoBERTa and BART exhibit superior per-\\nformance within the text-only baselines. Besides,\\nthe multimodal baselines generally outperform the\\ntext-based methods (Cambria, 2024). Secondly,\\namong multimodal baselines, methods that inte-\\ngrate different pipelines into one framework lag\\nsignificantly behind unified frameworks. Last but\\nnot least, Vanessa achieved state-of-the-art perfor-\\nmance, surpassing all baselines. It improved the F1\\nscore by 1.2% and 0.9%, and precision by 1.8% and\\n1.1%, compared to the second-best model, AoM,\\non two datasets. These results verify the effec-\\ntiveness of incorporating visual connotations and\\naesthetic attributes, as well as learning semantic\\nrelevance between text and image via CLIP scores.\\n4.3\\nAblation Study\\nAblation study results are presented in Table 3.\\nAesthetic-aware Multimodal Graph.\\nWe re-\\nmove the AMG and the corresponding nodes for\\nimpression and aesthetic caption in Hg. The sig-\\nnificant performance degradation across all evalua-\\ntion metrics demonstrates that the intra- and inter-\\ndependencies among multimodal features, visual\\nconnotations, and aesthetic attributes modeled by\\nAMG are crucial for capturing complex sentimental\\nrelationships across modalities.\\nImpression.\\nWe remove the impression from the\\nnodes Hg and its corresponding dependencies in\\nAMG, resulting in their exclusion from Vanessa.\\nThe performance decline observed in Table 3 ver-\\nifies the importance and effectiveness of incorpo-\\nrating visual connotations to extract implicit senti-\\nmental cues from images.\\nAesthetic Caption.\\nSimilar to the removal of the\\nimpression, we discard the aesthetic caption from\\nthe nodes Hg and its corresponding dependencies\\nin AMG. As can be seen from Table 3, this re-\\nmoval results in serious performance degradation,\\ndemonstrating that extracting explicit sentimental\\ncues from the visual modality through aesthetic\\nattributes enhances the understanding of visual ele-\\nments, so as to improve MABSA performance.\\nSSL-ITR.\\nFurthermore, Table 3 reveals that the\\nablation of the SSL-ITR module significantly de-\\ngrades performance across all metrics. This find-\\ning verifies the importance of the proposed self-\\nsupervised contrastive learning strategy in compre-\\nhending the semantic relevance of image-text pairs.\\nDynamic Token Merge.\\nWe substitute the DTM\\nmodule with a simple concatenation of multimodal\\nand textual features. As shown in Table 3, the\\nremoval of the DTM module results in perfor-\\nmance degradation, which indicates that integrating\\naesthetic-aware multimodal features with textual\\nfeatures through clustering the most representative\\nneighboring features is effective.\\n4.4\\nAnalysis of Contrastive Learning\\nWe investigate the impact of self-supervised\\ncontrastive learning for image-text relevance in\\nVanessa on representation quality. Specifically, we\\nrecord training checkpoints from the “w/o SSL-\\nITR” variants and the complete Vanessa, and visu-\\nalize the alignment and uniformity metrics of these\\ncheckpoints in Fig. 4. As demonstrated by Wang\\nand Isola (2020), lower Lalign and Luniform lead\\nto better performance.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 7}, page_content='Methods\\nTwitter2015\\nTwitter2017\\nP\\nR\\nF1\\nP\\nR\\nF1\\nText-based\\nRoBERTa (Liu et al., 2019)\\n61.8\\n65.3\\n63.5\\n65.5\\n66.9\\n66.2\\nD-GCN♣(Chen et al., 2020)\\n58.3\\n58.8\\n59.4\\n64.1\\n64.2\\n64.1\\nBART♣(Yan et al., 2021)\\n62.9\\n65.0\\n63.9\\n65.2\\n65.6\\n65.4\\nMultimodal\\nUMT+TomBERT♣(Yu and Jiang; Yu et al., 2020)\\n58.4\\n61.3\\n59.8\\n62.3\\n62.4\\n62.4\\nOSCGA+TomBERT♣(Yu and Jiang; Wu et al., 2020) 61.7\\n63.4\\n62.5\\n63.4\\n64.0\\n63.7\\nOSCGA-collapse♣(Wu et al., 2020)\\n63.1\\n63.7\\n63.2\\n63.5\\n63.5\\n63.5\\nRpBERT-collapse♣(Sun et al., 2021)\\n49.3\\n46.9\\n48.0\\n57.0\\n55.4\\n56.2\\nUMT-collapse♣(Yu et al., 2020)\\n61.0\\n60.4\\n61.6\\n60.8\\n60.0\\n61.7\\nJML♣(Ju et al., 2021)\\n65.0\\n63.2\\n64.1\\n66.5\\n65.5\\n66.0\\nVLP-MABSA♣(Ling et al., 2022)\\n65.1\\n68.3\\n66.6\\n66.9\\n69.2\\n68.0\\nCMMT♣(Yang et al., 2022b)\\n64.6\\n68.7\\n66.5\\n67.6\\n69.4\\n68.5\\nMOCOLNet (Mu et al., 2023)\\n66.3\\n67.8\\n67.1\\n67.2\\n68.7\\n67.9\\nVLP-MABSA-M2DF (Zhao et al., 2023a)\\n66.8\\n68.0\\n67.3\\n67.8\\n68.4\\n68.1\\nAtlantis (Xiao et al., 2024)\\n65.6\\n69.2\\n67.3\\n68.6\\n70.3\\n69.4\\nAoM♣(Zhou et al., 2023)\\n67.9\\n69.3\\n68.6\\n68.4\\n71.0\\n69.7\\nVanessa (Ours)\\n68.6\\n71.1\\n69.8*\\n69.2\\n72.1\\n70.6*\\nTable 2: MABSA evaluation results. ♣denotes the results from (Zhou et al., 2023). * denotes the improvement is\\nstatistically significant on a two-tailed t-test (p < 0.001). We color each row as the best and second best .\\nMethods\\nTwitter2015\\nTwitter2017\\nP\\nR\\nF1\\nP\\nR\\nF1\\nVanessa\\n68.6\\n71.1\\n69.8\\n69.2\\n72.1\\n70.6\\nw/o AMG\\n66.8\\n68.9\\n67.5\\n67.4\\n69.3\\n67.7\\nw/o Impr\\n67.7\\n70.0\\n68.8\\n68.3\\n70.6\\n69.5\\nw/o Aes-cap\\n67.1\\n69.4\\n68.1\\n68.0\\n70.2\\n69.2\\nw/o SSL-ITR 67.5\\n69.3\\n68.3\\n67.9\\n69.7\\n68.8\\nw/o DTM\\n67.8\\n70.2\\n68.5\\n68.4\\n70.7\\n69.4\\nTable 3: Ablation study results for the Vanessa. We\\ncolor each row as the best and second best .\\nFigure 4: Visualization of contrastive representations\\nfor checkpoints at 40 training step intervals.\\nIn Fig. 4, Vanessa consistently exhibits lower\\nLalign and Luniform values compared to the\\n“w/o SSL-ITR” variant during training, which indi-\\ncates that using the CLIP score improves Vanessa’s\\nability to learn sentimental clues for MABSA.\\n4.5\\nCase Study\\nFig. 5 presents two examples, accompanied by pre-\\ndictions from CMMT, AoM, and Vanessa. In ex-\\nample (a), CMMT incorrectly predicts the senti-\\nment polarity for both “Cape Town” and “Regardt\\nStander”, whereas AoM only misclassifies the sen-\\ntiment for “Regardt Stander”. Vanessa accurately\\npredicts the sentiments of both entities by effec-\\ntively utilizing emotion-laden descriptions derived\\nfrom impression and aesthetic attributes. This in-\\ndicates Vanessa’s excellent capability in capturing\\nand integrating implicit and explicit sentimental\\ncues. In example (b), both CMMT and AoM in-\\ncorrectly predict the sentiment of “LeBron James”.\\nDue to the low semantic relevance between the\\nimage and text (CLIP score = 0.06), Vanessa pri-\\nmarily focuses on the text and accurately predicts\\nthe sentiment for both “LeBron James” and “NBA”.\\nThese observations highlight Vanessa’s adaptabil-\\nity in handling scenarios with varying levels of\\nsemantic relevance across modalities, ensuring ro-\\nbust sentiment predictions when the visual context\\nprovides minimal relevant information.\\n4.6\\nQuantitative Analysis\\nWe perform quantitative analysis to investigate the\\nrelationship between the impressions, aesthetic cap-\\ntions and our Vanessa across the test sets of two\\ndatasets. We input the hidden features of impres-\\nsions and aesthetic captions produced by RoBERTa\\ninto a pre-trained TweetNLP (Loureiro et al., 2022)\\nto obtain their sentiment distributions.\\nSubse-\\nquently, we visualize the sentiment distributions\\nof these auxiliary sentences alongside Vanessa’s\\npredictions in the embedding space using the T-\\nSNE (Van der Maaten and Hinton, 2008), as il-\\nlustrated in Fig. 6. In Fig. 6 (a), impressions ex-\\nhibit a bias toward positive samples, potentially\\nintroducing ambiguity in the training process. In\\ncontrast, Fig. 6 (b) shows that aesthetic captions'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 8}, page_content='Image\\nText\\n(a) The city of [Cape Town] P o s at \\nnight...photo by [Regardt Stander]Pos .\\n(b) [LeBron James]Neu to Produce [NBA]Neu \\nDocumentary -\\nImpr.\\nI feel a sense of awe and wonder when i look at this \\nimage. The city lights and the dark background create a \\nsense of tranquility and tranquility.\\nI feel a sense of curiosity and intrigue about the significance of \\nthe company\\'s logo and its significance in the field of \\nadvertising. \\nAes-Cap\\nThe visual elements that contribute to my impression include \\nthe people standing in front of the sign, which represents the \\ncompany\\'s commitment to promoting the brand.\\nThe lighting of the city is very beautiful and the lighting \\nof the city is very beautiful.  \\nCMMT\\nAoM\\nVanessa\\n(Cape Town, Negative       )\\n(LeBron James, Positive        )\\n(NBA, Neutral         )\\n(Cape Town, Positive       )\\n(Cape Town, Positive       )\\n(Regardt Stander, Neutral        )\\n(Regardt Stander, Neutral        )\\n(Regardt Stander, Positive        )\\nCLIP score: 0.61\\nCLIP score: 0.06\\n(LeBron James, Positive        )\\n(NBA, Neutral         )\\n(LeBron James, Neutral        )\\n(NBA, Neutral         )\\nFigure 5: Two examples with predictions made by\\nCMMT, AoM and Vanessa. The ground truth aspect-\\nsentiment pair is annotated within the text.\\n(a) Impression\\n(b) Aesthetic\\n(c) Vanessa\\nTwitter2015\\nTwitter2017\\nFigure 6: Visualization of sentiment distributions for\\nauxiliary sentences and Vanessa’s predictions.\\npresent more distinct and separated sentiment clus-\\nters, likely providing clearer signals for model\\nlearning.\\nFrom Fig. 6 (c), the majority of the\\nVanessa-predicted NEU samples coincide with the\\nNEU samples in the aesthetic captions distribu-\\ntion. A subset of Vanessa-predicted NEG samples\\noverlaps with NEG samples in both aesthetic cap-\\ntions and impressions. Despite noticeable differ-\\nences in the distribution of POS samples between\\nmodel predictions and both aesthetic captions and\\nimpressions, a degree of similarity is observed in\\nthe right half of the plots. In summary, given the in-\\ntricate sentimental cues and alignment challenges\\nin MABSA (Mao et al., 2025), we hypothesize\\nthat aesthetic captions offer more definitive senti-\\nmental cues compared to impressions on these test\\nsets. Our ablation study supports this hypothesis,\\nas the \"w/o Aes-cap\" variant results in greater per-\\nformance degradation than the \"w/o Impr\" variant.\\n5\\nConclusion\\nWe proposed a novel Visual Connotation and\\nAesthetic\\nAttributes\\nUnderstanding\\nNetwork\\n(Vanessa) for MABSA. Firstly, the MA3 module\\nadaptively modeled the intra- and inter-dynamics of\\naesthetic-aware emotions across modalities. Sub-\\nsequently, the SSL-ITR module dynamically prior-\\nitized visual or textual modalities to improve the\\nmodel’s ability to discern and utilize relevant mul-\\ntimodal features. Finally, the DTM module adeptly\\nselected and merged aesthetic-aware and emotion-\\nally rich features at both implicit and explicit levels.\\nExperimental results on two widely used Twitter\\ndatasets verified the effectiveness of our Vanessa.\\nLimitations\\nThe proposed Vanessa has the following limitations.\\nFirstly, the aesthetic-aware multimodal dependency\\nmatrix is a homogeneous graph, which limits its\\nability to represent diverse features. This constraint\\nhinders the model’s capacity to deeply explore the\\nintra- and inter-dynamics between bi-modality, vi-\\nsual connotation, and aesthetic attributes. Future\\nwork will focus on constructing a heterogeneous\\ngraph to better model the diverse data, enhancing\\nthe model’s ability to analyze complex multimodal\\nrelationships. Secondly, the generated impressions\\nand aesthetic captions are not well-aligned with\\nspecific targets within the sentences, as the gener-\\nated content predominantly pertains to the image\\nand lacks sufficient relation to the specific targets.\\nThirdly, the reliability of results is paramount for\\napplications ranging from market research to so-\\ncial media monitoring. Enhancing the robustness\\nof models against abnormal or malicious inputs is\\nessential to maintain this reliability (Zhao et al.,\\n2023b, 2024).\\nEthics Statement\\nThis article adheres to the ACL Code of Ethics. The\\ndatasets utilized do not contain sensitive private\\ninformation and pose no harm to society. The pro-\\nposed method is for multimodal sentiment analysis\\nand enhancing machine understanding of human\\nsentiment. To the best of our knowledge, there are\\nno foreseeable risks associated with this technique.\\nAcknowledgments\\nThis research/project is funded by the Science and\\nTechnology Commission of Shanghai Municipal-\\nity Grant (No. 22511105901) and the Ministry of\\nEducation, Singapore under its MOE Academic Re-\\nsearch Fund Tier 2 (STEM RIE2025 Award MOE-\\nT2EP20123-0005).'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 9}, page_content='References\\nArjun R. Akula,\\nBrendan Driscoll,\\nPradyumna\\nNarayana, Soravit Changpinyo, Zhiwei Jia, Suyash\\nDamle, Garima Pruthi, Sugato Basu, Leonidas\\nGuibas, William T. Freeman, Yuanzhen Li, and Varun\\nJampani. 2023. Metaclue: Towards comprehensive\\nvisual metaphors research. 2023 IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition\\n(CVPR).\\nRudolf Arnheim. 1954. Art and visual perception: A\\npsychology of the creative eye. Univ of California\\nPress.\\nLisa Feldman Barrett and Moshe Bar. 2009. See it with\\nfeeling: affective predictions during object percep-\\ntion. Philosophical Transactions of the Royal Society\\nB: Biological Sciences, 364(1521):1325–1334.\\nJohn Berger. 1972. Ways of seeing. BBC and Penguin.\\nErik Cambria. 2024. Understanding Natural Language\\nUnderstanding. Springer, ISBN 978-3-031-73973-6.\\nErik Cambria, Newton Howard, Jane Hsu, and Amir\\nHussain. 2013.\\nSentic blending: Scalable multi-\\nmodal fusion for continuous interpretation of seman-\\ntics and sentics. In IEEE SSCI, pages 108–117.\\nErik Cambria, Rui Mao, Melvin Chen, Zhaoxia Wang,\\nand Seng-Beng Ho. 2023. Seven pillars for the future\\nof artificial intelligence. IEEE Intelligent Systems,\\n38(6):62–69.\\nErik Cambria, Xulang Zhang, Rui Mao, Melvin Chen,\\nand Kenneth Kwok. 2024.\\nSenticNet 8: Fusing\\nemotion AI and commonsense AI for interpretable,\\ntrustworthy, and explainable affective computing. In\\nProceedings of International Conference on Human-\\nComputer Interaction (HCII), Washington DC, USA.\\nTuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn,\\nArtemis Panagopoulou, Yue Yang, Marianna Apid-\\nianaki, and Smaranda Muresan. 2023.\\nI spy a\\nmetaphor: Large language models and diffusion mod-\\nels co-create visual metaphors. In Findings of the As-\\nsociation for Computational Linguistics: ACL 2023,\\npages 7370–7388. Association for Computational\\nLinguistics.\\nGuimin Chen, Yuanhe Tian, and Yan Song. 2020. Joint\\naspect extraction and sentiment analysis with direc-\\ntional graph convolutional networks. In Proceed-\\nings of the 28th international conference on compu-\\ntational linguistics, pages 272–279.\\nKelvin Du, Rui Mao, Frank Xing, and Erik Cambria.\\n2024. Explainable stock price movement prediction\\nusing contrastive learning. In Proceedings of the\\n33rd ACM International Conference on Information\\nand Knowledge Management (CIKM), Idaho, USA.\\nMingjing Du, Shifei Ding, and Hongjie Jia. 2016. Study\\non density peaks clustering based on k-nearest neigh-\\nbors and principal component analysis. Knowledge-\\nBased Systems, 99:135–145.\\nChunxiao Fan, Jie Lin, Rui Mao, and Erik Cam-\\nbria. 2024. Fusing pairwise modalities for emotion\\nrecognition in conversations. Information Fusion,\\n106:102306.\\nXianjie Guo, Kui Yu, Lin Liu, and Jiuyong Li. 2024.\\nFedcsl: A scalable and accurate approach to feder-\\nated causal structure learning. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence, vol-\\nume 38, pages 12235–12243.\\nXianjie Guo, Kui Yu, Lin Liu, Peipei Li, and Jiuyong Li.\\n2023. Adaptive skeleton construction for accurate\\ndag learning. IEEE Transactions on Knowledge and\\nData Engineering, 35(10):10526–10539.\\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan\\nLe Bras, and Yejin Choi. 2021.\\nClipscore:\\nA\\nreference-free evaluation metric for image captioning.\\nIn Proceedings of the 2021 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n7514–7528.\\nLinmei Hu, Ziwei Chen, Ziwang Zhao, Jianhua Yin, and\\nLiqiang Nie. 2022. Causal inference for leveraging\\nimage-text matching bias in multi-modal fake news\\ndetection. IEEE Transactions on Knowledge and\\nData Engineering, 35(11):11141–11152.\\nJames E Humphreys. 2012. Introduction to Lie alge-\\nbras and representation theory, volume 9. Springer\\nScience & Business Media.\\nPeng Jin, Jinfa Huang, Pengfei Xiong, Shangxuan Tian,\\nChang Liu, Xiangyang Ji, Li Yuan, and Jie Chen.\\n2023.\\nVideo-text as game players: Hierarchical\\nbanzhaf interaction for cross-modal representation\\nlearning. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition,\\npages 2472–2482.\\nXincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\\nShoushan Li, Min Zhang, and Guodong Zhou. 2021.\\nJoint multi-modal aspect-sentiment analysis with aux-\\niliary cross-modal relation detection. In Proceedings\\nof the 2021 conference on empirical methods in natu-\\nral language processing, pages 4395–4405.\\nJunjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Pey-\\nman Milanfar, and Feng Yang. 2023. Vila: Learn-\\ning image aesthetics from user comments with\\nvision-language pretraining. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, pages 10041–10051.\\nZaid Khan and Yun Fu. 2021. Exploiting bert for mul-\\ntimodal target sentiment classification through input\\nspace translation. In Proceedings of the 29th ACM\\ninternational conference on multimedia, pages 3034–\\n3042.\\nJulia Kruk, Caleb Ziems, and Diyi Yang. 2023. Impres-\\nsions: Visual semiotics and aesthetic impact under-\\nstanding. In Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 12273–12291.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 10}, page_content='Peter Lang and Margaret M Bradley. 2007. The interna-\\ntional affective picture system (iaps) in the study of\\nemotion and attention. Handbook of emotion elicita-\\ntion and assessment, 29:70–73.\\nSteven J Leon, Åke Björck, and Walter Gander. 2013.\\nGram-schmidt orthogonalization: 100 years and\\nmore. Numerical Linear Algebra with Applications,\\n20(3):492–532.\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\\n2023. Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large lan-\\nguage models. In International conference on ma-\\nchine learning, pages 19730–19742. PMLR.\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\\nHoi. 2022. Blip: Bootstrapping language-image pre-\\ntraining for unified vision-language understanding\\nand generation.\\nBin Liang, Lin Gui, Yulan He, Erik Cambria, and\\nRuifeng Xu. 2024. Fusion and discrimination: A\\nmultimodal graph contrastive learning framework for\\nmultimodal sarcasm detection. IEEE Transactions\\non Affective Computing, 15.\\nYan Ling, Jianfei Yu, and Rui Xia. 2022.\\nVision-\\nlanguage pre-training for multimodal aspect-based\\nsentiment analysis. In Proceedings of the 60th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 2149–\\n2159.\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\\nLee. 2024. Improved baselines with visual instruc-\\ntion tuning. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition,\\npages 26296–26306.\\nHui Liu, Wenya Wang, and Haoliang Li. 2022. To-\\nwards multi-modal sarcasm detection via hierarchical\\ncongruity modeling with knowledge enhancement.\\nIn Proceedings of the 2022 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n4995–5006.\\nHui Liu, Wenya Wang, and Haoliang Li. 2023a. Inter-\\npretable multimodal misinformation detection with\\nlogic reasoning. In Findings of the Association for\\nComputational Linguistics: ACL 2023, pages 9781–\\n9796.\\nHui Liu, Wenya Wang, Hao Sun, Anderson Rocha, and\\nHaoliang Li. 2023b. Robust domain misinformation\\ndetection via multi-modal feature alignment. IEEE\\nTransactions on Information Forensics and Security.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nproach. arXiv preprint arXiv:1907.11692.\\nDaniel Loureiro, Francesco Barbieri, Leonardo Neves,\\nLuis Espinosa Anke, and Jose Camacho-collados.\\n2022. TimeLMs: Diachronic language models from\\nTwitter. In Proceedings of the 60th Annual Meet-\\ning of the Association for Computational Linguistics:\\nSystem Demonstrations, pages 251–260, Dublin, Ire-\\nland. Association for Computational Linguistics.\\nQiang Lu, Xia Sun, Yunfei Long, Zhizezhang Gao, Jun\\nFeng, and Tao Sun. 2023. Sentiment analysis: Com-\\nprehensive reviews, recent advances, and open chal-\\nlenges. IEEE Transactions on Neural Networks and\\nLearning Systems.\\nRui Mao, Kelvin Du, Yu Ma, Luyao Zhu, and Erik\\nCambria. 2023. Discovering the cognition behind\\nlanguage: Financial metaphor analysis with MetaPro.\\nIn 2023 IEEE International Conference on Data Min-\\ning (ICDM), pages 1211–1216. IEEE.\\nRui Mao, Mengshi Ge, Sooji Han, Wei Li, Kai He,\\nLuyao Zhu, and Erik Cambria. 2025. A survey on\\npragmatic processing techniques. Information Fu-\\nsion, 114:102712.\\nRui Mao, Kai He, Claudia Beth Ong, Qian Liu, and\\nErik Cambria. 2024. MetaPro 2.0: Computational\\nmetaphor processing on the effectiveness of anoma-\\nlous language modeling. In Findings of the Associa-\\ntion for Computational Linguistics: ACL, pages 9891–\\n9908, Bangkok, Thailand. Association for Computa-\\ntional Linguistics.\\nRui Mao and Xiao Li. 2021. Bridging towers of multi-\\ntask learning with a gating mechanism for aspect-\\nbased sentiment analysis and sequential metaphor\\nidentification. In Proceedings of the AAAI conference\\non artificial intelligence, volume 35, pages 13534–\\n13542.\\nJie Mu, Feiping Nie, Wei Wang, Jian Xu, Jing Zhang,\\nand Han Liu. 2023. Mocolnet: A momentum con-\\ntrastive learning network for multimodal aspect-level\\nsentiment analysis. IEEE Transactions on Knowl-\\nedge and Data Engineering.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models from\\nnatural language supervision. In International confer-\\nence on machine learning, pages 8748–8763. PMLR.\\nVilayanur S Ramachandran and William Hirstein. 1999.\\nThe science of art: A neurological theory of aesthetic\\nexperience. Journal of consciousness Studies, 6(6-\\n7):15–51.\\nLin Sun, Jiquan Wang, Kai Zhang, Yindu Su, and Fang-\\nsheng Weng. 2021. Rpbert: a text-image relation\\npropagation-based bert model for multimodal ner.\\nIn Proceedings of the AAAI conference on artificial\\nintelligence, volume 35, pages 13860–13868.\\nYosephine Susanto, Andrew Livingstone, Bee Chin Ng,\\nand Erik Cambria. 2020. The Hourglass Model revis-\\nited. IEEE Intelligent Systems, 35(5):96–102.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 11}, page_content='Ana Valdivia, Victoria Luzón, Erik Cambria, and Fran-\\ncisco Herrera. 2018. Consensus vote models for de-\\ntecting and filtering neutrality in sentiment analysis.\\nInformation Fusion, 44:126–135.\\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\\nVisualizing data using t-sne. Journal of machine\\nlearning research, 9(11).\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. Advances in neural information processing\\nsystems, 30.\\nQianlong Wang, Hongling Xu, Zhiyuan Wen, Bin Liang,\\nMin Yang, Bing Qin, and Ruifeng Xu. 2023. Image-\\nto-text conversion and aspect-oriented filtration for\\nmultimodal aspect-based sentiment analysis. IEEE\\nTransactions on Affective Computing.\\nTongzhou Wang and Phillip Isola. 2020. Understanding\\ncontrastive representation learning through alignment\\nand uniformity on the hypersphere. In International\\nconference on machine learning, pages 9929–9939.\\nPMLR.\\nZhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen,\\nHo-fung Leung, and Qing Li. 2020. Multimodal rep-\\nresentation with embedded visual guiding objects for\\nnamed entity recognition in social media posts. In\\nProceedings of the 28th ACM International confer-\\nence on multimedia, pages 1038–1046.\\nLuwei Xiao, Xingjiao Wu, Junjie Xu, Weijie Li, Cheng\\nJin, and Liang He. 2024. Atlantis: Aesthetic-oriented\\nmultiple granularities fusion network for joint multi-\\nmodal aspect-based sentiment analysis. Information\\nFusion, page 102304.\\nLuwei Xiao, Xingjiao Wu, Shuwen Yang, Junjie Xu, Jie\\nZhou, and Liang He. 2023. Cross-modal fine-grained\\nalignment and fusion network for multimodal aspect-\\nbased sentiment analysis. Information Processing &\\nManagement, 60(6):103508.\\nLuwei Xiao, Yun Xue, Hua Wang, Xiaohui Hu,\\nDonghong Gu, and Yongsheng Zhu. 2022. Exploring\\nfine-grained syntactic information for aspect-based\\nsentiment classification with dual graph neural net-\\nworks. Neurocomputing, 471:48–59.\\nNan Xu, Wenji Mao, and Guandan Chen. 2019. Multi-\\ninteractive memory network for aspect based mul-\\ntimodal sentiment analysis.\\nIn Proceedings of\\nthe AAAI Conference on Artificial Intelligence, vol-\\nume 33, pages 371–378.\\nHang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng\\nZhang. 2021. A unified generative framework for\\naspect-based sentiment analysis.\\nIn Proceedings\\nof the 59th Annual Meeting of the Association for\\nComputational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing\\n(Volume 1: Long Papers), pages 2416–2429.\\nShuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui\\nTang. 2023. Clip-driven fine-grained text-image per-\\nson re-identification. IEEE Transactions on Image\\nProcessing.\\nHao Yang, Yanyan Zhao, and Bing Qin. 2022a. Face-\\nsensitive image-to-emotional-text cross-modal trans-\\nlation for multimodal aspect-based sentiment analy-\\nsis. In Proceedings of the 2022 Conference on Empir-\\nical Methods in Natural Language Processing, pages\\n3324–3335.\\nLi Yang, Jin-Cheon Na, and Jianfei Yu. 2022b. Cross-\\nmodal multitask transformer for end-to-end multi-\\nmodal aspect-based sentiment analysis. Information\\nProcessing & Management, 59(5):103038.\\nLi Yang, Jieming Wang, Jin-Cheon Na, and Jianfei\\nYu. 2023. Generating paraphrase sentences for mul-\\ntimodal entity-category-sentiment triple extraction.\\nKnowledge-Based Systems, 278:110823.\\nLi Yang, Zengzhi Wang, Ziyan Li, Jin-Cheon Na, and\\nJianfei Yu. 2024. An empirical study of multimodal\\nentity-based sentiment analysis with chatgpt: Improv-\\ning in-context learning via entity-aware contrastive\\nlearning. Information Processing & Management,\\n61(4):103724.\\nJianfei Yu, Kai Chen, and Rui Xia. 2022a. Hierarchical\\ninteractive multimodal transformer for aspect-based\\nmultimodal sentiment analysis. IEEE Transactions\\non Affective Computing.\\nJianfei Yu and Jing Jiang. Adapting bert for target-\\noriented multimodal sentiment classification.(2019).\\nIn Proceedings of the Twenty-Eighth International\\nJoint Conference on Artificial Intelligence, pages\\n5408–5414.\\nJianfei Yu, Jing Jiang, and Rui Xia. 2019.\\nEntity-\\nsensitive attention and fusion network for entity-level\\nmultimodal sentiment classification.\\nIEEE/ACM\\nTransactions on Audio, Speech, and Language Pro-\\ncessing, 28:429–439.\\nJianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020.\\nImproving multimodal named entity recognition via\\nentity span detection with unified multimodal trans-\\nformer. In Proceedings of the 58th Annual Meeting of\\nthe Association for Computational Linguistics, pages\\n3342–3352.\\nJianfei Yu, Jieming Wang, Rui Xia, and Junjie Li. 2022b.\\nTargeted multimodal sentiment classification based\\non coarse-to-fine grained image-target matching. In\\nIJCAI, pages 4482–4488.\\nTan Yue, Rui Mao, Heng Wang, Zonghai Hu, and Erik\\nCambria. 2023. KnowleNet: Knowledge fusion net-\\nwork for multimodal sarcasm detection. Information\\nFusion, 100:101921.\\nHui Zeng, Zisheng Cao, Lei Zhang, and Alan C Bovik.\\n2019. A unified probabilistic formulation of image\\naesthetic assessment. IEEE Transactions on Image\\nProcessing, 29:1548–1561.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 12}, page_content='Dong Zhang, Suzhong Wei, Shoushan Li, Hanqian Wu,\\nQiaoming Zhu, and Guodong Zhou. 2021. Multi-\\nmodal graph fusion for named entity recognition\\nwith targeted visual guidance. In Proceedings of the\\nAAAI conference on artificial intelligence, volume 35,\\npages 14347–14355.\\nHonglei Zhang, He Liu, Haoxuan Li, and Yidong Li.\\n2024a. Transfr: Transferable federated recommen-\\ndation with pre-trained language models.\\narXiv\\npreprint arXiv:2402.01124.\\nHonglei Zhang, Xin Zhou, Zhiqi Shen, and Yidong\\nLi. 2024b. Privfr: Privacy-enhanced federated rec-\\nommendation with shared hash embedding. IEEE\\nTransactions on Neural Networks and Learning Sys-\\ntems.\\nWenxuan Zhang, Xin Li, Yang Deng, Lidong Bing,\\nand Wai Lam. 2022. A survey on aspect-based senti-\\nment analysis: Tasks, methods, and challenges. IEEE\\nTransactions on Knowledge and Data Engineering,\\n35(11):11019–11038.\\nXulang Zhang, Rui Mao, Kai He, and Erik Cambria.\\n2023. Neuro-symbolic sentiment analysis with dy-\\nnamic word sense disambiguation. In EMNLP, pages\\n8772–8783.\\nFei Zhao, Chunhui Li, Zhen Wu, Yawen Ouyang, Jian-\\nbing Zhang, and Xinyu Dai. 2023a. M2df: Multi-\\ngrained multi-curriculum denoising framework for\\nmultimodal aspect-based sentiment analysis. In Pro-\\nceedings of the 2023 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 9057–\\n9070.\\nShuai Zhao, Luu Anh Tuan, Jie Fu, Jinming Wen, and\\nWeiqi Luo. 2024. Exploring clean label backdoor\\nattacks and defense in language models. IEEE/ACM\\nTransactions on Audio, Speech, and Language Pro-\\ncessing.\\nShuai Zhao, Jinming Wen, Anh Luu, Junbo Zhao, and\\nJie Fu. 2023b. Prompt as triggers for backdoor at-\\ntack: Examining the vulnerability in language mod-\\nels. In Proceedings of the 2023 Conference on Empir-\\nical Methods in Natural Language Processing, pages\\n12303–12317.\\nJie Zhou, Jiabao Zhao, Jimmy Xiangji Huang, Qin-\\nmin Vivian Hu, and Liang He. 2021. Masad: A large-\\nscale dataset for multimodal aspect-based sentiment\\nanalysis. Neurocomputing, 455:47–58.\\nRu Zhou, Wenya Guo, Xumeng Liu, Shenglong Yu,\\nYing Zhang, and Xiaojie Yuan. 2023. Aom: De-\\ntecting aspect-oriented information for multimodal\\naspect-based sentiment analysis.\\nIn Findings of\\nthe Association for Computational Linguistics: ACL\\n2023, pages 8184–8196.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 13}, page_content='A\\nAppendix\\nA.1\\nImpact of different relation detection\\nTo further assess the effectiveness of our substitu-\\ntion for noise reduction techniques, we conducted\\nablation studies by removing the SSL-ITR module\\nfrom our framework. In this setup, we directly in-\\nput the image-text pairs into the Relation Detection\\nmodule as described in (Ju et al., 2021). Consistent\\nwith the methodology in (Ju et al., 2021), we cal-\\nculated both soft and hard relation scores between\\nthe modalities. These relation scores were used to\\nweight the image-related features—including those\\nextracted by CLIP, impressions, and aesthetic cap-\\ntions. The weighted features were then fed into\\nthe MA3 module to obtain the final MABSA re-\\nsults. The experimental outcomes are summarized\\nin Table 4.\\nMethods\\nTwitter2015\\nTwitter2017\\nP\\nR\\nF1\\nP\\nR\\nF1\\nVanessa(hard)\\n65.7\\n67.6\\n66.5\\n67.1\\n69.6\\n68.1\\nVanessa(soft)\\n66.5\\n69.0\\n67.6\\n68.4\\n70.8\\n69.3\\nVanessa(SSL-ITR)\\n68.6\\n71.1\\n69.8\\n69.2\\n72.1\\n70.6\\nTable 4: The results of different relation detection meth-\\nods of Vanessa. Vanessa (hard)\" refers to the hard rela-\\ntion score, while \"Vanessa (soft)\" denotes the soft rela-\\ntion score. Vanessa(SSL-ITR) is our proposed method.\\nWe color each row as the best .\\nAs illustrated in Table 4, the inclusion of\\nthe SSL-ITR module significantly enhances the\\nmodel’s performance across both datasets. Specifi-\\ncally, our vanessa with SSL-ITR achieves the high-\\nest F1 scores of 69.8% on Twitter2015 and 70.6%\\non Twitter2017, outperforming the versions with-\\nout SSL-ITR by a considerable margin. The models\\nutilizing hard and soft relation scores without SSL-\\nITR exhibit lower F1 scores, indicating that the ab-\\nsence of the SSL-ITR module impairs the model’s\\nability to effectively reduce noise and capture the\\nnuanced interactions between modalities. These\\nresults demonstrate that the SSL-ITR module plays\\na crucial role in enhancing semantic alignment be-\\ntween images and text by effectively filtering out\\nirrelevant or noisy information. By leveraging self-\\nsupervised contrastive learning under the guidance\\nof CLIP scores, the SSL-ITR module improves the\\nquality of the image-text representations, boosting\\nthe overall performance of the MABSA task.\\n(a) Hyper-parameter α\\n(b) Hyper-parameter β\\nFigure 7:\\nEffect of the hyper-parameters on two\\ndatasets.\\nA.2\\nImpact of hyper-parameters\\nWe conducted extra experiments to evaluate the\\nimpact of hyper-parameters on Vanessa’s perfor-\\nmance. The α modulates the influence of con-\\ntrastive learning on image-text relevance and β\\ncontrols the strength of the transformation matrix\\nthat aligns image and text feature spaces. From\\nFig. 7(a), we observed that as α increases, the\\nmodel’s performance improves, reaching its op-\\ntimal value at α = 1. This observation suggests\\nthat the contrastive loss is most effective when bal-\\nanced appropriately, allowing the model to learn\\npractical representations of image-text relevance.\\nIn Fig. 7(b), as the value of β varies, the perfor-\\nmance of Vanessa exhibits relatively minor fluctu-\\nations, reaching its peak at β = 0.5. This optimal\\nsetting indicates that the transformation matrix ef-\\nfectively aligns the image and text representations\\nwhen the contribution of β is neither too weak nor\\ntoo strong. In conclusion, optimal tuning of both α\\nand β is essential for balancing semantic alignment\\nwith preserving the unique characteristics of each\\nmodality. Moderate values maximize performance\\nby enhancing multimodal integration without over-\\nfitting or distorting feature spaces, as reflected in\\nthe results from both datasets.\\nA.3\\nComparison with large multimodal\\nmodels\\nWe further conducted a comparative evaluation\\nagainst the open-source Multimodal Large Lan-\\nguage Model (MLLM) LLaVA-1.5-7b (Liu et al.,\\n2024) in a zero-shot setting on the test set. As il-\\nlustrated in Table 5, the proposed Vanessa substan-\\ntially outperforms LLaVA-1.5-7b across all evalua-\\ntion metrics on both datasets. Vanessa achieves F1\\nscores that are more than twice those of LLaVA-\\n1.5-7b (32.5% and 34.4%, respectively). This sig-\\nnificant improvement is also reflected in the preci-\\nsion and recall, where Vanessa consistently demon-\\nstrates superior performance. These results under-\\nscore the efficacy of our task-specific approach in\\nthe domain of MABSA. While LLaVA-1.5-7b, as a'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-02T05:50:18+00:00', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'file_path': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/vanessa.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-02T05:50:18+00:00', 'trapped': '', 'modDate': 'D:20241002055018Z', 'creationDate': 'D:20241002055018Z', 'page': 14}, page_content='Methods\\nTwitter2015\\nTwitter2017\\nP\\nR\\nF1\\nP\\nR\\nF1\\nLLaVA-1.5-7b\\n30.8\\n33.7\\n32.5\\n33.3\\n35.6\\n35.2\\nVanessa\\n68.6\\n71.1\\n69.8\\n69.2\\n72.1\\n70.6\\nTable 5: Main results compared with LLaVA-1.5-7b.\\nWe color each row as the best .\\nlarge-scale MLLM, offers generalizability and has\\nshown impressive capabilities in zero-shot settings,\\nit falls short in capturing the fine-grained sentiment\\ncues present in multimodal social media data. In\\ncontrast, Vanessa is explicitly designed to model\\nthe intricate relationships between images and text.\\nBy incorporating specialized components such as\\nthe Multi-Aesthetic Attributes Aggregation (MA3)\\nmodule and the Self-Supervised Image-Text Rele-\\nvance (SSL-ITR) module, Vanessa effectively cap-\\ntures both explicit and implicit sentimental cues.')]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(pdf_documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "eUM4JeyDP0BV",
        "outputId": "69318d86-a5af-4bd0-93be-2db3f2250af8"
      },
      "id": "eUM4JeyDP0BV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 256);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c879503",
      "metadata": {
        "id": "6c879503"
      },
      "source": [
        "### RAG Pipelines- Data Ingestion to Vector DB Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "C540T3qKChmZ"
      },
      "id": "C540T3qKChmZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "D_tDYnDA7rHY"
      },
      "id": "D_tDYnDA7rHY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev7VLdUS99iS",
        "outputId": "7172f621-1d32-47bb-d19b-3174cf9cb73b"
      },
      "id": "ev7VLdUS99iS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d6377b9",
      "metadata": {
        "id": "8d6377b9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a312dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a312dc",
        "outputId": "c119347d-c4af-4a5e-ba1f-33275a3acfb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 PDF files to process\n",
            "\n",
            "Processing: AoM.pdf\n",
            "  ✓ Loaded 11 pages\n",
            "\n",
            "Total documents loaded: 11\n"
          ]
        }
      ],
      "source": [
        "### Read all the pdf's inside the directory\n",
        "def process_all_pdfs(pdf_directory):\n",
        "    \"\"\"Process all PDF files in a directory\"\"\"\n",
        "    all_documents = []\n",
        "    pdf_dir = Path(pdf_directory)\n",
        "\n",
        "    # Find all PDF files recursively\n",
        "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "        try:\n",
        "            loader = PyPDFLoader(str(pdf_file))\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Add source information to metadata\n",
        "            for doc in documents:\n",
        "                doc.metadata['source_file'] = pdf_file.name\n",
        "                doc.metadata['file_type'] = 'pdf'\n",
        "\n",
        "            all_documents.extend(documents)\n",
        "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
        "    return all_documents\n",
        "\n",
        "# Process all PDFs in the data directory\n",
        "all_pdf_documents = process_all_pdfs(\"/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96748c96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96748c96",
        "outputId": "5ac2d24b-c09c-4be6-ff63-b6a5d81d9e7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\\nYing Zhang1 Xiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented\\nMethod (AoM) to detect aspect-relevant seman-\\ntic and sentiment information. Specifically, an\\naspect-aware attention module is designed to\\nsimultaneously select textual tokens and im-\\nage blocks that are semantically related to the\\naspects. To accurately aggregate sentiment in-\\nformation, we explicitly introduce sentiment\\nembedding into AoM, and use a graph convo-\\nlutional network to model the vision-text and\\ntext-text interaction. Extensive experiments\\ndemonstrate the superiority of AoM to existing\\nmethods. The source code is publicly released\\nat https://github.com/SilyRab/AoM.\\n1 Introduction\\nAs an important and promising task in the field of\\nsentiment analysis, Multimodal Aspect-Based Sen-\\ntiment Analysis (MABSA) has attracted increasing\\nattention (Lv et al., 2021; Ju et al., 2021). Given an\\nimage and corresponding text, MABSA is defined\\nas jointly extracting all aspect terms from image-\\ntext pairs and predicting their sentiment polarities\\n(Ju et al., 2021).\\nIn this scenario of fine-grained sentiment recog-\\nnition for multimodal information, the input image-\\ntext pairs are always complex. (1) The semantics\\nof sentence is complex which adds sentiment con-\\nfusion among different aspects. Take Figure 1 as an\\n∗Corresponding author.\\nComplain about Kyoto a bit\\nand someone takes you to\\nsee the mayor. Interesting!\\nMayor Kadokawa, thanks\\nfor your time!\\nregion 1\\nregion 2\\nAspect Kyoto mayor Mayor Kadokawa\\nSentiment Negative Neutral Positive\\nFigure 1: An example of MABSA task, including the as-\\npects, their corresponding descriptions, and sentiments.\\nexample, there are 3 aspects in the sentence with 3\\ndifferent sentiments, The sentiment of “mayor\" can\\nbe easily confused by the keyword, “Interesting”,\\nwhich is of positive sentiment. (2) The images con-\\ntain a large amount of detailed information, and the\\nvisual contents are usually related to only one or\\nseveral of the aspects. For example, as shown in\\nFigure 1, the objects in red boxes are more helpful\\nin analyzing the sentiment of “Mayor Kadokawa”\\nthan the other aspects. The complex input greatly\\nchallenges the recognition of aspect-based senti-\\nment.\\nConsidering the multimodal input, existing meth-\\nods are typically dedicated to associated visual and\\ntextual contents (Ju et al., 2021; Ling et al., 2022;\\nYang et al., 2022). Ju et al. (2021) uses image-\\ntext relation to evaluate the contribution of visual\\ncontents to aspect sentiment, based on which to de-\\ntermine whether the image is involved in sentiment\\nanalysis. Ling et al. (2022) and Yang et al. (2022)\\nalign visual representations of objects and their at-\\ntributes with corresponding textual contents. To\\nsummarize, the whole image is directly associated\\nwith textual content in these methods. Intuitively,\\nwithout aligning image blocks to corresponding as-\\npects, the coarse whole-image-text association can\\nintroduce aspect-irrelevant visual noise, which fur-\\nther hinders aspect sentiment analysis. In addition,\\nthe performance can be further impacted by the\\narXiv:2306.01004v1  [cs.CL]  31 May 2023'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='textual noise from the confusion among different\\naspects.\\nIn this paper, we propose an Aspect-oriented\\nMethod (AoM) to mitigate aforementioned noises\\nfrom both image and text. AoM can detect aspect-\\nrelevant information from perspectives of both se-\\nmantics and sentiment. There are two key modules\\nin AoM: Aspect-Aware Attention Module (A3M)\\nfor semantically fine-grained image-text alignment\\nand Aspect-Guided Graph Convolutional Network\\n(AG-GCN) for sentiment information aggregation.\\nIn A3M, we first extract aspect features associated\\nwith each visual and textual token. And then aspect-\\nrelevant token representations are computed based\\non their relevance to the corresponding aspect fea-\\ntures. In AG-GCN, we first explicitly add sentiment\\nembeddings to the obtained representations of vi-\\nsual and textual tokens. A multimodal weighted-\\nassociation matrix is constructed containing aspect-\\nto-image-block similarity and word-to-word depen-\\ndency. Then we use a graph convolutional network\\nto aggregate sentiment information according to\\nthe constructed multimodal matrix.\\nThe contributions can be summarized as follows:\\n(1) We propose an aspect-oriented network to\\nmitigate the visual and textual noises from the com-\\nplex image-text interactions.\\n(2) We design an aspect-aware attention mod-\\nule and an aspect-guided graph convolutional net-\\nwork to effectively detect aspect-relevant multi-\\nmodal contents from the perspectives of semantic\\nand sentiment, respectively.\\n(3) Experiments on two benchmark datasets, in-\\ncluding Twitter2015 and Twitter2017, show that\\nour approach generally outperforms the state-of-\\nthe-art methods.\\n2 Related Work\\nIn this section, we review the existing methods for\\nboth ABSA and MABSA.\\n2.1 Aspect-based Sentiment Analysis\\nIn the past few years, Aspect-Based Sentiment\\nAnalysis (ABSA) in the textual fields has attracted\\nmuch attention and gained mature research (Chen\\nand Qian, 2020; Oh et al., 2021; Xu et al., 2020).\\nOn the one hand, most recent works are based on\\nthe pre-trained language model BERT because of\\nits remarkable performance in many NLP tasks\\n(Liang et al., 2022a). On the other hand, some\\nrecent efforts focus on modeling the dependency\\nrelationship between aspects and their correspond-\\ning descriptions, in which graph convolutional net-\\nworks (GCNs) (Chen et al., 2022; Liang et al.,\\n2022b, 2020; Li et al., 2021a; Pang et al., 2021)\\nor graph attention networks (GATs) (Yuan et al.,\\n2020) over dependency with the syntactic structure\\nof a sentence are fully exploited.\\n2.2 Multimodal Aspect-based Sentiment\\nAnalysis\\nWith the enrichment of multimodal users’ posts\\nin social media, researchers find that images of-\\nfer great supplementary information in aspect term\\nextraction (Wu et al., 2020a; Zhang et al., 2018;\\nAsgari-Chenaghlu et al., 2021) and sentiment anal-\\nysis (Wu et al., 2022; Zhang et al., 2022; Li\\net al., 2021b; Hazarika et al., 2020; Cai et al.,\\n2019). Thus, Multimodal Aspect-based Sentiment\\nAnalysis (MABSA) begins to be widely studied.\\nMABSA task can be divided into two independent\\nsub-tasks, i.e., Multimodal Aspect Term Extraction\\n(MATE) and Multimodal Aspect-oriented Senti-\\nment Classification (MASC). The former extracts\\nall aspect terms in the sentence at the prompt of the\\nimage, and the latter predicts the sentiment polari-\\nties for the aspects.\\nJu et al. (2021) first realizes MABSA in a unified\\nframework and designs an auxiliary cross-modal\\nrelation detection to control whether the visual in-\\nformation will be used in prediction. For captur-\\ning cross-modal alignment, Ling et al. (2022) con-\\nstructs a generative multimodal architecture based\\non BART for both vision-language pre-training and\\nthe downstream MABSA tasks. Yang et al. (2022)\\ndynamically controls the contributions of the vi-\\nsual information to different aspects via the trick\\nthat the lower confidence of the results predicted\\nby purely textual is, the more contributions from\\nimages will be considered.\\nOn the one hand, the above methods ignore the\\nalignment of fine-grained visual blocks and the\\ncorresponding aspects, which introduce irrelevant\\nvisual noise. On the other hand, modeling of syntax\\ndependency and sentiment information for aspect\\ndescriptions is absent in these methods, which is\\nproved important in sentiment analysis (Liang et al.,\\n2022a; Kalaivani et al., 2022; Xu et al., 2022).\\nTo tackle the aforementioned issues, we pro-\\npose an aspect-oriented model consisting of Aspect-\\nAware Attention Module and Aspect-Guided Graph\\nConvolutional Network which respectively work'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='A\\n3\\nM\\nComplain about Kyoto ... time ! \\nSenticNet\\nlinear layer (1 to 768)\\ncosine  similarity\\nSenticNet\\n AG\\n-\\nGCN\\nGCN Layers\\nGCN Layers\\natomic feature\\nsoftmax linear layer\\nsigmoid linear layer\\nℎ𝑡𝑡\\nℎ1\\n𝐶𝐶𝐶𝐶\\nℎ𝑡𝑡\\n...\\nℎ𝑡𝑡 ℎ𝑡𝑡\\nℎ2\\n𝐶𝐶𝐶𝐶 ℎ𝑙𝑙\\n𝐶𝐶𝐶𝐶\\n𝛼𝛼𝑡𝑡1 𝛼𝛼𝑡𝑡2\\n𝛼𝛼𝑡𝑡𝑙𝑙\\nℎ𝑡𝑡\\n𝐶𝐶 ℎ𝑡𝑡\\n𝛽𝛽𝑡𝑡 1 −𝛽𝛽𝑡𝑡\\n�ℎ𝑡𝑡\\n... �ℎ𝑉𝑉𝑚𝑚 ...�ℎ𝑇𝑇𝑛𝑛\\n3 3         NEG       13          13        NEU       17          18        POS      <eos>\\nBART decoder\\n<bos>  <AESC> Kyoto Kyoto NEG mayor mayor NEU Mayor Kadokawa POS\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\naspect-guided dependency, D\\nA\\n3\\nM\\nKyoto\\nMayor\\nKadokawa\\nKyoto\\nMayor\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑉𝑉1\\n�ℎ𝑉𝑉2\\n�ℎ𝑉𝑉3\\n�ℎ𝑉𝑉4\\n�ℎ𝑉𝑉5\\n�ℎ𝑉𝑉6\\n�ℎ𝑉𝑉𝑚𝑚\\nComplain Kyoto\\n�ℎ𝑇𝑇1\\nKyoto Kyoto\\n�ℎ𝑇𝑇3\\nmayor mayor\\n�ℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20\\n... ...... ... ... ......\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos><img> </img>\\nBART encoder\\nResNet                                        BART embedding\\nmask\\n𝒘𝒘𝒊𝒊\\n...𝑤𝑤𝑆𝑆1 𝑤𝑤𝑆𝑆2 𝑤𝑤𝑆𝑆3 𝑤𝑤𝑆𝑆𝑛𝑛\\nAG\\n-\\nGCN\\n GCN Layers\\nGCN Layers\\nweighted association matrix, AS from \\nSenticNet\\nH\\nweighted association matrix, A\\n𝑺𝑺\\n�𝑯𝑯S�𝑯𝑯\\n�𝑯𝑯S\\n�𝑯𝑯\\n�𝑯𝑯\\nHS\\nFigure 2: The overview of our proposed aspect-oriented model AoM.\\nto capture semantic information by fine-grained\\nimage-text alignment and effectively aggregate\\naspect-relevant sentiment information.\\n3 Methodology\\n3.1 Overview\\nTask Definition. Formally, given a tweet that\\ncontains an image V and a sentence with n\\nwords S = (w1, w2, ..., wn), our goal is to ac-\\nquire the sequence Y representing all aspects\\nand their associated sentiment polarities. We\\nformulate the output of MABSA as Y =\\n[as\\n1, ae\\n1, s1, ..., as\\ni , ae\\ni , si, ...as\\nk, ae\\nk, sk], where as\\ni , ae\\ni\\nand si depict the start index, end index of the i-th\\naspect and its sentiment polarity in the tweet, and\\nk is the number of aspects.\\nModel preview. Figure 2 shows the overview\\nof our model architecture, which builds on an\\nencoder-decoder architecture based on BART\\n(Lewis et al., 2019). Between the encoder and\\nthe decoder of BART, we creatively implement\\nthe Aspect-Aware Attention Module (A 3M) and\\nAspect-Guided Graph Convolutional Network (AG-\\nGCN) to align the textual aspect to its associated\\nvisual blocks and textual description, simultane-\\nously mitigate interference both from semantics\\nand sentiment among different aspects. In the fol-\\nlowing subsections, we will illustrate the details of\\nthe proposed model.\\nFeature Extractor. The initial word embeddings\\nare obtained from the pre-trained BART due to\\nits excellent ability of textual representation. The\\nembeddings of visual blocks are obtained by pre-\\nprocessing via ResNet (Chen et al., 2014) following\\n(Yu et al., 2019). We consider every feature of a\\nvisual block or word token as an atomic feature. We\\nadd <img> and </img> before and after the visual\\nfeatures, <bos> and <eos> for the textual features.\\nThen, we concatenate the multimodal features as\\nX which is the input of BART encoder.\\nWe can get the multimodal hidden state H =\\n{hV\\n0 , ...hV\\ni , ...hV\\nm, hT\\n0 , ..., hT\\nj , ...hT\\nn } with m visual\\nblocks and n words, where hV\\ni and hT\\nj refer to\\nfeatures of the i-th visual block and the j-th word\\nin the sentence.\\n3.2 Aspect-Aware Attention Module (A 3M)\\nSince aspects are not specially modeled by BART\\nencoder, we creatively design the Aspect-Aware\\nAttention Module (A3M) aiming to capture aspect-\\nrelevant semantic information. For this purpose, we\\nalign the multimodal information of target objects\\nand filter out the semantic noise from images.\\nFirst, as aspects are usually noun phrases from\\nthe sentences, we extract those phrases as the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='candidate aspects (CA) with the NLP tool Spacy1.\\nAnd from the hidden state H of the BART encoder,\\nwe obtain the features of all candidate aspects de-\\nnoted as HCA = {hCA\\n1 , ..., hCA\\ni , ..., hCA\\nl }, where\\nl is the number of noun phrases in the sentence. To\\nget the relationship between candidate aspects and\\natomic features, we implement an attention-based\\nmechanism guided by the candidate aspects. Given\\nthe t-th hidden feature ht, its attention distribution\\nαt over k candidate aspects is obtained by:\\nZt =tanh((WCAHCA+bCA)⊕(WHht +bH)), (1)\\nαt = softmax(WαZt + bα), (2)\\nwhere Zt ∈ R2d×k is the comprehensive feature\\nextracted from both the candidate aspects and the\\nhidden states. HCA ∈ Rd×k denotes the features\\nof candidate aspects. WCA ∈ Rd×d, WH ∈ Rd×d,\\nWα ∈ R1×2d, bCA, bH and bα are the learned\\nparameters.⊕ is an operator between a matrix and a\\nvector, where the vector is repeated into the appro-\\npriate size to concatenate with the matrix. We then\\nget the aspect-related hidden feature hA\\nt by calcu-\\nlating the weighted sum of all candidate aspects\\nfollowing the below equation:\\nhA\\nt =\\nkX\\ni\\nαt,ihCA\\ni . (3)\\nFor example, if a visual block is strongly associ-\\nated with the j-th aspect, the corresponding αt,j is\\napproximately 1. hA\\nt would be equal to the aspect\\nsemantically. And if the visual block is not related\\nto any specific candidate aspects, both αt and hA\\nt\\nwould be zero-like vectors of no information.\\nConsidering that not every visual block can be\\nused for prediction, βt is learned to add up the\\natomic feature ht and its aspect-related hidden fea-\\nture hA\\nt . Details are as follows:\\nβt = sigmoid(Wβ[W1ht; W2hA\\nt ] +bβ), (4)\\nˆht = βtht + (1− βt)hA\\nt , (5)\\nwhere Wβ, W1, W2, bβ are parameters, and [; ]\\ndenotes the concatenation operator for vectors.\\nˆht ∈ ˆH is the final output of A3M after the seman-\\ntic alignment and the noise reduction procedure.\\nThus we get the noiseless and aligned information\\nfor every atomic feature.\\nPre-training To align the two modalities and re-\\nduce noise, we conduct a pre-training task in A3M.\\n1Spacy: https://spacy.io/\\naverage\\nrelated or unrelated\\nH\\nimage-text relation classification layer\\nA\\n3\\nM\\nKyoto\\nMayor\\nKadokawa\\nKyoto\\nMayor\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑉𝑉1\\n�ℎ𝑉𝑉2\\n�ℎ𝑉𝑉3\\n�ℎ𝑉𝑉4\\n�ℎ𝑉𝑉5\\n�ℎ𝑉𝑉6\\n�ℎ𝑉𝑉𝑉𝑉\\nComplain Kyoto\\n�ℎ𝑇𝑇1\\nKyoto Kyoto\\n�ℎ𝑇𝑇3\\nmayor mayor\\n�ℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20\\n... ...... ... ... ......\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos>\\n<img> </img>\\nBART encoder\\nResNet                                        BART embedding\\nFigure 3: The framework of the pre-training task.\\nSpecifically, we detect the image-text relationship\\non the datasets TRC (Vempala and Preo¸ tiuc-Pietro,\\n2019) as illustrated by Figure 3. We first obtain the\\naverage feature of image blocks from the output of\\nA3M and then pass it to a fully connected softmax\\nlayer, which outputs a probability distribution over\\nwhether the image is related to the text. Finally, we\\nuse cross entropy loss to train our model.\\n3.3 Aspect-Guided Graph Convolutional\\nNetwork (AG-GCN)\\nThe aspect-focused interaction between visual\\nmodality and textual modality in A3M concentrates\\non the context semantics, and that is not adequate\\nfor MABSA. Sentiment interference among dif-\\nferent aspects still exists and influences sentiment\\nprediction. Thus, we design the Aspect-Guided\\nGraph Convolutional Network (AG-GCN) module\\nto introduce external sentiment information and\\nmitigate emotional confusion among different as-\\npects to a certain extent.\\nSpecifically, for wordwi in the sentence, we gain\\nits affective score wS\\ni from SenticNet (Ma et al.,\\n2018) and project it to the space with the same\\ndimension as hA\\nt , with si obtained. Then we add\\nthe sentiment feature si to the output of A3M:\\nwS\\ni = SenticNet (wi), (6)\\nsi = WSwS\\ni + bS, (7)\\nhS\\ni = ˆhi + si, (8)\\nwhere WS, bS are the learned parameters. hS\\ni is the\\nfeature with affective knowledge.\\nNext, we build a boolean dependency matrix\\nD among visual blocks and words. First, for the\\nword-to-word part, submatrix DT Trepresenting'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Complain\\nabout takes\\nKyoto\\nbit\\na someone you . to the\\nmayor\\nsee\\nand\\nInteresting\\n!\\nthanks\\n, !\\nKadokawa\\nMayor\\nfor\\ntime\\nyour\\ndependency tree\\nFigure 4: The dependency tree of the example men-\\ntioned in the introduction.\\nthe dependency tree2 of the input sentence like Fig-\\nure 4. If two words can be associated within two\\ngenerations, the element of DT Twould be set to\\n1, otherwise 0 instead. For example, “Kyoto” is as-\\nsociated with “bit” (child),“a” (grandchild),“about”\\n(father) and “Complain” (grandfather). Second, the\\nvisual dependency submatrix DV V is initialized\\nas a diagonal matrix. And as for the word-image-\\nblock dependency, denoted as DT V and equaled\\nto DT\\nV T, we set all the elements in the i-th line of\\nDT Vto 1 if the i-th word is an aspect, otherwise 0.\\nAnd the matrix D is defined as:\\nD =\\n\\x14DV V DV T\\nDT V DT T\\n\\x15\\n, (9)\\nConsidering the different importance of differ-\\nent dependencies, we attach weights onto D with\\ncosine similarity among ˆhi as follows:\\nAij = DijFcosine_similarity( ˆhi, ˆhj), (10)\\nwhere both D, A∈ R(m+n)×(m+n), and A is the\\nweighted association matrix.\\nAG-GCN takes HS from Eq.8 as initial node\\nrepresentations in the graph. For the i-th node at\\nthe l-th layer, the hidden state hS\\ni,l is updated by the\\nfollowing equation:\\nhS\\ni,l = ReLU(\\nnX\\nj=1\\nAijWlhS\\ni,l−1 + bl), (11)\\nwhere Wl,bl are learned parameters and we use\\nReLU as activation function. Significantly, hS\\ni,0\\nis equal to hS\\ni . Accordingly, we get the final out-\\nput ˆHS from the last GCN layer which is rich in\\nsentiment information. Every underlying aspect\\naggregates its relevant information from both the\\nimage-text pair. Moreover, sentiment confusion\\nof different aspects is weakened because the as-\\nsociation matrix makes little interference among\\ndifferent aspects.\\n2We use spaCy toolkit to construct the dependency tree\\nreferring from https://spacy.io\\nTwitter2015 Twitter2017\\n#sentence 3,502 2,910\\n#with one aspect 2,159 (61.65%) 976 (33.54%)\\n#with multiple aspects 1,343 (38.35%) 1,934 (66.46%)\\n#with multiple sentiments 1,257 1,690\\nTable 1: Statistics of the two benchmark datasets. Line 1\\nis the number of sentences. #X in the last 3 lines denotes\\nthe number of sentences with such characteristics X.\\n3.4 Prediction and Loss Function\\nThe BART decoder takes the combination of ˆH,\\nˆHS, and the previous decoder output Y<t as inputs,\\nand predicts the token probability distribution as\\nfollows:\\n˜H = λ1 ˆH + λ2 ˆHS, (12)\\nhd\\nt = Decoder( ˜H; Y<t) (13)\\nHT = (W + ˜HT )/2 (14)\\nP(yt) =softmax([HT ; Cd]hd\\nt ) (15)\\nwhere λ1, λ2 are the hyper-parameters to control\\nthe contribution from the two modules. ˜HT is the\\ntextual part of ˜H. W denotes the embeddings of\\ninput tokens. Cd means the embeddings of the [\\npositive, neutral, negative, <eos>]. The loss func-\\ntion is as follows:\\nL = −EX∼D\\nOX\\nt=1\\nlogP (yt|Y<t, X), (16)\\nwhere O = 2M + 2N + 2is the length of Y , and\\nX denotes the multimodal input.\\n4 Experiment\\n4.1 Experimental settings\\nDatasets. Our two benchmark datasets are Twit-\\nter2015 and Twitter2017 (Yu and Jiang (2019)). As\\nshown in the statistics of Table 1, sentences with\\nmultiple aspects take up a considerable part of the\\ntwo datasets.\\nImplementation Details. Our model is based on\\nBART (Lewis et al., 2019), and the pre-training\\ntask is trained for 40 epochs with batch size 64,\\nand for 35 epochs with batch size 16 on MABSA.\\nThe learning rates are both 7e-5 and hidden sizes\\nare 768. Hyper-parameters λ1 and λ2 are 1 and 0.5\\nrespectively. Besides, we pre-train A3M on TRC\\ndataset (Vempala and Preo¸ tiuc-Pietro, 2019), which\\nis divided into two groups according to whether the\\ntext is represented.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Twitter2015 Twitter2017\\nMethods P R F1 P R F1\\nText-based\\nSPAN* (Hu et al., 2019) 53.7 53.9 53.8 59.6 61.7 60.6\\nD-GCN* (Chen et al., 2020) 58.3 58.8 59.4 64.2 64.1 64.1\\nBART* (Yan et al., 2021) 62.9 65.0 63.9 65.2 65.6 65.4\\nMultimodal\\nUMT+TomBERT* (Yu et al., 2020; Yu and Jiang, 2019) 58.4 61.3 59.8 62.3 62.4 62.4\\nOSCGA+TomBERT* (Wu et al., 2020c; Yu and Jiang, 2019) 61.7 63.4 62.5 63.4 64.0 63.7\\nOSCGA-collapse* (Wu et al., 2020c) 63.1 63.7 63.2 63.5 63.5 63.5\\nRpBERT-collapse* (Sun et al., 2021) 49.3 46.9 48.0 57.0 55.4 56.2\\nUMT-collapse (Yu et al., 2020) 61.0 60.4 61.6 60.8 60.0 61.7\\nJML (Ju et al., 2021) 65.0 63.2 64.1 66.5 65.5 66.0\\nVLP-MABSA* (Ling et al., 2022) 65.1 68.3 66.6 66.9 69.2 68.0\\nCMMT (Yang et al., 2022) 64.6 68.7 66.5 67.6 69.4 68.5\\nAoM (ours) 67.9 69.3 68.6 68.4 71.0 69.7\\nTable 2: Results of different methods for MABSA on the two Twitter datasets. * denotes the results from Ling et al.\\n(2022). The best results are bold-typed and the second best ones are underlined.\\nEvaluation Metrics. We evaluate the performance\\nof our model on MABSA task and MATE task by\\nMicro-F1 score (F1), Precision (P) and Recall (R),\\nwhile on MASC task we use Accuracy (Acc) and\\nF1 following previous studies.\\n4.2 Baselines\\nWe compare our proposed model with four types\\nof methods listed below.\\nApproaches for textual ABSA. 1) SPAN (Hu\\net al., 2019) detects opinion targets with their senti-\\nments. 2) D-GCN (Chen et al., 2020) models de-\\npendency relations among words via dependency\\ntree. 3) BART (Yan et al., 2021) solves seven\\nABSA subtasks in an end-to-end framework.\\nApproaches for MATE. 1) RAN (Wu et al.,\\n2020b) focus on alignment of text and object re-\\ngions. 2) UMT (Yu et al., 2020) takes text-based\\nentity span detection as an auxiliary task. 3) OS-\\nCGA (Wu et al., 2020c) foucus on alignments of\\nvisual objects and entities.\\nApproaches for MASC. 1) ESAFN (Yu et al.,\\n2019) is an entity-level sentiment analysis method\\nbased on LSTM. 2) TomBERT (Yu and Jiang,\\n2019) applies BERT to obtain aspect-sensitive tex-\\ntual representations. 3) CapTrBERT (Khan and\\nFu, 2021) translates images into text and construct\\nan auxiliary sentence for fusion.\\nApproaches for MABSA. 1) UMT-collapse\\n(Yu et al., 2020), OSCGA-collapse (Wu et al.,\\n2020c) and RpBERT-collapse (Sun et al., 2021)\\nare adapted from models for MATE by using col-\\nlapsed labels to represent aspect and sentiment\\npairs. 2) UMT+TomBERT, OSCGA+TomBERT\\nare two pipeline approaches by combining UMT\\n(Yu et al., 2020) or OSCGA (Wu et al., 2020c) with\\nTomBERT (Yu and Jiang, 2019). 3)JML (Ju et al.,\\n2021) is the first joint model for MABSA with aux-\\niliary cross-modal relation detection module. 4)\\nCMMT (Yang et al., 2022) implements a gate to\\ncontrol the multimodal information contributions\\nduring inter-modal interactions. 5) VLP-MABSA\\n(Ling et al., 2022) performs five task-specific pre-\\ntraining tasks to model aspects, opinions and align-\\nments.\\n4.3 Main Results\\nIn this section, we show the excellent performance\\nof AoM on the two datasets for the three tasks\\ncompared with SOTAs.\\nPerformance on MABSA: The results for\\nMABSA are shown in Table 2. First, our AoM\\nfar exceeds all text-based models, which means\\ndetection of richer visual information and textual\\ninformation in our model is helpful. Second, multi-\\nmodal pipeline methods and adaptive methods are\\ngenerally unsatisfactory, because they ignore the\\ninteraction between the semantic information and\\nsentiment for the two sub-tasks. Last, AoM out-\\nperforms all multimodal methods in every metric.\\nEspecially, AoM achieves the improvement of 2%\\nand 1.2% with respect to F1 in contrast with the sec-\\nond best models on two datasets (VLP-MABSA for\\nTwitter2015 and CMMT for Twitter2017), which\\ndemonstrates the effectiveness of learning aspect-\\nrelevant visual blocks and textual words compared\\nto focusing on all visual and textual inputs.\\nPerformance on MATE:As shown in Table 3,\\nAoM is ahead of most of the current models and\\nperforms the best in Twitter 2015 by 0.3% higher\\nthan the second best CMMT on F1. The perfor-\\nmance of CMMT in Twitter2017 is 0.8% higher'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Twitter2015 Twitter2017\\nMethods P R F1 P R F1\\nRAN* 80.5 81.5 81.0 90.7 90.7 90.0\\nUMT* 77.8 81.7 79.7 86.7 86.8 86.7\\nOSCGA* 81.7 82.1 81.9 90.2 90.7 90.4\\nJML* 83.6 81.2 82.4 92.0 90.7 91.4\\nVLP-MABSA* 83.6 87.985.7 90.8 92.6 91.7\\nCMMT 83.9 88.1 85.9 92.2 93.9 93.1\\nAoM (ours) 84.6 87.9 86.2 91.8 92.8 92.3\\nTable 3: Results of different methods for MATE. * de-\\nnotes the results from Ling et al. (2022).\\nTwitter2015 Twitter2017\\nMethods ACC F1 ACC F1\\nESAFN 73.4 67.4 67.8 64.2\\nTomBERT 77.2 71.8 70.5 68.0\\nCapTrBERT 78.0 73.2 72.3 70.2\\nJML 78.7 - 72.7 -\\nVLP-MABSA 78.6 73.873.8 71.8\\nCMMT 77.9 - 73.8 -\\nAoM (ours)80.2 75.9 76.4 75.0\\nTable 4: Results of different methods for MASC.\\nthan ours, probably due to our model wrongly pre-\\ndicting some noun phrases as aspects. But consid-\\nering the improvement in MASC and MABSA, it\\nis still worthy treating all noun phrases in the sen-\\ntence as candidate aspects when acquiring aspect-\\nrelevant visual information.\\nPerformance on MASC: Table 4 shows the per-\\nformance of MASC. It is exciting that our model\\noutperforms the second-best results by 1.5% and\\n2.6% in accuracy, 2.1% and 3.2% points in F1 score\\non Twitter2015 and Twitter2017. It demonstrates\\nthat AoM has the ability to detect aspect-related\\nsentiment information from both images and text,\\neven disturbed by other noisy aspects.\\n4.4 Ablation Study\\nIn this section, we research the effectiveness of\\neach component in AoM, the results are shown in\\nTable 5.\\nW/o A3M&AG-GCN shows that after remov-\\ning the two specially designed modules, the per-\\nTwitter2015 Twitter2017\\nMethods P R F1 P R F1\\nFull 67.9 69.3 68.6 68.4 71.0 69.7\\nw/o A3M&AG-GCN 65.7 67.3 66.5 66.5 69.0 67.8\\nw/o A3M&TRC 62.1 61.0 61.6 63.7 64.1 63.9\\nw/o TRC 66.8 68.4 67.6 67.8 69.8 68.8\\nw/o AG-GCN 67.0 69.4 68.2 67.8 69.7 68.8\\nw/o SenticNet 65.7 70.5 68.0 68.1 69.4 68.7\\nw/o TRC&AG-GCN 66.7 69.2 68.0 67.8 69.5 68.6\\nTable 5: The performance comparison of our full model\\nand its ablated approaches.\\nImage\\nText\\nVLP-MABSA\\nBART+A3M\\nAoM\\n(a) NBA Western Conference Finals: \\nGolden State Warriors shock \\nOklahoma City Thunder,…\\n( NBA, NEU ) (√, √)\\n---\\n( Oklahoma, NEU ) (×, ×)\\n( NBA, NEU ) (√, √)\\n( Golden State Warriors , POS ) (√, √)\\n( Oklahoma City Thunder, NEG ) (√, √)\\n( NBA, NEU ) (√, √)\\n( Golden State Warriors , POS ) (√, √)\\n( Oklahoma City Thunder, NEG ) (√, √)\\n(b) This subtle difference between \\nDaniel Radcliffe and Elijah Wood\\nis pretty unsettling.\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah, NEU ) (×, ×)\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah Wood, NEG ) (√, √)\\n( Daniel Radcliffe, NEG ) (√, √)\\n( Elijah Wood, NEG ) (√, √)\\nFigure 5: Two cases with predictions by VLP-MABSA\\n(Ling et al., 2022), BART+A3M, and our model.\\nformance declines by 2.1% on Twitter2015 and\\n1.9% on Twitter2017. It fully demonstrates their\\ncontributions to learning effective information.\\nW/o A3M&TRC performs worse after remov-\\ning A 3M including the pre-training on TRC. It\\nproves the necessity of modeling semantic align-\\nment between visual blocks and aspects in A 3M.\\nWith the alignment, AG-GCN can obtain appropri-\\nate aspect-image-block and text-text association.\\nW/o TRC pre-training shows a slight drop after\\nwe remove the TRC pre-training on A3M, which\\nimplies relevant pre-training task is useful for the\\nmodel to learn better parameters.\\nW/o AG-GCN displays the performance with-\\nout AG-GCN, declining by 0.42% on Twitter2015\\nand 0.9% on Twitter2017. It means that AG-GCN\\ndoes make the prediction focus on specific aspects\\nrelated to blocks and words with syntax dependen-\\ncies. In other words, the multimodal interference\\nfrom other aspects can be mitigated.\\nW/o SenticNet is the model without sentiment\\ninformation in AG-GCN. Its performance shows\\nadding external affective knowledge can enhance\\nthe sentiment comprehension of the model.\\nW/o TRC&AG-GCN is the BART model only\\nwith our A3M module. We can see from Table 5\\nthat w/o TRC&AG-GCNimproves w/o A3M&AG-\\nGCN by 1.5% and 0.8%. So it is effective to align\\nthe fine-grained visual block to related aspect and\\nreduce irrelevant information.\\n4.5 Case Study\\nTo better analyze how the Aspect-Aware Attention\\nModule and Aspect-Guided Graph Convolutional\\nNetwork work, we present the case study as fol-\\nlows.\\nFigure 5 displays two examples with predic-\\ntions from VLP-MABSA (Ling et al., 2022),'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='A3M\\nKyoto\\nbit\\nsomeone\\nmayor\\nMayor Kadokawa\\nthanks\\ntime\\n(I.a) attention of CAs\\n(I.b) learned visual attention\\nComplain about\\nKyoto a bit and\\nsomeone takes you\\nto see the mayor.\\nInteresting! Mayor\\nKadokawa, thanks\\nfor your time!\\nInputs\\nAspect-Sentiment Pairs\\n<Kyoto, NEG>\\n<mayor, NEU>\\n<Mayor Kadokawa, POS>\\nThe image-related probability \\ndistribution of candidate aspects.\\nAG-GCN\\n(II.a) word-to-word association matrix (II.b) word-to-visual-block association matrix\\n(II.c) aspect-relevant visual attention map\\nMayor Kadokawa\\nMayor Kadokawa thanks\\nKyoto Mayor Kadokawa\\n(II.d) information relevant to “Mayor Kadokawa”\\n0.8 0.7 0.6 0.1 0.00.20.30.40.5\\nFigure 6: Visualization of the attention maps in A3M and the sub-parts of the weighted-association matrix AG-GCN.\\nBART+A3M and our AoM. In example (a), VLP-\\nMABSA misses the aspect “Golden State War-\\nriors”, gets an incomplete aspect “Oklahoma City\\nThunder” and wrongly predicts the sentiment. It\\nmay be caused by the interference from the vi-\\nsual region which represents pride expression of a\\nperson. However, BART+A3M gets all right pre-\\ndictions due to the ability of aspect-oriented atten-\\ntion. In example (b), compared with our whole\\nmodel, BART+A3M wrongly predicts the senti-\\nment of “Daniel Radcliffe” which should be nega-\\ntive. We attribute the wrong prediction to lacking\\nsyntax association which benefits sentiment trans-\\nmission. In other words, AG-GCN contributes to\\nthe correctness.\\n4.6 Attention Visualization\\nTo investigate the effectiveness of detecting aspect-\\nrelevant information, we visualize the attention pro-\\ncess as shown in Figure 6.\\nFor A3M: (i) Figure 6-(I.a) shows the attention\\nweights of candidate aspects computed according\\nto the images. We can see that “Mayor Kadokawa”\\nis the most relevant aspect. (ii) Figure 6-(I.b)\\nshows the proportions of the visual information re-\\nserved at the last step in A3M, where we weighted\\nadd up the representations of visual blocks and\\nthe corresponding aspects. The heat map shows\\nthat the visual information associated with “Mayor\\nKadokawa” is reserved to a great extent, while the\\nhelpless information from other blocks is disre-\\ngarded as noise. It demonstrates that attention in\\nA3M is able to detect aspect-relevant information.\\nFor AG-GCN: (i) Figure 6-(II.a) shows the\\nword-to-word part of the weighted association ma-\\ntrix. The matrix effectively excludes sentiment\\ninterference from other aspects by adding syntax\\ndependency information. For example, the senti-\\nment of “mayor” cannot be influenced by irrelevant\\nkeywords, such as “Complain” and “thanks”. (ii)\\nFigure 6-(II.b) shows the dependencies between\\nvisual blocks and words. (iii) Specifically, we vi-\\nsualize the visual attention of aspects “Kyoto” (see\\nFigure 6-(II.c) left) and “Mayor Kadokawa” (see\\nFigure 6-(II.c) right). We can see that “Kyoto” pays\\nmore attention to the pictures hanging on the wall\\nwhich are full of Japanese elements related to the\\nplace, while “Mayor Kadokawa” focus more on the\\njoyful expressions of the two people. (iv) Figure\\n6-(II.d) shows the words and image blocks “Mayor\\nKadokawa” focused on in sentiment transmission.\\nIt’s obvious that these attentions are helpful for the\\nprediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='we explicitly add sentiment embedding into AoM.\\nThen, a graph convolutional network is used to ag-\\ngregate the semantic and sentiment embedding un-\\nder the guidance of both image-text similarity and\\nsyntax dependency in sentences. The experimental\\nresults on two widely used datasets demonstrate\\nthe effectiveness of our method.\\nLimitations\\nThough our proposed method outperforms cur-\\nrent state-of-the-art methods, there are still many\\nchallenges we should overcome in future research.\\nFirst, for colloquial expression which confuses cur-\\nrent dependency tree parser, we should come up\\nwith new solutions. Second, emotional prediction\\nof tweet posts describing current issues needs exter-\\nnal knowledge, which is absent in existing research.\\nAcknowledgments\\nWe thank anonymous reviewers for their valu-\\nable comments. This work was supported by\\nthe Natural Science Foundation of Tianjin,\\nChina (No.22JCJQJC00150, 22JCQNJC01580),\\nthe National Natural Science Foundation of\\nChina (No.62272250), Tianjin Research In-\\nnovation Project for Postgraduate Students\\n(No.2022SKYZ232), and the Fundamental\\nResearch Funds for the Central Universities (No.\\n63231149).\\nReferences\\nMeysam Asgari-Chenaghlu, M. Reza Feizi-Derakhshi,\\nLeili Farzinvash, M. A. Balafar, and Cina Motamed.\\n2021. CWI: A multimodal deep learning approach\\nfor named entity recognition from social media us-\\ning character, word and image features. Neural\\nComputing and Applications, 34(3):1905–1922.\\nYitao Cai, Huiyu Cai, and Xiaojun Wan. 2019.\\nMulti-Modal Sarcasm Detection in Twitter with\\nHierarchical Fusion Model. In Proceedings of\\nthe 57th Annual Meeting of the Association for\\nComputational Linguistics, pages 2506–2515, Flo-\\nrence, Italy. Association for Computational Linguis-\\ntics.\\nGuimin Chen, Yuanhe Tian, and Yan Song. 2020.\\nJoint aspect extraction and sentiment analysis with\\ndirectional graph convolutional networks. In\\nProceedings of the 28th international conference on\\ncomputational linguistics, pages 272–279.\\nHao Chen, Zepeng Zhai, Fangxiang Feng, Ruifan\\nLi, and Xiaojie Wang. 2022. Enhanced Multi-\\nChannel Graph Convolutional Network for\\nAspect Sentiment Triplet Extraction. In\\nProceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics (V olume\\n1: Long Papers), pages 2974–2985, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nTao Chen, Damian Borth, Trevor Darrell, and Shih-\\nFu Chang. 2014. Deepsentibank: Visual sentiment\\nconcept classification with deep convolutional neural\\nnetworks.\\nZhuang Chen and Tieyun Qian. 2020. Relation-\\nAware Collaborative Learning for Unified Aspect-\\nBased Sentiment Analysis. In Proceedings of\\nthe 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 3685–3694, On-\\nline. Association for Computational Linguistics.\\nDevamanyu Hazarika, Roger Zimmermann, and Sou-\\njanya Poria. 2020. Misa: Modality-invariant\\nand -specific representations for multimodal senti-\\nment analysis. In Proceedings of the 28th ACM\\nInternational Conference on Multimedia, MM ’20,\\npage 1122–1131, New York, NY , USA. Association\\nfor Computing Machinery.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Dongsheng\\nLi, and Yiwei Lv. 2019. Open-domain targeted senti-\\nment analysis via span-based extraction and classifi-\\ncation. arXiv preprint arXiv:1906.03820.\\nXincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\\nShoushan Li, Min Zhang, and Guodong Zhou.\\n2021. Joint Multi-modal Aspect-Sentiment Anal-\\nysis with Auxiliary Cross-modal Relation Detec-\\ntion. In Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 4395–4405, Online and Punta Cana, Domini-\\ncan Republic. Association for Computational Lin-\\nguistics.\\nKS Kalaivani, M Rakshana, K Mounika, and D Sindhu.\\n2022. Senticnet-based feature weighting scheme for\\nsentiment classification. In Mobile Computing and\\nSustainable Informatics, pages 839–848. Springer.\\nZaid Khan and Yun Fu. 2021. Exploiting bert for\\nmultimodal target sentiment classification through\\ninput space translation. In Proceedings of the\\n29th ACM International Conference on Multimedia,\\npages 3034–3042.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\\nnoising sequence-to-sequence pre-training for natural\\nlanguage generation, translation, and comprehension.\\narXiv preprint arXiv:1910.13461.\\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu\\nMa, Xiaojie Wang, and Eduard Hovy.\\n2021a. Dual Graph Convolutional Networks\\nfor Aspect-based Sentiment Analysis. In\\nProceedings of the 59th Annual Meeting of\\nthe Association for Computational Linguistics'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='and the 11th International Joint Conference\\non Natural Language Processing (V olume 1:\\nLong Papers), pages 6319–6329, Online. Associa-\\ntion for Computational Linguistics.\\nYuanqing Li, Ke Zhang, Jingyu Wang, and Xinbo Gao.\\n2021b. A cognitive brain model for multimodal sen-\\ntiment analysis based on attention neural networks.\\nNeurocomputing, 430:159–173.\\nBin Liang, Hang Su, Lin Gui, Erik Cambria, and\\nRuifeng Xu. 2022a. Aspect-based sentiment anal-\\nysis via affective knowledge enhanced graph con-\\nvolutional networks. Knowledge-Based Systems,\\n235:107643.\\nBin Liang, Rongdi Yin, Lin Gui, Jiachen Du, and\\nRuifeng Xu. 2020. Jointly Learning Aspect-Focused\\nand Inter-Aspect Relations with Graph Convolu-\\ntional Networks for Aspect Sentiment Analysis. In\\nProceedings of the 28th International Conference\\non Computational Linguistics, pages 150–161,\\nBarcelona, Spain (Online). International Committee\\non Computational Linguistics.\\nShuo Liang, Wei Wei, Xian-Ling Mao, Fei Wang, and\\nZhiyong He. 2022b. BiSyn-GAT+: Bi-Syntax Aware\\nGraph Attention Network for Aspect-based Senti-\\nment Analysis. In Findings of the Association for\\nComputational Linguistics: ACL 2022, pages 1835–\\n1848, Dublin, Ireland. Association for Computational\\nLinguistics.\\nYan Ling, Jianfei Yu, and Rui Xia. 2022. Vision-\\nLanguage Pre-Training for Multimodal Aspect-\\nBased Sentiment Analysis. In Proceedings of\\nthe 60th Annual Meeting of the Association\\nfor Computational Linguistics (V olume 1:\\nLong Papers), pages 2149–2159, Dublin, Ire-\\nland. Association for Computational Linguistics.\\nYanxia Lv, Fangna Wei, Lihong Cao, Sancheng Peng,\\nJianwei Niu, Shui Yu, and Cuirong Wang. 2021.\\nAspect-level sentiment analysis using context and\\naspect memory network. Neurocomputing, 428:195–\\n205.\\nYukun Ma, Haiyun Peng, and Erik Cambria. 2018. Tar-\\ngeted aspect-based sentiment analysis via embed-\\nding commonsense knowledge into an attentive lstm.\\nProceedings of the AAAI Conference on Artificial\\nIntelligence, 32(1).\\nShinhyeok Oh, Dongyub Lee, Taesun Whang, IlNam\\nPark, Seo Gaeun, EungGyun Kim, and Harksoo\\nKim. 2021. Deep Context- and Relation-Aware\\nLearning for Aspect-based Sentiment Analysis.\\nIn Proceedings of the 59th Annual Meeting of\\nthe Association for Computational Linguistics\\nand the 11th International Joint Conference\\non Natural Language Processing (V olume 2:\\nShort Papers), pages 495–503, Online. Association\\nfor Computational Linguistics.\\nShiguan Pang, Yun Xue, Zehao Yan, Weihao Huang,\\nand Jinhui Feng. 2021. Dynamic and Multi-Channel\\nGraph Convolutional Networks for Aspect-Based\\nSentiment Analysis. In Findings of the Association\\nfor Computational Linguistics: ACL-IJCNLP 2021,\\npages 2627–2636, Online. Association for Computa-\\ntional Linguistics.\\nLin Sun, Jiquan Wang, Kai Zhang, Yindu Su, and Fang-\\nsheng Weng. 2021. Rpbert: A text-image relation\\npropagation-based bert model for multimodal ner.\\nProceedings of the AAAI Conference on Artificial\\nIntelligence, 35(15):13860–13868.\\nAlakananda Vempala and Daniel Preo¸ tiuc-Pietro. 2019.\\nCategorizing and inferring the relationship between\\nthe text and image of twitter posts. In Proceedings\\nof the 57th annual meeting of the Association for\\nComputational Linguistics, pages 2830–2840.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi, and Lian Chi. 2020a. Multimodal as-\\npect extraction with region-aware alignment net-\\nwork. In Natural Language Processing and Chinese\\nComputing, pages 145–156, Cham. Springer Interna-\\ntional Publishing.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi, and Lian Chi. 2020b. Multimodal aspect ex-\\ntraction with region-aware alignment network. In\\nCCF International Conference on Natural Language\\nProcessing and Chinese Computing, pages 145–156.\\nSpringer.\\nYang Wu, Yanyan Zhao, Hao Yang, Song Chen,\\nBing Qin, Xiaohuan Cao, and Wenting Zhao.\\n2022. Sentiment Word Aware Multimodal Re-\\nfinement for Multimodal Sentiment Analysis with\\nASR Errors. In Findings of the Association for\\nComputational Linguistics: ACL 2022, pages 1397–\\n1406, Dublin, Ireland. Association for Computational\\nLinguistics.\\nZhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen,\\nHo-fung Leung, and Qing Li. 2020c. Multimodal\\nrepresentation with embedded visual guiding ob-\\njects for named entity recognition in social media\\nposts. In Proceedings of the 28th ACM International\\nConference on Multimedia, pages 1038–1046.\\nJunjie Xu, Shuwen Yang, Luwei Xiao, Zhichao Fu,\\nXingjiao Wu, Tianlong Ma, and Liang He. 2022.\\nGraph convolution over the semantic-syntactic hybrid\\ngraph enhanced by affective knowledge for aspect-\\nlevel sentiment classification. In 2022 International\\nJoint Conference on Neural Networks (IJCNN),\\npages 1–8. IEEE.\\nLu Xu, Hao Li, Wei Lu, and Lidong Bing.\\n2020. Position-Aware Tagging for Aspect\\nSentiment Triplet Extraction. In Proceedings\\nof the 2020 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP), pages\\n2339–2349, Online. Association for Computational\\nLinguistics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Hang Yan, Junqi Dai, Xipeng Qiu, Zheng Zhang,\\net al. 2021. A unified generative framework for\\naspect-based sentiment analysis. arXiv preprint\\narXiv:2106.04300.\\nLi Yang, Jin-Cheon Na, and Jianfei Yu. 2022.\\nCross-Modal Multitask Transformer for End-to-\\nEnd Multimodal Aspect-Based Sentiment Anal-\\nysis. Information Processing & Management,\\n59(5):103038.\\nJianfei Yu and Jing Jiang. 2019. Adapting bert\\nfor target-oriented multimodal sentiment classi-\\nfication. In Proceedings of the Twenty-Eighth\\nInternational Joint Conference on Artificial\\nIntelligence, IJCAI-19, pages 5408–5414. Interna-\\ntional Joint Conferences on Artificial Intelligence\\nOrganization.\\nJianfei Yu, Jing Jiang, and Rui Xia. 2019. Entity-\\nsensitive attention and fusion network for entity-level\\nmultimodal sentiment classification. IEEE/ACM\\nTransactions on Audio, Speech, and Language\\nProcessing, 28:429–439.\\nJianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020.\\nImproving multimodal named entity recognition via\\nentity span detection with unified multimodal trans-\\nformer. In Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics,\\npages 3342–3352, Online. Association for Computa-\\ntional Linguistics.\\nLi Yuan, Jin Wang, Liang-Chih Yu, and Xuejie Zhang.\\n2020. Graph attention network with memory fusion\\nfor aspect-level sentiment analysis. In Proceedings\\nof the 1st Conference of the Asia-Pacific Chapter of\\nthe Association for Computational Linguistics and\\nthe 10th International Joint Conference on Natural\\nLanguage Processing, pages 27–36, Suzhou, China.\\nAssociation for Computational Linguistics.\\nQi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang.\\n2018. Adaptive co-attention network for named en-\\ntity recognition in tweets. In Thirty-Second AAAI\\nConference on Artificial Intelligence.\\nYuhao Zhang, Ying Zhang, Wenya Guo, Xiangrui Cai,\\nand Xiaojie Yuan. 2022. Learning disentangled rep-\\nresentation for multimodal cross-domain sentiment\\nanalysis. IEEE Transactions on Neural Networks\\nand Learning Systems.')]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "all_pdf_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab6b6cc",
      "metadata": {
        "id": "fab6b6cc"
      },
      "outputs": [],
      "source": [
        "### Text splitting get into chunks\n",
        "\n",
        "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
        "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
        "\n",
        "    # Show example of a chunk\n",
        "    if split_docs:\n",
        "        print(f\"\\nExample chunk:\")\n",
        "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
        "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
        "\n",
        "    return split_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b75a72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7b75a72",
        "outputId": "9765ce43-943b-4b72-85d0-f371c00261d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 11 documents into 57 chunks\n",
            "\n",
            "Example chunk:\n",
            "Content: AoM: Detecting Aspect-oriented Information for Multimodal\n",
            "Aspect-Based Sentiment Analysis\n",
            "Ru Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\n",
            "Ying Zhang1 Xiaojie Yuan1\n",
            "1 College of Computer Science, TKLNDS...\n",
            "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\\nYing Zhang1 Xiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='timent of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented\\nMethod (AoM) to detect aspect-relevant seman-\\ntic and sentiment information. Specifically, an\\naspect-aware attention module is designed to\\nsimultaneously select textual tokens and im-\\nage blocks that are semantically related to the\\naspects. To accurately aggregate sentiment in-\\nformation, we explicitly introduce sentiment\\nembedding into AoM, and use a graph convo-\\nlutional network to model the vision-text and\\ntext-text interaction. Extensive experiments\\ndemonstrate the superiority of AoM to existing\\nmethods. The source code is publicly released\\nat https://github.com/SilyRab/AoM.\\n1 Introduction\\nAs an important and promising task in the field of\\nsentiment analysis, Multimodal Aspect-Based Sen-\\ntiment Analysis (MABSA) has attracted increasing\\nattention (Lv et al., 2021; Ju et al., 2021). Given an'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='sentiment analysis, Multimodal Aspect-Based Sen-\\ntiment Analysis (MABSA) has attracted increasing\\nattention (Lv et al., 2021; Ju et al., 2021). Given an\\nimage and corresponding text, MABSA is defined\\nas jointly extracting all aspect terms from image-\\ntext pairs and predicting their sentiment polarities\\n(Ju et al., 2021).\\nIn this scenario of fine-grained sentiment recog-\\nnition for multimodal information, the input image-\\ntext pairs are always complex. (1) The semantics\\nof sentence is complex which adds sentiment con-\\nfusion among different aspects. Take Figure 1 as an\\n∗Corresponding author.\\nComplain about Kyoto a bit\\nand someone takes you to\\nsee the mayor. Interesting!\\nMayor Kadokawa, thanks\\nfor your time!\\nregion 1\\nregion 2\\nAspect Kyoto mayor Mayor Kadokawa\\nSentiment Negative Neutral Positive\\nFigure 1: An example of MABSA task, including the as-\\npects, their corresponding descriptions, and sentiments.\\nexample, there are 3 aspects in the sentence with 3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Sentiment Negative Neutral Positive\\nFigure 1: An example of MABSA task, including the as-\\npects, their corresponding descriptions, and sentiments.\\nexample, there are 3 aspects in the sentence with 3\\ndifferent sentiments, The sentiment of “mayor\" can\\nbe easily confused by the keyword, “Interesting”,\\nwhich is of positive sentiment. (2) The images con-\\ntain a large amount of detailed information, and the\\nvisual contents are usually related to only one or\\nseveral of the aspects. For example, as shown in\\nFigure 1, the objects in red boxes are more helpful\\nin analyzing the sentiment of “Mayor Kadokawa”\\nthan the other aspects. The complex input greatly\\nchallenges the recognition of aspect-based senti-\\nment.\\nConsidering the multimodal input, existing meth-\\nods are typically dedicated to associated visual and\\ntextual contents (Ju et al., 2021; Ling et al., 2022;\\nYang et al., 2022). Ju et al. (2021) uses image-\\ntext relation to evaluate the contribution of visual'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='textual contents (Ju et al., 2021; Ling et al., 2022;\\nYang et al., 2022). Ju et al. (2021) uses image-\\ntext relation to evaluate the contribution of visual\\ncontents to aspect sentiment, based on which to de-\\ntermine whether the image is involved in sentiment\\nanalysis. Ling et al. (2022) and Yang et al. (2022)\\nalign visual representations of objects and their at-\\ntributes with corresponding textual contents. To\\nsummarize, the whole image is directly associated\\nwith textual content in these methods. Intuitively,\\nwithout aligning image blocks to corresponding as-\\npects, the coarse whole-image-text association can\\nintroduce aspect-irrelevant visual noise, which fur-\\nther hinders aspect sentiment analysis. In addition,\\nthe performance can be further impacted by the\\narXiv:2306.01004v1  [cs.CL]  31 May 2023'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='textual noise from the confusion among different\\naspects.\\nIn this paper, we propose an Aspect-oriented\\nMethod (AoM) to mitigate aforementioned noises\\nfrom both image and text. AoM can detect aspect-\\nrelevant information from perspectives of both se-\\nmantics and sentiment. There are two key modules\\nin AoM: Aspect-Aware Attention Module (A3M)\\nfor semantically fine-grained image-text alignment\\nand Aspect-Guided Graph Convolutional Network\\n(AG-GCN) for sentiment information aggregation.\\nIn A3M, we first extract aspect features associated\\nwith each visual and textual token. And then aspect-\\nrelevant token representations are computed based\\non their relevance to the corresponding aspect fea-\\ntures. In AG-GCN, we first explicitly add sentiment\\nembeddings to the obtained representations of vi-\\nsual and textual tokens. A multimodal weighted-\\nassociation matrix is constructed containing aspect-\\nto-image-block similarity and word-to-word depen-\\ndency. Then we use a graph convolutional network'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='sual and textual tokens. A multimodal weighted-\\nassociation matrix is constructed containing aspect-\\nto-image-block similarity and word-to-word depen-\\ndency. Then we use a graph convolutional network\\nto aggregate sentiment information according to\\nthe constructed multimodal matrix.\\nThe contributions can be summarized as follows:\\n(1) We propose an aspect-oriented network to\\nmitigate the visual and textual noises from the com-\\nplex image-text interactions.\\n(2) We design an aspect-aware attention mod-\\nule and an aspect-guided graph convolutional net-\\nwork to effectively detect aspect-relevant multi-\\nmodal contents from the perspectives of semantic\\nand sentiment, respectively.\\n(3) Experiments on two benchmark datasets, in-\\ncluding Twitter2015 and Twitter2017, show that\\nour approach generally outperforms the state-of-\\nthe-art methods.\\n2 Related Work\\nIn this section, we review the existing methods for\\nboth ABSA and MABSA.\\n2.1 Aspect-based Sentiment Analysis'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='our approach generally outperforms the state-of-\\nthe-art methods.\\n2 Related Work\\nIn this section, we review the existing methods for\\nboth ABSA and MABSA.\\n2.1 Aspect-based Sentiment Analysis\\nIn the past few years, Aspect-Based Sentiment\\nAnalysis (ABSA) in the textual fields has attracted\\nmuch attention and gained mature research (Chen\\nand Qian, 2020; Oh et al., 2021; Xu et al., 2020).\\nOn the one hand, most recent works are based on\\nthe pre-trained language model BERT because of\\nits remarkable performance in many NLP tasks\\n(Liang et al., 2022a). On the other hand, some\\nrecent efforts focus on modeling the dependency\\nrelationship between aspects and their correspond-\\ning descriptions, in which graph convolutional net-\\nworks (GCNs) (Chen et al., 2022; Liang et al.,\\n2022b, 2020; Li et al., 2021a; Pang et al., 2021)\\nor graph attention networks (GATs) (Yuan et al.,\\n2020) over dependency with the syntactic structure\\nof a sentence are fully exploited.\\n2.2 Multimodal Aspect-based Sentiment'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='or graph attention networks (GATs) (Yuan et al.,\\n2020) over dependency with the syntactic structure\\nof a sentence are fully exploited.\\n2.2 Multimodal Aspect-based Sentiment\\nAnalysis\\nWith the enrichment of multimodal users’ posts\\nin social media, researchers find that images of-\\nfer great supplementary information in aspect term\\nextraction (Wu et al., 2020a; Zhang et al., 2018;\\nAsgari-Chenaghlu et al., 2021) and sentiment anal-\\nysis (Wu et al., 2022; Zhang et al., 2022; Li\\net al., 2021b; Hazarika et al., 2020; Cai et al.,\\n2019). Thus, Multimodal Aspect-based Sentiment\\nAnalysis (MABSA) begins to be widely studied.\\nMABSA task can be divided into two independent\\nsub-tasks, i.e., Multimodal Aspect Term Extraction\\n(MATE) and Multimodal Aspect-oriented Senti-\\nment Classification (MASC). The former extracts\\nall aspect terms in the sentence at the prompt of the\\nimage, and the latter predicts the sentiment polari-\\nties for the aspects.\\nJu et al. (2021) first realizes MABSA in a unified'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='all aspect terms in the sentence at the prompt of the\\nimage, and the latter predicts the sentiment polari-\\nties for the aspects.\\nJu et al. (2021) first realizes MABSA in a unified\\nframework and designs an auxiliary cross-modal\\nrelation detection to control whether the visual in-\\nformation will be used in prediction. For captur-\\ning cross-modal alignment, Ling et al. (2022) con-\\nstructs a generative multimodal architecture based\\non BART for both vision-language pre-training and\\nthe downstream MABSA tasks. Yang et al. (2022)\\ndynamically controls the contributions of the vi-\\nsual information to different aspects via the trick\\nthat the lower confidence of the results predicted\\nby purely textual is, the more contributions from\\nimages will be considered.\\nOn the one hand, the above methods ignore the\\nalignment of fine-grained visual blocks and the\\ncorresponding aspects, which introduce irrelevant\\nvisual noise. On the other hand, modeling of syntax'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='On the one hand, the above methods ignore the\\nalignment of fine-grained visual blocks and the\\ncorresponding aspects, which introduce irrelevant\\nvisual noise. On the other hand, modeling of syntax\\ndependency and sentiment information for aspect\\ndescriptions is absent in these methods, which is\\nproved important in sentiment analysis (Liang et al.,\\n2022a; Kalaivani et al., 2022; Xu et al., 2022).\\nTo tackle the aforementioned issues, we pro-\\npose an aspect-oriented model consisting of Aspect-\\nAware Attention Module and Aspect-Guided Graph\\nConvolutional Network which respectively work'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='A\\n3\\nM\\nComplain about Kyoto ... time ! \\nSenticNet\\nlinear layer (1 to 768)\\ncosine  similarity\\nSenticNet\\n AG\\n-\\nGCN\\nGCN Layers\\nGCN Layers\\natomic feature\\nsoftmax linear layer\\nsigmoid linear layer\\nℎ𝑡𝑡\\nℎ1\\n𝐶𝐶𝐶𝐶\\nℎ𝑡𝑡\\n...\\nℎ𝑡𝑡 ℎ𝑡𝑡\\nℎ2\\n𝐶𝐶𝐶𝐶 ℎ𝑙𝑙\\n𝐶𝐶𝐶𝐶\\n𝛼𝛼𝑡𝑡1 𝛼𝛼𝑡𝑡2\\n𝛼𝛼𝑡𝑡𝑙𝑙\\nℎ𝑡𝑡\\n𝐶𝐶 ℎ𝑡𝑡\\n𝛽𝛽𝑡𝑡 1 −𝛽𝛽𝑡𝑡\\n�ℎ𝑡𝑡\\n... �ℎ𝑉𝑉𝑚𝑚 ...�ℎ𝑇𝑇𝑛𝑛\\n3 3         NEG       13          13        NEU       17          18        POS      <eos>\\nBART decoder\\n<bos>  <AESC> Kyoto Kyoto NEG mayor mayor NEU Mayor Kadokawa POS\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\naspect-guided dependency, D\\nA\\n3\\nM\\nKyoto\\nMayor\\nKadokawa\\nKyoto\\nMayor\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑉𝑉1\\n�ℎ𝑉𝑉2\\n�ℎ𝑉𝑉3\\n�ℎ𝑉𝑉4\\n�ℎ𝑉𝑉5\\n�ℎ𝑉𝑉6\\n�ℎ𝑉𝑉𝑚𝑚\\nComplain Kyoto\\n�ℎ𝑇𝑇1\\nKyoto Kyoto\\n�ℎ𝑇𝑇3\\nmayor mayor\\n�ℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Mayor\\nKadokawa\\n�ℎ𝑉𝑉1\\n�ℎ𝑉𝑉2\\n�ℎ𝑉𝑉3\\n�ℎ𝑉𝑉4\\n�ℎ𝑉𝑉5\\n�ℎ𝑉𝑉6\\n�ℎ𝑉𝑉𝑚𝑚\\nComplain Kyoto\\n�ℎ𝑇𝑇1\\nKyoto Kyoto\\n�ℎ𝑇𝑇3\\nmayor mayor\\n�ℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20\\n... ...... ... ... ......\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos><img> </img>\\nBART encoder\\nResNet                                        BART embedding\\nmask\\n𝒘𝒘𝒊𝒊\\n...𝑤𝑤𝑆𝑆1 𝑤𝑤𝑆𝑆2 𝑤𝑤𝑆𝑆3 𝑤𝑤𝑆𝑆𝑛𝑛\\nAG\\n-\\nGCN\\n GCN Layers\\nGCN Layers\\nweighted association matrix, AS from \\nSenticNet\\nH\\nweighted association matrix, A\\n𝑺𝑺\\n�𝑯𝑯S�𝑯𝑯\\n�𝑯𝑯S\\n�𝑯𝑯\\n�𝑯𝑯\\nHS\\nFigure 2: The overview of our proposed aspect-oriented model AoM.\\nto capture semantic information by fine-grained\\nimage-text alignment and effectively aggregate\\naspect-relevant sentiment information.\\n3 Methodology\\n3.1 Overview\\nTask Definition. Formally, given a tweet that\\ncontains an image V and a sentence with n\\nwords S = (w1, w2, ..., wn), our goal is to ac-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='3 Methodology\\n3.1 Overview\\nTask Definition. Formally, given a tweet that\\ncontains an image V and a sentence with n\\nwords S = (w1, w2, ..., wn), our goal is to ac-\\nquire the sequence Y representing all aspects\\nand their associated sentiment polarities. We\\nformulate the output of MABSA as Y =\\n[as\\n1, ae\\n1, s1, ..., as\\ni , ae\\ni , si, ...as\\nk, ae\\nk, sk], where as\\ni , ae\\ni\\nand si depict the start index, end index of the i-th\\naspect and its sentiment polarity in the tweet, and\\nk is the number of aspects.\\nModel preview. Figure 2 shows the overview\\nof our model architecture, which builds on an\\nencoder-decoder architecture based on BART\\n(Lewis et al., 2019). Between the encoder and\\nthe decoder of BART, we creatively implement\\nthe Aspect-Aware Attention Module (A 3M) and\\nAspect-Guided Graph Convolutional Network (AG-\\nGCN) to align the textual aspect to its associated\\nvisual blocks and textual description, simultane-\\nously mitigate interference both from semantics'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Aspect-Guided Graph Convolutional Network (AG-\\nGCN) to align the textual aspect to its associated\\nvisual blocks and textual description, simultane-\\nously mitigate interference both from semantics\\nand sentiment among different aspects. In the fol-\\nlowing subsections, we will illustrate the details of\\nthe proposed model.\\nFeature Extractor. The initial word embeddings\\nare obtained from the pre-trained BART due to\\nits excellent ability of textual representation. The\\nembeddings of visual blocks are obtained by pre-\\nprocessing via ResNet (Chen et al., 2014) following\\n(Yu et al., 2019). We consider every feature of a\\nvisual block or word token as an atomic feature. We\\nadd <img> and </img> before and after the visual\\nfeatures, <bos> and <eos> for the textual features.\\nThen, we concatenate the multimodal features as\\nX which is the input of BART encoder.\\nWe can get the multimodal hidden state H =\\n{hV\\n0 , ...hV\\ni , ...hV\\nm, hT\\n0 , ..., hT\\nj , ...hT\\nn } with m visual\\nblocks and n words, where hV'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='X which is the input of BART encoder.\\nWe can get the multimodal hidden state H =\\n{hV\\n0 , ...hV\\ni , ...hV\\nm, hT\\n0 , ..., hT\\nj , ...hT\\nn } with m visual\\nblocks and n words, where hV\\ni and hT\\nj refer to\\nfeatures of the i-th visual block and the j-th word\\nin the sentence.\\n3.2 Aspect-Aware Attention Module (A 3M)\\nSince aspects are not specially modeled by BART\\nencoder, we creatively design the Aspect-Aware\\nAttention Module (A3M) aiming to capture aspect-\\nrelevant semantic information. For this purpose, we\\nalign the multimodal information of target objects\\nand filter out the semantic noise from images.\\nFirst, as aspects are usually noun phrases from\\nthe sentences, we extract those phrases as the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='candidate aspects (CA) with the NLP tool Spacy1.\\nAnd from the hidden state H of the BART encoder,\\nwe obtain the features of all candidate aspects de-\\nnoted as HCA = {hCA\\n1 , ..., hCA\\ni , ..., hCA\\nl }, where\\nl is the number of noun phrases in the sentence. To\\nget the relationship between candidate aspects and\\natomic features, we implement an attention-based\\nmechanism guided by the candidate aspects. Given\\nthe t-th hidden feature ht, its attention distribution\\nαt over k candidate aspects is obtained by:\\nZt =tanh((WCAHCA+bCA)⊕(WHht +bH)), (1)\\nαt = softmax(WαZt + bα), (2)\\nwhere Zt ∈ R2d×k is the comprehensive feature\\nextracted from both the candidate aspects and the\\nhidden states. HCA ∈ Rd×k denotes the features\\nof candidate aspects. WCA ∈ Rd×d, WH ∈ Rd×d,\\nWα ∈ R1×2d, bCA, bH and bα are the learned\\nparameters.⊕ is an operator between a matrix and a\\nvector, where the vector is repeated into the appro-\\npriate size to concatenate with the matrix. We then'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Wα ∈ R1×2d, bCA, bH and bα are the learned\\nparameters.⊕ is an operator between a matrix and a\\nvector, where the vector is repeated into the appro-\\npriate size to concatenate with the matrix. We then\\nget the aspect-related hidden feature hA\\nt by calcu-\\nlating the weighted sum of all candidate aspects\\nfollowing the below equation:\\nhA\\nt =\\nkX\\ni\\nαt,ihCA\\ni . (3)\\nFor example, if a visual block is strongly associ-\\nated with the j-th aspect, the corresponding αt,j is\\napproximately 1. hA\\nt would be equal to the aspect\\nsemantically. And if the visual block is not related\\nto any specific candidate aspects, both αt and hA\\nt\\nwould be zero-like vectors of no information.\\nConsidering that not every visual block can be\\nused for prediction, βt is learned to add up the\\natomic feature ht and its aspect-related hidden fea-\\nture hA\\nt . Details are as follows:\\nβt = sigmoid(Wβ[W1ht; W2hA\\nt ] +bβ), (4)\\nˆht = βtht + (1− βt)hA\\nt , (5)\\nwhere Wβ, W1, W2, bβ are parameters, and [; ]'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='ture hA\\nt . Details are as follows:\\nβt = sigmoid(Wβ[W1ht; W2hA\\nt ] +bβ), (4)\\nˆht = βtht + (1− βt)hA\\nt , (5)\\nwhere Wβ, W1, W2, bβ are parameters, and [; ]\\ndenotes the concatenation operator for vectors.\\nˆht ∈ ˆH is the final output of A3M after the seman-\\ntic alignment and the noise reduction procedure.\\nThus we get the noiseless and aligned information\\nfor every atomic feature.\\nPre-training To align the two modalities and re-\\nduce noise, we conduct a pre-training task in A3M.\\n1Spacy: https://spacy.io/\\naverage\\nrelated or unrelated\\nH\\nimage-text relation classification layer\\nA\\n3\\nM\\nKyoto\\nMayor\\nKadokawa\\nKyoto\\nMayor\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑉𝑉1\\n�ℎ𝑉𝑉2\\n�ℎ𝑉𝑉3\\n�ℎ𝑉𝑉4\\n�ℎ𝑉𝑉5\\n�ℎ𝑉𝑉6\\n�ℎ𝑉𝑉𝑉𝑉\\nComplain Kyoto\\n�ℎ𝑇𝑇1\\nKyoto Kyoto\\n�ℎ𝑇𝑇3\\nmayor mayor\\n�ℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20\\n... ...... ... ... ......\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos>\\n<img> </img>'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20\\n... ...... ... ... ......\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos>\\n<img> </img>\\nBART encoder\\nResNet                                        BART embedding\\nFigure 3: The framework of the pre-training task.\\nSpecifically, we detect the image-text relationship\\non the datasets TRC (Vempala and Preo¸ tiuc-Pietro,\\n2019) as illustrated by Figure 3. We first obtain the\\naverage feature of image blocks from the output of\\nA3M and then pass it to a fully connected softmax\\nlayer, which outputs a probability distribution over\\nwhether the image is related to the text. Finally, we\\nuse cross entropy loss to train our model.\\n3.3 Aspect-Guided Graph Convolutional\\nNetwork (AG-GCN)\\nThe aspect-focused interaction between visual\\nmodality and textual modality in A3M concentrates\\non the context semantics, and that is not adequate\\nfor MABSA. Sentiment interference among dif-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='The aspect-focused interaction between visual\\nmodality and textual modality in A3M concentrates\\non the context semantics, and that is not adequate\\nfor MABSA. Sentiment interference among dif-\\nferent aspects still exists and influences sentiment\\nprediction. Thus, we design the Aspect-Guided\\nGraph Convolutional Network (AG-GCN) module\\nto introduce external sentiment information and\\nmitigate emotional confusion among different as-\\npects to a certain extent.\\nSpecifically, for wordwi in the sentence, we gain\\nits affective score wS\\ni from SenticNet (Ma et al.,\\n2018) and project it to the space with the same\\ndimension as hA\\nt , with si obtained. Then we add\\nthe sentiment feature si to the output of A3M:\\nwS\\ni = SenticNet (wi), (6)\\nsi = WSwS\\ni + bS, (7)\\nhS\\ni = ˆhi + si, (8)\\nwhere WS, bS are the learned parameters. hS\\ni is the\\nfeature with affective knowledge.\\nNext, we build a boolean dependency matrix\\nD among visual blocks and words. First, for the\\nword-to-word part, submatrix DT Trepresenting'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Complain\\nabout takes\\nKyoto\\nbit\\na someone you . to the\\nmayor\\nsee\\nand\\nInteresting\\n!\\nthanks\\n, !\\nKadokawa\\nMayor\\nfor\\ntime\\nyour\\ndependency tree\\nFigure 4: The dependency tree of the example men-\\ntioned in the introduction.\\nthe dependency tree2 of the input sentence like Fig-\\nure 4. If two words can be associated within two\\ngenerations, the element of DT Twould be set to\\n1, otherwise 0 instead. For example, “Kyoto” is as-\\nsociated with “bit” (child),“a” (grandchild),“about”\\n(father) and “Complain” (grandfather). Second, the\\nvisual dependency submatrix DV V is initialized\\nas a diagonal matrix. And as for the word-image-\\nblock dependency, denoted as DT V and equaled\\nto DT\\nV T, we set all the elements in the i-th line of\\nDT Vto 1 if the i-th word is an aspect, otherwise 0.\\nAnd the matrix D is defined as:\\nD =\\n\\x14DV V DV T\\nDT V DT T\\n\\x15\\n, (9)\\nConsidering the different importance of differ-\\nent dependencies, we attach weights onto D with\\ncosine similarity among ˆhi as follows:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='D =\\n\\x14DV V DV T\\nDT V DT T\\n\\x15\\n, (9)\\nConsidering the different importance of differ-\\nent dependencies, we attach weights onto D with\\ncosine similarity among ˆhi as follows:\\nAij = DijFcosine_similarity( ˆhi, ˆhj), (10)\\nwhere both D, A∈ R(m+n)×(m+n), and A is the\\nweighted association matrix.\\nAG-GCN takes HS from Eq.8 as initial node\\nrepresentations in the graph. For the i-th node at\\nthe l-th layer, the hidden state hS\\ni,l is updated by the\\nfollowing equation:\\nhS\\ni,l = ReLU(\\nnX\\nj=1\\nAijWlhS\\ni,l−1 + bl), (11)\\nwhere Wl,bl are learned parameters and we use\\nReLU as activation function. Significantly, hS\\ni,0\\nis equal to hS\\ni . Accordingly, we get the final out-\\nput ˆHS from the last GCN layer which is rich in\\nsentiment information. Every underlying aspect\\naggregates its relevant information from both the\\nimage-text pair. Moreover, sentiment confusion\\nof different aspects is weakened because the as-\\nsociation matrix makes little interference among\\ndifferent aspects.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='image-text pair. Moreover, sentiment confusion\\nof different aspects is weakened because the as-\\nsociation matrix makes little interference among\\ndifferent aspects.\\n2We use spaCy toolkit to construct the dependency tree\\nreferring from https://spacy.io\\nTwitter2015 Twitter2017\\n#sentence 3,502 2,910\\n#with one aspect 2,159 (61.65%) 976 (33.54%)\\n#with multiple aspects 1,343 (38.35%) 1,934 (66.46%)\\n#with multiple sentiments 1,257 1,690\\nTable 1: Statistics of the two benchmark datasets. Line 1\\nis the number of sentences. #X in the last 3 lines denotes\\nthe number of sentences with such characteristics X.\\n3.4 Prediction and Loss Function\\nThe BART decoder takes the combination of ˆH,\\nˆHS, and the previous decoder output Y<t as inputs,\\nand predicts the token probability distribution as\\nfollows:\\n˜H = λ1 ˆH + λ2 ˆHS, (12)\\nhd\\nt = Decoder( ˜H; Y<t) (13)\\nHT = (W + ˜HT )/2 (14)\\nP(yt) =softmax([HT ; Cd]hd\\nt ) (15)\\nwhere λ1, λ2 are the hyper-parameters to control'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='follows:\\n˜H = λ1 ˆH + λ2 ˆHS, (12)\\nhd\\nt = Decoder( ˜H; Y<t) (13)\\nHT = (W + ˜HT )/2 (14)\\nP(yt) =softmax([HT ; Cd]hd\\nt ) (15)\\nwhere λ1, λ2 are the hyper-parameters to control\\nthe contribution from the two modules. ˜HT is the\\ntextual part of ˜H. W denotes the embeddings of\\ninput tokens. Cd means the embeddings of the [\\npositive, neutral, negative, <eos>]. The loss func-\\ntion is as follows:\\nL = −EX∼D\\nOX\\nt=1\\nlogP (yt|Y<t, X), (16)\\nwhere O = 2M + 2N + 2is the length of Y , and\\nX denotes the multimodal input.\\n4 Experiment\\n4.1 Experimental settings\\nDatasets. Our two benchmark datasets are Twit-\\nter2015 and Twitter2017 (Yu and Jiang (2019)). As\\nshown in the statistics of Table 1, sentences with\\nmultiple aspects take up a considerable part of the\\ntwo datasets.\\nImplementation Details. Our model is based on\\nBART (Lewis et al., 2019), and the pre-training\\ntask is trained for 40 epochs with batch size 64,\\nand for 35 epochs with batch size 16 on MABSA.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Implementation Details. Our model is based on\\nBART (Lewis et al., 2019), and the pre-training\\ntask is trained for 40 epochs with batch size 64,\\nand for 35 epochs with batch size 16 on MABSA.\\nThe learning rates are both 7e-5 and hidden sizes\\nare 768. Hyper-parameters λ1 and λ2 are 1 and 0.5\\nrespectively. Besides, we pre-train A3M on TRC\\ndataset (Vempala and Preo¸ tiuc-Pietro, 2019), which\\nis divided into two groups according to whether the\\ntext is represented.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Twitter2015 Twitter2017\\nMethods P R F1 P R F1\\nText-based\\nSPAN* (Hu et al., 2019) 53.7 53.9 53.8 59.6 61.7 60.6\\nD-GCN* (Chen et al., 2020) 58.3 58.8 59.4 64.2 64.1 64.1\\nBART* (Yan et al., 2021) 62.9 65.0 63.9 65.2 65.6 65.4\\nMultimodal\\nUMT+TomBERT* (Yu et al., 2020; Yu and Jiang, 2019) 58.4 61.3 59.8 62.3 62.4 62.4\\nOSCGA+TomBERT* (Wu et al., 2020c; Yu and Jiang, 2019) 61.7 63.4 62.5 63.4 64.0 63.7\\nOSCGA-collapse* (Wu et al., 2020c) 63.1 63.7 63.2 63.5 63.5 63.5\\nRpBERT-collapse* (Sun et al., 2021) 49.3 46.9 48.0 57.0 55.4 56.2\\nUMT-collapse (Yu et al., 2020) 61.0 60.4 61.6 60.8 60.0 61.7\\nJML (Ju et al., 2021) 65.0 63.2 64.1 66.5 65.5 66.0\\nVLP-MABSA* (Ling et al., 2022) 65.1 68.3 66.6 66.9 69.2 68.0\\nCMMT (Yang et al., 2022) 64.6 68.7 66.5 67.6 69.4 68.5\\nAoM (ours) 67.9 69.3 68.6 68.4 71.0 69.7\\nTable 2: Results of different methods for MABSA on the two Twitter datasets. * denotes the results from Ling et al.\\n(2022). The best results are bold-typed and the second best ones are underlined.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Table 2: Results of different methods for MABSA on the two Twitter datasets. * denotes the results from Ling et al.\\n(2022). The best results are bold-typed and the second best ones are underlined.\\nEvaluation Metrics. We evaluate the performance\\nof our model on MABSA task and MATE task by\\nMicro-F1 score (F1), Precision (P) and Recall (R),\\nwhile on MASC task we use Accuracy (Acc) and\\nF1 following previous studies.\\n4.2 Baselines\\nWe compare our proposed model with four types\\nof methods listed below.\\nApproaches for textual ABSA. 1) SPAN (Hu\\net al., 2019) detects opinion targets with their senti-\\nments. 2) D-GCN (Chen et al., 2020) models de-\\npendency relations among words via dependency\\ntree. 3) BART (Yan et al., 2021) solves seven\\nABSA subtasks in an end-to-end framework.\\nApproaches for MATE. 1) RAN (Wu et al.,\\n2020b) focus on alignment of text and object re-\\ngions. 2) UMT (Yu et al., 2020) takes text-based\\nentity span detection as an auxiliary task. 3) OS-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Approaches for MATE. 1) RAN (Wu et al.,\\n2020b) focus on alignment of text and object re-\\ngions. 2) UMT (Yu et al., 2020) takes text-based\\nentity span detection as an auxiliary task. 3) OS-\\nCGA (Wu et al., 2020c) foucus on alignments of\\nvisual objects and entities.\\nApproaches for MASC. 1) ESAFN (Yu et al.,\\n2019) is an entity-level sentiment analysis method\\nbased on LSTM. 2) TomBERT (Yu and Jiang,\\n2019) applies BERT to obtain aspect-sensitive tex-\\ntual representations. 3) CapTrBERT (Khan and\\nFu, 2021) translates images into text and construct\\nan auxiliary sentence for fusion.\\nApproaches for MABSA. 1) UMT-collapse\\n(Yu et al., 2020), OSCGA-collapse (Wu et al.,\\n2020c) and RpBERT-collapse (Sun et al., 2021)\\nare adapted from models for MATE by using col-\\nlapsed labels to represent aspect and sentiment\\npairs. 2) UMT+TomBERT, OSCGA+TomBERT\\nare two pipeline approaches by combining UMT\\n(Yu et al., 2020) or OSCGA (Wu et al., 2020c) with\\nTomBERT (Yu and Jiang, 2019). 3)JML (Ju et al.,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='pairs. 2) UMT+TomBERT, OSCGA+TomBERT\\nare two pipeline approaches by combining UMT\\n(Yu et al., 2020) or OSCGA (Wu et al., 2020c) with\\nTomBERT (Yu and Jiang, 2019). 3)JML (Ju et al.,\\n2021) is the first joint model for MABSA with aux-\\niliary cross-modal relation detection module. 4)\\nCMMT (Yang et al., 2022) implements a gate to\\ncontrol the multimodal information contributions\\nduring inter-modal interactions. 5) VLP-MABSA\\n(Ling et al., 2022) performs five task-specific pre-\\ntraining tasks to model aspects, opinions and align-\\nments.\\n4.3 Main Results\\nIn this section, we show the excellent performance\\nof AoM on the two datasets for the three tasks\\ncompared with SOTAs.\\nPerformance on MABSA: The results for\\nMABSA are shown in Table 2. First, our AoM\\nfar exceeds all text-based models, which means\\ndetection of richer visual information and textual\\ninformation in our model is helpful. Second, multi-\\nmodal pipeline methods and adaptive methods are\\ngenerally unsatisfactory, because they ignore the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='information in our model is helpful. Second, multi-\\nmodal pipeline methods and adaptive methods are\\ngenerally unsatisfactory, because they ignore the\\ninteraction between the semantic information and\\nsentiment for the two sub-tasks. Last, AoM out-\\nperforms all multimodal methods in every metric.\\nEspecially, AoM achieves the improvement of 2%\\nand 1.2% with respect to F1 in contrast with the sec-\\nond best models on two datasets (VLP-MABSA for\\nTwitter2015 and CMMT for Twitter2017), which\\ndemonstrates the effectiveness of learning aspect-\\nrelevant visual blocks and textual words compared\\nto focusing on all visual and textual inputs.\\nPerformance on MATE:As shown in Table 3,\\nAoM is ahead of most of the current models and\\nperforms the best in Twitter 2015 by 0.3% higher\\nthan the second best CMMT on F1. The perfor-\\nmance of CMMT in Twitter2017 is 0.8% higher'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Twitter2015 Twitter2017\\nMethods P R F1 P R F1\\nRAN* 80.5 81.5 81.0 90.7 90.7 90.0\\nUMT* 77.8 81.7 79.7 86.7 86.8 86.7\\nOSCGA* 81.7 82.1 81.9 90.2 90.7 90.4\\nJML* 83.6 81.2 82.4 92.0 90.7 91.4\\nVLP-MABSA* 83.6 87.985.7 90.8 92.6 91.7\\nCMMT 83.9 88.1 85.9 92.2 93.9 93.1\\nAoM (ours) 84.6 87.9 86.2 91.8 92.8 92.3\\nTable 3: Results of different methods for MATE. * de-\\nnotes the results from Ling et al. (2022).\\nTwitter2015 Twitter2017\\nMethods ACC F1 ACC F1\\nESAFN 73.4 67.4 67.8 64.2\\nTomBERT 77.2 71.8 70.5 68.0\\nCapTrBERT 78.0 73.2 72.3 70.2\\nJML 78.7 - 72.7 -\\nVLP-MABSA 78.6 73.873.8 71.8\\nCMMT 77.9 - 73.8 -\\nAoM (ours)80.2 75.9 76.4 75.0\\nTable 4: Results of different methods for MASC.\\nthan ours, probably due to our model wrongly pre-\\ndicting some noun phrases as aspects. But consid-\\nering the improvement in MASC and MABSA, it\\nis still worthy treating all noun phrases in the sen-\\ntence as candidate aspects when acquiring aspect-\\nrelevant visual information.\\nPerformance on MASC: Table 4 shows the per-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='is still worthy treating all noun phrases in the sen-\\ntence as candidate aspects when acquiring aspect-\\nrelevant visual information.\\nPerformance on MASC: Table 4 shows the per-\\nformance of MASC. It is exciting that our model\\noutperforms the second-best results by 1.5% and\\n2.6% in accuracy, 2.1% and 3.2% points in F1 score\\non Twitter2015 and Twitter2017. It demonstrates\\nthat AoM has the ability to detect aspect-related\\nsentiment information from both images and text,\\neven disturbed by other noisy aspects.\\n4.4 Ablation Study\\nIn this section, we research the effectiveness of\\neach component in AoM, the results are shown in\\nTable 5.\\nW/o A3M&AG-GCN shows that after remov-\\ning the two specially designed modules, the per-\\nTwitter2015 Twitter2017\\nMethods P R F1 P R F1\\nFull 67.9 69.3 68.6 68.4 71.0 69.7\\nw/o A3M&AG-GCN 65.7 67.3 66.5 66.5 69.0 67.8\\nw/o A3M&TRC 62.1 61.0 61.6 63.7 64.1 63.9\\nw/o TRC 66.8 68.4 67.6 67.8 69.8 68.8\\nw/o AG-GCN 67.0 69.4 68.2 67.8 69.7 68.8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='w/o A3M&AG-GCN 65.7 67.3 66.5 66.5 69.0 67.8\\nw/o A3M&TRC 62.1 61.0 61.6 63.7 64.1 63.9\\nw/o TRC 66.8 68.4 67.6 67.8 69.8 68.8\\nw/o AG-GCN 67.0 69.4 68.2 67.8 69.7 68.8\\nw/o SenticNet 65.7 70.5 68.0 68.1 69.4 68.7\\nw/o TRC&AG-GCN 66.7 69.2 68.0 67.8 69.5 68.6\\nTable 5: The performance comparison of our full model\\nand its ablated approaches.\\nImage\\nText\\nVLP-MABSA\\nBART+A3M\\nAoM\\n(a) NBA Western Conference Finals: \\nGolden State Warriors shock \\nOklahoma City Thunder,…\\n( NBA, NEU ) (√, √)\\n---\\n( Oklahoma, NEU ) (×, ×)\\n( NBA, NEU ) (√, √)\\n( Golden State Warriors , POS ) (√, √)\\n( Oklahoma City Thunder, NEG ) (√, √)\\n( NBA, NEU ) (√, √)\\n( Golden State Warriors , POS ) (√, √)\\n( Oklahoma City Thunder, NEG ) (√, √)\\n(b) This subtle difference between \\nDaniel Radcliffe and Elijah Wood\\nis pretty unsettling.\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah, NEU ) (×, ×)\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah Wood, NEG ) (√, √)\\n( Daniel Radcliffe, NEG ) (√, √)\\n( Elijah Wood, NEG ) (√, √)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='is pretty unsettling.\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah, NEU ) (×, ×)\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah Wood, NEG ) (√, √)\\n( Daniel Radcliffe, NEG ) (√, √)\\n( Elijah Wood, NEG ) (√, √)\\nFigure 5: Two cases with predictions by VLP-MABSA\\n(Ling et al., 2022), BART+A3M, and our model.\\nformance declines by 2.1% on Twitter2015 and\\n1.9% on Twitter2017. It fully demonstrates their\\ncontributions to learning effective information.\\nW/o A3M&TRC performs worse after remov-\\ning A 3M including the pre-training on TRC. It\\nproves the necessity of modeling semantic align-\\nment between visual blocks and aspects in A 3M.\\nWith the alignment, AG-GCN can obtain appropri-\\nate aspect-image-block and text-text association.\\nW/o TRC pre-training shows a slight drop after\\nwe remove the TRC pre-training on A3M, which\\nimplies relevant pre-training task is useful for the\\nmodel to learn better parameters.\\nW/o AG-GCN displays the performance with-\\nout AG-GCN, declining by 0.42% on Twitter2015'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='implies relevant pre-training task is useful for the\\nmodel to learn better parameters.\\nW/o AG-GCN displays the performance with-\\nout AG-GCN, declining by 0.42% on Twitter2015\\nand 0.9% on Twitter2017. It means that AG-GCN\\ndoes make the prediction focus on specific aspects\\nrelated to blocks and words with syntax dependen-\\ncies. In other words, the multimodal interference\\nfrom other aspects can be mitigated.\\nW/o SenticNet is the model without sentiment\\ninformation in AG-GCN. Its performance shows\\nadding external affective knowledge can enhance\\nthe sentiment comprehension of the model.\\nW/o TRC&AG-GCN is the BART model only\\nwith our A3M module. We can see from Table 5\\nthat w/o TRC&AG-GCNimproves w/o A3M&AG-\\nGCN by 1.5% and 0.8%. So it is effective to align\\nthe fine-grained visual block to related aspect and\\nreduce irrelevant information.\\n4.5 Case Study\\nTo better analyze how the Aspect-Aware Attention\\nModule and Aspect-Guided Graph Convolutional'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='the fine-grained visual block to related aspect and\\nreduce irrelevant information.\\n4.5 Case Study\\nTo better analyze how the Aspect-Aware Attention\\nModule and Aspect-Guided Graph Convolutional\\nNetwork work, we present the case study as fol-\\nlows.\\nFigure 5 displays two examples with predic-\\ntions from VLP-MABSA (Ling et al., 2022),'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='A3M\\nKyoto\\nbit\\nsomeone\\nmayor\\nMayor Kadokawa\\nthanks\\ntime\\n(I.a) attention of CAs\\n(I.b) learned visual attention\\nComplain about\\nKyoto a bit and\\nsomeone takes you\\nto see the mayor.\\nInteresting! Mayor\\nKadokawa, thanks\\nfor your time!\\nInputs\\nAspect-Sentiment Pairs\\n<Kyoto, NEG>\\n<mayor, NEU>\\n<Mayor Kadokawa, POS>\\nThe image-related probability \\ndistribution of candidate aspects.\\nAG-GCN\\n(II.a) word-to-word association matrix (II.b) word-to-visual-block association matrix\\n(II.c) aspect-relevant visual attention map\\nMayor Kadokawa\\nMayor Kadokawa thanks\\nKyoto Mayor Kadokawa\\n(II.d) information relevant to “Mayor Kadokawa”\\n0.8 0.7 0.6 0.1 0.00.20.30.40.5\\nFigure 6: Visualization of the attention maps in A3M and the sub-parts of the weighted-association matrix AG-GCN.\\nBART+A3M and our AoM. In example (a), VLP-\\nMABSA misses the aspect “Golden State War-\\nriors”, gets an incomplete aspect “Oklahoma City\\nThunder” and wrongly predicts the sentiment. It\\nmay be caused by the interference from the vi-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='MABSA misses the aspect “Golden State War-\\nriors”, gets an incomplete aspect “Oklahoma City\\nThunder” and wrongly predicts the sentiment. It\\nmay be caused by the interference from the vi-\\nsual region which represents pride expression of a\\nperson. However, BART+A3M gets all right pre-\\ndictions due to the ability of aspect-oriented atten-\\ntion. In example (b), compared with our whole\\nmodel, BART+A3M wrongly predicts the senti-\\nment of “Daniel Radcliffe” which should be nega-\\ntive. We attribute the wrong prediction to lacking\\nsyntax association which benefits sentiment trans-\\nmission. In other words, AG-GCN contributes to\\nthe correctness.\\n4.6 Attention Visualization\\nTo investigate the effectiveness of detecting aspect-\\nrelevant information, we visualize the attention pro-\\ncess as shown in Figure 6.\\nFor A3M: (i) Figure 6-(I.a) shows the attention\\nweights of candidate aspects computed according\\nto the images. We can see that “Mayor Kadokawa”\\nis the most relevant aspect. (ii) Figure 6-(I.b)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='For A3M: (i) Figure 6-(I.a) shows the attention\\nweights of candidate aspects computed according\\nto the images. We can see that “Mayor Kadokawa”\\nis the most relevant aspect. (ii) Figure 6-(I.b)\\nshows the proportions of the visual information re-\\nserved at the last step in A3M, where we weighted\\nadd up the representations of visual blocks and\\nthe corresponding aspects. The heat map shows\\nthat the visual information associated with “Mayor\\nKadokawa” is reserved to a great extent, while the\\nhelpless information from other blocks is disre-\\ngarded as noise. It demonstrates that attention in\\nA3M is able to detect aspect-relevant information.\\nFor AG-GCN: (i) Figure 6-(II.a) shows the\\nword-to-word part of the weighted association ma-\\ntrix. The matrix effectively excludes sentiment\\ninterference from other aspects by adding syntax\\ndependency information. For example, the senti-\\nment of “mayor” cannot be influenced by irrelevant\\nkeywords, such as “Complain” and “thanks”. (ii)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='interference from other aspects by adding syntax\\ndependency information. For example, the senti-\\nment of “mayor” cannot be influenced by irrelevant\\nkeywords, such as “Complain” and “thanks”. (ii)\\nFigure 6-(II.b) shows the dependencies between\\nvisual blocks and words. (iii) Specifically, we vi-\\nsualize the visual attention of aspects “Kyoto” (see\\nFigure 6-(II.c) left) and “Mayor Kadokawa” (see\\nFigure 6-(II.c) right). We can see that “Kyoto” pays\\nmore attention to the pictures hanging on the wall\\nwhich are full of Japanese elements related to the\\nplace, while “Mayor Kadokawa” focus more on the\\njoyful expressions of the two people. (iv) Figure\\n6-(II.d) shows the words and image blocks “Mayor\\nKadokawa” focused on in sentiment transmission.\\nIt’s obvious that these attentions are helpful for the\\nprediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='prediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='we explicitly add sentiment embedding into AoM.\\nThen, a graph convolutional network is used to ag-\\ngregate the semantic and sentiment embedding un-\\nder the guidance of both image-text similarity and\\nsyntax dependency in sentences. The experimental\\nresults on two widely used datasets demonstrate\\nthe effectiveness of our method.\\nLimitations\\nThough our proposed method outperforms cur-\\nrent state-of-the-art methods, there are still many\\nchallenges we should overcome in future research.\\nFirst, for colloquial expression which confuses cur-\\nrent dependency tree parser, we should come up\\nwith new solutions. Second, emotional prediction\\nof tweet posts describing current issues needs exter-\\nnal knowledge, which is absent in existing research.\\nAcknowledgments\\nWe thank anonymous reviewers for their valu-\\nable comments. This work was supported by\\nthe Natural Science Foundation of Tianjin,\\nChina (No.22JCJQJC00150, 22JCQNJC01580),\\nthe National Natural Science Foundation of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='able comments. This work was supported by\\nthe Natural Science Foundation of Tianjin,\\nChina (No.22JCJQJC00150, 22JCQNJC01580),\\nthe National Natural Science Foundation of\\nChina (No.62272250), Tianjin Research In-\\nnovation Project for Postgraduate Students\\n(No.2022SKYZ232), and the Fundamental\\nResearch Funds for the Central Universities (No.\\n63231149).\\nReferences\\nMeysam Asgari-Chenaghlu, M. Reza Feizi-Derakhshi,\\nLeili Farzinvash, M. A. Balafar, and Cina Motamed.\\n2021. CWI: A multimodal deep learning approach\\nfor named entity recognition from social media us-\\ning character, word and image features. Neural\\nComputing and Applications, 34(3):1905–1922.\\nYitao Cai, Huiyu Cai, and Xiaojun Wan. 2019.\\nMulti-Modal Sarcasm Detection in Twitter with\\nHierarchical Fusion Model. In Proceedings of\\nthe 57th Annual Meeting of the Association for\\nComputational Linguistics, pages 2506–2515, Flo-\\nrence, Italy. Association for Computational Linguis-\\ntics.\\nGuimin Chen, Yuanhe Tian, and Yan Song. 2020.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics, pages 2506–2515, Flo-\\nrence, Italy. Association for Computational Linguis-\\ntics.\\nGuimin Chen, Yuanhe Tian, and Yan Song. 2020.\\nJoint aspect extraction and sentiment analysis with\\ndirectional graph convolutional networks. In\\nProceedings of the 28th international conference on\\ncomputational linguistics, pages 272–279.\\nHao Chen, Zepeng Zhai, Fangxiang Feng, Ruifan\\nLi, and Xiaojie Wang. 2022. Enhanced Multi-\\nChannel Graph Convolutional Network for\\nAspect Sentiment Triplet Extraction. In\\nProceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics (V olume\\n1: Long Papers), pages 2974–2985, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nTao Chen, Damian Borth, Trevor Darrell, and Shih-\\nFu Chang. 2014. Deepsentibank: Visual sentiment\\nconcept classification with deep convolutional neural\\nnetworks.\\nZhuang Chen and Tieyun Qian. 2020. Relation-\\nAware Collaborative Learning for Unified Aspect-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='concept classification with deep convolutional neural\\nnetworks.\\nZhuang Chen and Tieyun Qian. 2020. Relation-\\nAware Collaborative Learning for Unified Aspect-\\nBased Sentiment Analysis. In Proceedings of\\nthe 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 3685–3694, On-\\nline. Association for Computational Linguistics.\\nDevamanyu Hazarika, Roger Zimmermann, and Sou-\\njanya Poria. 2020. Misa: Modality-invariant\\nand -specific representations for multimodal senti-\\nment analysis. In Proceedings of the 28th ACM\\nInternational Conference on Multimedia, MM ’20,\\npage 1122–1131, New York, NY , USA. Association\\nfor Computing Machinery.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Dongsheng\\nLi, and Yiwei Lv. 2019. Open-domain targeted senti-\\nment analysis via span-based extraction and classifi-\\ncation. arXiv preprint arXiv:1906.03820.\\nXincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\\nShoushan Li, Min Zhang, and Guodong Zhou.\\n2021. Joint Multi-modal Aspect-Sentiment Anal-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='cation. arXiv preprint arXiv:1906.03820.\\nXincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\\nShoushan Li, Min Zhang, and Guodong Zhou.\\n2021. Joint Multi-modal Aspect-Sentiment Anal-\\nysis with Auxiliary Cross-modal Relation Detec-\\ntion. In Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 4395–4405, Online and Punta Cana, Domini-\\ncan Republic. Association for Computational Lin-\\nguistics.\\nKS Kalaivani, M Rakshana, K Mounika, and D Sindhu.\\n2022. Senticnet-based feature weighting scheme for\\nsentiment classification. In Mobile Computing and\\nSustainable Informatics, pages 839–848. Springer.\\nZaid Khan and Yun Fu. 2021. Exploiting bert for\\nmultimodal target sentiment classification through\\ninput space translation. In Proceedings of the\\n29th ACM International Conference on Multimedia,\\npages 3034–3042.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='pages 3034–3042.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\\nnoising sequence-to-sequence pre-training for natural\\nlanguage generation, translation, and comprehension.\\narXiv preprint arXiv:1910.13461.\\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu\\nMa, Xiaojie Wang, and Eduard Hovy.\\n2021a. Dual Graph Convolutional Networks\\nfor Aspect-based Sentiment Analysis. In\\nProceedings of the 59th Annual Meeting of\\nthe Association for Computational Linguistics'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='and the 11th International Joint Conference\\non Natural Language Processing (V olume 1:\\nLong Papers), pages 6319–6329, Online. Associa-\\ntion for Computational Linguistics.\\nYuanqing Li, Ke Zhang, Jingyu Wang, and Xinbo Gao.\\n2021b. A cognitive brain model for multimodal sen-\\ntiment analysis based on attention neural networks.\\nNeurocomputing, 430:159–173.\\nBin Liang, Hang Su, Lin Gui, Erik Cambria, and\\nRuifeng Xu. 2022a. Aspect-based sentiment anal-\\nysis via affective knowledge enhanced graph con-\\nvolutional networks. Knowledge-Based Systems,\\n235:107643.\\nBin Liang, Rongdi Yin, Lin Gui, Jiachen Du, and\\nRuifeng Xu. 2020. Jointly Learning Aspect-Focused\\nand Inter-Aspect Relations with Graph Convolu-\\ntional Networks for Aspect Sentiment Analysis. In\\nProceedings of the 28th International Conference\\non Computational Linguistics, pages 150–161,\\nBarcelona, Spain (Online). International Committee\\non Computational Linguistics.\\nShuo Liang, Wei Wei, Xian-Ling Mao, Fei Wang, and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='on Computational Linguistics, pages 150–161,\\nBarcelona, Spain (Online). International Committee\\non Computational Linguistics.\\nShuo Liang, Wei Wei, Xian-Ling Mao, Fei Wang, and\\nZhiyong He. 2022b. BiSyn-GAT+: Bi-Syntax Aware\\nGraph Attention Network for Aspect-based Senti-\\nment Analysis. In Findings of the Association for\\nComputational Linguistics: ACL 2022, pages 1835–\\n1848, Dublin, Ireland. Association for Computational\\nLinguistics.\\nYan Ling, Jianfei Yu, and Rui Xia. 2022. Vision-\\nLanguage Pre-Training for Multimodal Aspect-\\nBased Sentiment Analysis. In Proceedings of\\nthe 60th Annual Meeting of the Association\\nfor Computational Linguistics (V olume 1:\\nLong Papers), pages 2149–2159, Dublin, Ire-\\nland. Association for Computational Linguistics.\\nYanxia Lv, Fangna Wei, Lihong Cao, Sancheng Peng,\\nJianwei Niu, Shui Yu, and Cuirong Wang. 2021.\\nAspect-level sentiment analysis using context and\\naspect memory network. Neurocomputing, 428:195–\\n205.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Yanxia Lv, Fangna Wei, Lihong Cao, Sancheng Peng,\\nJianwei Niu, Shui Yu, and Cuirong Wang. 2021.\\nAspect-level sentiment analysis using context and\\naspect memory network. Neurocomputing, 428:195–\\n205.\\nYukun Ma, Haiyun Peng, and Erik Cambria. 2018. Tar-\\ngeted aspect-based sentiment analysis via embed-\\nding commonsense knowledge into an attentive lstm.\\nProceedings of the AAAI Conference on Artificial\\nIntelligence, 32(1).\\nShinhyeok Oh, Dongyub Lee, Taesun Whang, IlNam\\nPark, Seo Gaeun, EungGyun Kim, and Harksoo\\nKim. 2021. Deep Context- and Relation-Aware\\nLearning for Aspect-based Sentiment Analysis.\\nIn Proceedings of the 59th Annual Meeting of\\nthe Association for Computational Linguistics\\nand the 11th International Joint Conference\\non Natural Language Processing (V olume 2:\\nShort Papers), pages 495–503, Online. Association\\nfor Computational Linguistics.\\nShiguan Pang, Yun Xue, Zehao Yan, Weihao Huang,\\nand Jinhui Feng. 2021. Dynamic and Multi-Channel'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Short Papers), pages 495–503, Online. Association\\nfor Computational Linguistics.\\nShiguan Pang, Yun Xue, Zehao Yan, Weihao Huang,\\nand Jinhui Feng. 2021. Dynamic and Multi-Channel\\nGraph Convolutional Networks for Aspect-Based\\nSentiment Analysis. In Findings of the Association\\nfor Computational Linguistics: ACL-IJCNLP 2021,\\npages 2627–2636, Online. Association for Computa-\\ntional Linguistics.\\nLin Sun, Jiquan Wang, Kai Zhang, Yindu Su, and Fang-\\nsheng Weng. 2021. Rpbert: A text-image relation\\npropagation-based bert model for multimodal ner.\\nProceedings of the AAAI Conference on Artificial\\nIntelligence, 35(15):13860–13868.\\nAlakananda Vempala and Daniel Preo¸ tiuc-Pietro. 2019.\\nCategorizing and inferring the relationship between\\nthe text and image of twitter posts. In Proceedings\\nof the 57th annual meeting of the Association for\\nComputational Linguistics, pages 2830–2840.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi, and Lian Chi. 2020a. Multimodal as-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='of the 57th annual meeting of the Association for\\nComputational Linguistics, pages 2830–2840.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi, and Lian Chi. 2020a. Multimodal as-\\npect extraction with region-aware alignment net-\\nwork. In Natural Language Processing and Chinese\\nComputing, pages 145–156, Cham. Springer Interna-\\ntional Publishing.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi, and Lian Chi. 2020b. Multimodal aspect ex-\\ntraction with region-aware alignment network. In\\nCCF International Conference on Natural Language\\nProcessing and Chinese Computing, pages 145–156.\\nSpringer.\\nYang Wu, Yanyan Zhao, Hao Yang, Song Chen,\\nBing Qin, Xiaohuan Cao, and Wenting Zhao.\\n2022. Sentiment Word Aware Multimodal Re-\\nfinement for Multimodal Sentiment Analysis with\\nASR Errors. In Findings of the Association for\\nComputational Linguistics: ACL 2022, pages 1397–\\n1406, Dublin, Ireland. Association for Computational\\nLinguistics.\\nZhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics: ACL 2022, pages 1397–\\n1406, Dublin, Ireland. Association for Computational\\nLinguistics.\\nZhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen,\\nHo-fung Leung, and Qing Li. 2020c. Multimodal\\nrepresentation with embedded visual guiding ob-\\njects for named entity recognition in social media\\nposts. In Proceedings of the 28th ACM International\\nConference on Multimedia, pages 1038–1046.\\nJunjie Xu, Shuwen Yang, Luwei Xiao, Zhichao Fu,\\nXingjiao Wu, Tianlong Ma, and Liang He. 2022.\\nGraph convolution over the semantic-syntactic hybrid\\ngraph enhanced by affective knowledge for aspect-\\nlevel sentiment classification. In 2022 International\\nJoint Conference on Neural Networks (IJCNN),\\npages 1–8. IEEE.\\nLu Xu, Hao Li, Wei Lu, and Lidong Bing.\\n2020. Position-Aware Tagging for Aspect\\nSentiment Triplet Extraction. In Proceedings\\nof the 2020 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP), pages\\n2339–2349, Online. Association for Computational\\nLinguistics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Hang Yan, Junqi Dai, Xipeng Qiu, Zheng Zhang,\\net al. 2021. A unified generative framework for\\naspect-based sentiment analysis. arXiv preprint\\narXiv:2106.04300.\\nLi Yang, Jin-Cheon Na, and Jianfei Yu. 2022.\\nCross-Modal Multitask Transformer for End-to-\\nEnd Multimodal Aspect-Based Sentiment Anal-\\nysis. Information Processing & Management,\\n59(5):103038.\\nJianfei Yu and Jing Jiang. 2019. Adapting bert\\nfor target-oriented multimodal sentiment classi-\\nfication. In Proceedings of the Twenty-Eighth\\nInternational Joint Conference on Artificial\\nIntelligence, IJCAI-19, pages 5408–5414. Interna-\\ntional Joint Conferences on Artificial Intelligence\\nOrganization.\\nJianfei Yu, Jing Jiang, and Rui Xia. 2019. Entity-\\nsensitive attention and fusion network for entity-level\\nmultimodal sentiment classification. IEEE/ACM\\nTransactions on Audio, Speech, and Language\\nProcessing, 28:429–439.\\nJianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020.\\nImproving multimodal named entity recognition via'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Transactions on Audio, Speech, and Language\\nProcessing, 28:429–439.\\nJianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020.\\nImproving multimodal named entity recognition via\\nentity span detection with unified multimodal trans-\\nformer. In Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics,\\npages 3342–3352, Online. Association for Computa-\\ntional Linguistics.\\nLi Yuan, Jin Wang, Liang-Chih Yu, and Xuejie Zhang.\\n2020. Graph attention network with memory fusion\\nfor aspect-level sentiment analysis. In Proceedings\\nof the 1st Conference of the Asia-Pacific Chapter of\\nthe Association for Computational Linguistics and\\nthe 10th International Joint Conference on Natural\\nLanguage Processing, pages 27–36, Suzhou, China.\\nAssociation for Computational Linguistics.\\nQi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang.\\n2018. Adaptive co-attention network for named en-\\ntity recognition in tweets. In Thirty-Second AAAI\\nConference on Artificial Intelligence.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-05T01:00:14+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-05T01:00:14+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'AoM.pdf', 'file_type': 'pdf'}, page_content='Qi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang.\\n2018. Adaptive co-attention network for named en-\\ntity recognition in tweets. In Thirty-Second AAAI\\nConference on Artificial Intelligence.\\nYuhao Zhang, Ying Zhang, Wenya Guo, Xiangrui Cai,\\nand Xiaojie Yuan. 2022. Learning disentangled rep-\\nresentation for multimodal cross-domain sentiment\\nanalysis. IEEE Transactions on Neural Networks\\nand Learning Systems.')]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "chunks=split_documents(all_pdf_documents)\n",
        "chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97fe92ea",
      "metadata": {
        "id": "97fe92ea"
      },
      "source": [
        "### embedding And vectorStoreDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ae3031",
      "metadata": {
        "id": "e3ae3031"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import uuid\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543614c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "543614c4",
        "outputId": "f705c52e-942a-4746-d11b-7cac4dd716cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model: all-MiniLM-L6-v2\n",
            "Model loaded successfully. Embedding dimension: 384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.EmbeddingManager at 0x7e236c3d3f50>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "class EmbeddingManager:\n",
        "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize the embedding manager\n",
        "\n",
        "        Args:\n",
        "            model_name: HuggingFace model name for sentence embeddings\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading embedding model: {self.model_name}\")\n",
        "            self.model = SentenceTransformer(self.model_name)\n",
        "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {self.model_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate embeddings for a list of texts\n",
        "\n",
        "        Args:\n",
        "            texts: List of text strings to embed\n",
        "\n",
        "        Returns:\n",
        "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
        "        \"\"\"\n",
        "        if not self.model:\n",
        "            raise ValueError(\"Model not loaded\")\n",
        "\n",
        "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "## initialize the embedding manager\n",
        "\n",
        "embedding_manager=EmbeddingManager()\n",
        "embedding_manager"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f62c9e3b",
      "metadata": {
        "id": "f62c9e3b"
      },
      "source": [
        "### VectorStore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "mArN5tzACM4F"
      },
      "id": "mArN5tzACM4F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correct store embedding in RAM"
      ],
      "metadata": {
        "id": "fQm_xFNgWo3h"
      },
      "id": "fQm_xFNgWo3h"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid\n",
        "import numpy as np\n",
        "import chromadb\n",
        "from typing import List, Any\n",
        "\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        collection_name: str = \"pdf_documents\",\n",
        "        persist_directory: str = \"/content/drive/MyDrive/AI Engineering/Traditional RAG/vector_store\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the vector store\n",
        "\n",
        "        Args:\n",
        "            collection_name: Name of the ChromaDB collection\n",
        "            persist_directory: Directory to persist the vector store\n",
        "        \"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        self.persist_directory = persist_directory\n",
        "        self.client = None\n",
        "        self.collection = None\n",
        "        self._initialize_store()\n",
        "\n",
        "    def _initialize_store(self):\n",
        "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
        "        try:\n",
        "            # Ensure persistence dir exists\n",
        "            os.makedirs(self.persist_directory, exist_ok=True)\n",
        "\n",
        "            print(\"👉 Trying to initialize persistent ChromaDB...\")\n",
        "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
        "\n",
        "            # Create or get collection\n",
        "            self.collection = self.client.get_or_create_collection(\n",
        "                name=self.collection_name,\n",
        "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
        "            )\n",
        "            print(f\"✅ Persistent vector store initialized. Collection: {self.collection_name}\")\n",
        "            print(f\"   Existing documents in collection: {self.collection.count()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Persistent DB failed: {e}\")\n",
        "            print(\"👉 Falling back to in-memory ChromaDB (data will NOT be saved).\")\n",
        "\n",
        "            # Fallback to in-memory\n",
        "            self.client = chromadb.Client()\n",
        "            self.collection = self.client.get_or_create_collection(\n",
        "                name=self.collection_name,\n",
        "                metadata={\"description\": \"In-memory PDF document embeddings for RAG\"}\n",
        "            )\n",
        "            print(f\"✅ In-memory vector store initialized. Collection: {self.collection_name}\")\n",
        "\n",
        "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
        "        \"\"\"\n",
        "        Add documents and their embeddings to the vector store\n",
        "\n",
        "        Args:\n",
        "            documents: List of LangChain documents\n",
        "            embeddings: Corresponding embeddings for the documents\n",
        "        \"\"\"\n",
        "        if len(documents) != len(embeddings):\n",
        "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
        "\n",
        "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
        "\n",
        "        ids = []\n",
        "        metadatas = []\n",
        "        documents_text = []\n",
        "        embeddings_list = []\n",
        "\n",
        "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
        "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
        "            ids.append(doc_id)\n",
        "\n",
        "            metadata = dict(doc.metadata)\n",
        "            metadata[\"doc_index\"] = i\n",
        "            metadata[\"content_length\"] = len(doc.page_content)\n",
        "            metadatas.append(metadata)\n",
        "\n",
        "            documents_text.append(doc.page_content)\n",
        "            embeddings_list.append(embedding.tolist())\n",
        "\n",
        "        try:\n",
        "            self.collection.add(\n",
        "                ids=ids,\n",
        "                embeddings=embeddings_list,\n",
        "                metadatas=metadatas,\n",
        "                documents=documents_text,\n",
        "            )\n",
        "            print(f\"✅ Successfully added {len(documents)} documents to vector store\")\n",
        "            print(f\"   Total documents in collection: {self.collection.count()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error adding documents to vector store: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "# ---- Initialize ----\n",
        "vectorstore = VectorStore()\n",
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AeUeTV9SSN-",
        "outputId": "e5353ce7-2de4-42f4-a2fd-21929fdf8a54"
      },
      "id": "0AeUeTV9SSN-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👉 Trying to initialize persistent ChromaDB...\n",
            "⚠️ Persistent DB failed: Database error: error returned from database: (code: 1) no such table: tenants\n",
            "👉 Falling back to in-memory ChromaDB (data will NOT be saved).\n",
            "✅ In-memory vector store initialized. Collection: pdf_documents\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.VectorStore at 0x7e236c8b85f0>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts=[doc.page_content for doc in chunks]\n",
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbczCNLaKLVL",
        "outputId": "d6559d7b-ba07-4367-ea00-277c474796e8"
      },
      "id": "PbczCNLaKLVL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\\nYing Zhang1 Xiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented',\n",
              " 'timent of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented\\nMethod (AoM) to detect aspect-relevant seman-\\ntic and sentiment information. Specifically, an\\naspect-aware attention module is designed to\\nsimultaneously select textual tokens and im-\\nage blocks that are semantically related to the\\naspects. To accurately aggregate sentiment in-\\nformation, we explicitly introduce sentiment\\nembedding into AoM, and use a graph convo-\\nlutional network to model the vision-text and\\ntext-text interaction. Extensive experiments\\ndemonstrate the superiority of AoM to existing\\nmethods. The source code is publicly released\\nat https://github.com/SilyRab/AoM.\\n1 Introduction\\nAs an important and promising task in the field of\\nsentiment analysis, Multimodal Aspect-Based Sen-\\ntiment Analysis (MABSA) has attracted increasing\\nattention (Lv et al., 2021; Ju et al., 2021). Given an',\n",
              " 'sentiment analysis, Multimodal Aspect-Based Sen-\\ntiment Analysis (MABSA) has attracted increasing\\nattention (Lv et al., 2021; Ju et al., 2021). Given an\\nimage and corresponding text, MABSA is defined\\nas jointly extracting all aspect terms from image-\\ntext pairs and predicting their sentiment polarities\\n(Ju et al., 2021).\\nIn this scenario of fine-grained sentiment recog-\\nnition for multimodal information, the input image-\\ntext pairs are always complex. (1) The semantics\\nof sentence is complex which adds sentiment con-\\nfusion among different aspects. Take Figure 1 as an\\n∗Corresponding author.\\nComplain about Kyoto a bit\\nand someone takes you to\\nsee the mayor. Interesting!\\nMayor Kadokawa, thanks\\nfor your time!\\nregion 1\\nregion 2\\nAspect Kyoto mayor Mayor Kadokawa\\nSentiment Negative Neutral Positive\\nFigure 1: An example of MABSA task, including the as-\\npects, their corresponding descriptions, and sentiments.\\nexample, there are 3 aspects in the sentence with 3',\n",
              " 'Sentiment Negative Neutral Positive\\nFigure 1: An example of MABSA task, including the as-\\npects, their corresponding descriptions, and sentiments.\\nexample, there are 3 aspects in the sentence with 3\\ndifferent sentiments, The sentiment of “mayor\" can\\nbe easily confused by the keyword, “Interesting”,\\nwhich is of positive sentiment. (2) The images con-\\ntain a large amount of detailed information, and the\\nvisual contents are usually related to only one or\\nseveral of the aspects. For example, as shown in\\nFigure 1, the objects in red boxes are more helpful\\nin analyzing the sentiment of “Mayor Kadokawa”\\nthan the other aspects. The complex input greatly\\nchallenges the recognition of aspect-based senti-\\nment.\\nConsidering the multimodal input, existing meth-\\nods are typically dedicated to associated visual and\\ntextual contents (Ju et al., 2021; Ling et al., 2022;\\nYang et al., 2022). Ju et al. (2021) uses image-\\ntext relation to evaluate the contribution of visual',\n",
              " 'textual contents (Ju et al., 2021; Ling et al., 2022;\\nYang et al., 2022). Ju et al. (2021) uses image-\\ntext relation to evaluate the contribution of visual\\ncontents to aspect sentiment, based on which to de-\\ntermine whether the image is involved in sentiment\\nanalysis. Ling et al. (2022) and Yang et al. (2022)\\nalign visual representations of objects and their at-\\ntributes with corresponding textual contents. To\\nsummarize, the whole image is directly associated\\nwith textual content in these methods. Intuitively,\\nwithout aligning image blocks to corresponding as-\\npects, the coarse whole-image-text association can\\nintroduce aspect-irrelevant visual noise, which fur-\\nther hinders aspect sentiment analysis. In addition,\\nthe performance can be further impacted by the\\narXiv:2306.01004v1  [cs.CL]  31 May 2023',\n",
              " 'textual noise from the confusion among different\\naspects.\\nIn this paper, we propose an Aspect-oriented\\nMethod (AoM) to mitigate aforementioned noises\\nfrom both image and text. AoM can detect aspect-\\nrelevant information from perspectives of both se-\\nmantics and sentiment. There are two key modules\\nin AoM: Aspect-Aware Attention Module (A3M)\\nfor semantically fine-grained image-text alignment\\nand Aspect-Guided Graph Convolutional Network\\n(AG-GCN) for sentiment information aggregation.\\nIn A3M, we first extract aspect features associated\\nwith each visual and textual token. And then aspect-\\nrelevant token representations are computed based\\non their relevance to the corresponding aspect fea-\\ntures. In AG-GCN, we first explicitly add sentiment\\nembeddings to the obtained representations of vi-\\nsual and textual tokens. A multimodal weighted-\\nassociation matrix is constructed containing aspect-\\nto-image-block similarity and word-to-word depen-\\ndency. Then we use a graph convolutional network',\n",
              " 'sual and textual tokens. A multimodal weighted-\\nassociation matrix is constructed containing aspect-\\nto-image-block similarity and word-to-word depen-\\ndency. Then we use a graph convolutional network\\nto aggregate sentiment information according to\\nthe constructed multimodal matrix.\\nThe contributions can be summarized as follows:\\n(1) We propose an aspect-oriented network to\\nmitigate the visual and textual noises from the com-\\nplex image-text interactions.\\n(2) We design an aspect-aware attention mod-\\nule and an aspect-guided graph convolutional net-\\nwork to effectively detect aspect-relevant multi-\\nmodal contents from the perspectives of semantic\\nand sentiment, respectively.\\n(3) Experiments on two benchmark datasets, in-\\ncluding Twitter2015 and Twitter2017, show that\\nour approach generally outperforms the state-of-\\nthe-art methods.\\n2 Related Work\\nIn this section, we review the existing methods for\\nboth ABSA and MABSA.\\n2.1 Aspect-based Sentiment Analysis',\n",
              " 'our approach generally outperforms the state-of-\\nthe-art methods.\\n2 Related Work\\nIn this section, we review the existing methods for\\nboth ABSA and MABSA.\\n2.1 Aspect-based Sentiment Analysis\\nIn the past few years, Aspect-Based Sentiment\\nAnalysis (ABSA) in the textual fields has attracted\\nmuch attention and gained mature research (Chen\\nand Qian, 2020; Oh et al., 2021; Xu et al., 2020).\\nOn the one hand, most recent works are based on\\nthe pre-trained language model BERT because of\\nits remarkable performance in many NLP tasks\\n(Liang et al., 2022a). On the other hand, some\\nrecent efforts focus on modeling the dependency\\nrelationship between aspects and their correspond-\\ning descriptions, in which graph convolutional net-\\nworks (GCNs) (Chen et al., 2022; Liang et al.,\\n2022b, 2020; Li et al., 2021a; Pang et al., 2021)\\nor graph attention networks (GATs) (Yuan et al.,\\n2020) over dependency with the syntactic structure\\nof a sentence are fully exploited.\\n2.2 Multimodal Aspect-based Sentiment',\n",
              " 'or graph attention networks (GATs) (Yuan et al.,\\n2020) over dependency with the syntactic structure\\nof a sentence are fully exploited.\\n2.2 Multimodal Aspect-based Sentiment\\nAnalysis\\nWith the enrichment of multimodal users’ posts\\nin social media, researchers find that images of-\\nfer great supplementary information in aspect term\\nextraction (Wu et al., 2020a; Zhang et al., 2018;\\nAsgari-Chenaghlu et al., 2021) and sentiment anal-\\nysis (Wu et al., 2022; Zhang et al., 2022; Li\\net al., 2021b; Hazarika et al., 2020; Cai et al.,\\n2019). Thus, Multimodal Aspect-based Sentiment\\nAnalysis (MABSA) begins to be widely studied.\\nMABSA task can be divided into two independent\\nsub-tasks, i.e., Multimodal Aspect Term Extraction\\n(MATE) and Multimodal Aspect-oriented Senti-\\nment Classification (MASC). The former extracts\\nall aspect terms in the sentence at the prompt of the\\nimage, and the latter predicts the sentiment polari-\\nties for the aspects.\\nJu et al. (2021) first realizes MABSA in a unified',\n",
              " 'all aspect terms in the sentence at the prompt of the\\nimage, and the latter predicts the sentiment polari-\\nties for the aspects.\\nJu et al. (2021) first realizes MABSA in a unified\\nframework and designs an auxiliary cross-modal\\nrelation detection to control whether the visual in-\\nformation will be used in prediction. For captur-\\ning cross-modal alignment, Ling et al. (2022) con-\\nstructs a generative multimodal architecture based\\non BART for both vision-language pre-training and\\nthe downstream MABSA tasks. Yang et al. (2022)\\ndynamically controls the contributions of the vi-\\nsual information to different aspects via the trick\\nthat the lower confidence of the results predicted\\nby purely textual is, the more contributions from\\nimages will be considered.\\nOn the one hand, the above methods ignore the\\nalignment of fine-grained visual blocks and the\\ncorresponding aspects, which introduce irrelevant\\nvisual noise. On the other hand, modeling of syntax',\n",
              " 'On the one hand, the above methods ignore the\\nalignment of fine-grained visual blocks and the\\ncorresponding aspects, which introduce irrelevant\\nvisual noise. On the other hand, modeling of syntax\\ndependency and sentiment information for aspect\\ndescriptions is absent in these methods, which is\\nproved important in sentiment analysis (Liang et al.,\\n2022a; Kalaivani et al., 2022; Xu et al., 2022).\\nTo tackle the aforementioned issues, we pro-\\npose an aspect-oriented model consisting of Aspect-\\nAware Attention Module and Aspect-Guided Graph\\nConvolutional Network which respectively work',\n",
              " 'A\\n3\\nM\\nComplain about Kyoto ... time ! \\nSenticNet\\nlinear layer (1 to 768)\\ncosine  similarity\\nSenticNet\\n AG\\n-\\nGCN\\nGCN Layers\\nGCN Layers\\natomic feature\\nsoftmax linear layer\\nsigmoid linear layer\\nℎ𝑡𝑡\\nℎ1\\n𝐶𝐶𝐶𝐶\\nℎ𝑡𝑡\\n...\\nℎ𝑡𝑡 ℎ𝑡𝑡\\nℎ2\\n𝐶𝐶𝐶𝐶 ℎ𝑙𝑙\\n𝐶𝐶𝐶𝐶\\n𝛼𝛼𝑡𝑡1 𝛼𝛼𝑡𝑡2\\n𝛼𝛼𝑡𝑡𝑙𝑙\\nℎ𝑡𝑡\\n𝐶𝐶 ℎ𝑡𝑡\\n𝛽𝛽𝑡𝑡 1 −𝛽𝛽𝑡𝑡\\n�ℎ𝑡𝑡\\n... �ℎ𝑉𝑉𝑚𝑚 ...�ℎ𝑇𝑇𝑛𝑛\\n3 3         NEG       13          13        NEU       17          18        POS      <eos>\\nBART decoder\\n<bos>  <AESC> Kyoto Kyoto NEG mayor mayor NEU Mayor Kadokawa POS\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\nComplain\\nKyoto\\nmayor\\nMayor\\nKadokawa\\nthanks\\nabout\\nInteresting\\naspect-guided dependency, D\\nA\\n3\\nM\\nKyoto\\nMayor\\nKadokawa\\nKyoto\\nMayor\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑉𝑉1\\n�ℎ𝑉𝑉2\\n�ℎ𝑉𝑉3\\n�ℎ𝑉𝑉4\\n�ℎ𝑉𝑉5\\n�ℎ𝑉𝑉6\\n�ℎ𝑉𝑉𝑚𝑚\\nComplain Kyoto\\n�ℎ𝑇𝑇1\\nKyoto Kyoto\\n�ℎ𝑇𝑇3\\nmayor mayor\\n�ℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20',\n",
              " 'Mayor\\nKadokawa\\n�ℎ𝑉𝑉1\\n�ℎ𝑉𝑉2\\n�ℎ𝑉𝑉3\\n�ℎ𝑉𝑉4\\n�ℎ𝑉𝑉5\\n�ℎ𝑉𝑉6\\n�ℎ𝑉𝑉𝑚𝑚\\nComplain Kyoto\\n�ℎ𝑇𝑇1\\nKyoto Kyoto\\n�ℎ𝑇𝑇3\\nmayor mayor\\n�ℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20\\n... ...... ... ... ......\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos><img> </img>\\nBART encoder\\nResNet                                        BART embedding\\nmask\\n𝒘𝒘𝒊𝒊\\n...𝑤𝑤𝑆𝑆1 𝑤𝑤𝑆𝑆2 𝑤𝑤𝑆𝑆3 𝑤𝑤𝑆𝑆𝑛𝑛\\nAG\\n-\\nGCN\\n GCN Layers\\nGCN Layers\\nweighted association matrix, AS from \\nSenticNet\\nH\\nweighted association matrix, A\\n𝑺𝑺\\n�𝑯𝑯S�𝑯𝑯\\n�𝑯𝑯S\\n�𝑯𝑯\\n�𝑯𝑯\\nHS\\nFigure 2: The overview of our proposed aspect-oriented model AoM.\\nto capture semantic information by fine-grained\\nimage-text alignment and effectively aggregate\\naspect-relevant sentiment information.\\n3 Methodology\\n3.1 Overview\\nTask Definition. Formally, given a tweet that\\ncontains an image V and a sentence with n\\nwords S = (w1, w2, ..., wn), our goal is to ac-',\n",
              " '3 Methodology\\n3.1 Overview\\nTask Definition. Formally, given a tweet that\\ncontains an image V and a sentence with n\\nwords S = (w1, w2, ..., wn), our goal is to ac-\\nquire the sequence Y representing all aspects\\nand their associated sentiment polarities. We\\nformulate the output of MABSA as Y =\\n[as\\n1, ae\\n1, s1, ..., as\\ni , ae\\ni , si, ...as\\nk, ae\\nk, sk], where as\\ni , ae\\ni\\nand si depict the start index, end index of the i-th\\naspect and its sentiment polarity in the tweet, and\\nk is the number of aspects.\\nModel preview. Figure 2 shows the overview\\nof our model architecture, which builds on an\\nencoder-decoder architecture based on BART\\n(Lewis et al., 2019). Between the encoder and\\nthe decoder of BART, we creatively implement\\nthe Aspect-Aware Attention Module (A 3M) and\\nAspect-Guided Graph Convolutional Network (AG-\\nGCN) to align the textual aspect to its associated\\nvisual blocks and textual description, simultane-\\nously mitigate interference both from semantics',\n",
              " 'Aspect-Guided Graph Convolutional Network (AG-\\nGCN) to align the textual aspect to its associated\\nvisual blocks and textual description, simultane-\\nously mitigate interference both from semantics\\nand sentiment among different aspects. In the fol-\\nlowing subsections, we will illustrate the details of\\nthe proposed model.\\nFeature Extractor. The initial word embeddings\\nare obtained from the pre-trained BART due to\\nits excellent ability of textual representation. The\\nembeddings of visual blocks are obtained by pre-\\nprocessing via ResNet (Chen et al., 2014) following\\n(Yu et al., 2019). We consider every feature of a\\nvisual block or word token as an atomic feature. We\\nadd <img> and </img> before and after the visual\\nfeatures, <bos> and <eos> for the textual features.\\nThen, we concatenate the multimodal features as\\nX which is the input of BART encoder.\\nWe can get the multimodal hidden state H =\\n{hV\\n0 , ...hV\\ni , ...hV\\nm, hT\\n0 , ..., hT\\nj , ...hT\\nn } with m visual\\nblocks and n words, where hV',\n",
              " 'X which is the input of BART encoder.\\nWe can get the multimodal hidden state H =\\n{hV\\n0 , ...hV\\ni , ...hV\\nm, hT\\n0 , ..., hT\\nj , ...hT\\nn } with m visual\\nblocks and n words, where hV\\ni and hT\\nj refer to\\nfeatures of the i-th visual block and the j-th word\\nin the sentence.\\n3.2 Aspect-Aware Attention Module (A 3M)\\nSince aspects are not specially modeled by BART\\nencoder, we creatively design the Aspect-Aware\\nAttention Module (A3M) aiming to capture aspect-\\nrelevant semantic information. For this purpose, we\\nalign the multimodal information of target objects\\nand filter out the semantic noise from images.\\nFirst, as aspects are usually noun phrases from\\nthe sentences, we extract those phrases as the',\n",
              " 'candidate aspects (CA) with the NLP tool Spacy1.\\nAnd from the hidden state H of the BART encoder,\\nwe obtain the features of all candidate aspects de-\\nnoted as HCA = {hCA\\n1 , ..., hCA\\ni , ..., hCA\\nl }, where\\nl is the number of noun phrases in the sentence. To\\nget the relationship between candidate aspects and\\natomic features, we implement an attention-based\\nmechanism guided by the candidate aspects. Given\\nthe t-th hidden feature ht, its attention distribution\\nαt over k candidate aspects is obtained by:\\nZt =tanh((WCAHCA+bCA)⊕(WHht +bH)), (1)\\nαt = softmax(WαZt + bα), (2)\\nwhere Zt ∈ R2d×k is the comprehensive feature\\nextracted from both the candidate aspects and the\\nhidden states. HCA ∈ Rd×k denotes the features\\nof candidate aspects. WCA ∈ Rd×d, WH ∈ Rd×d,\\nWα ∈ R1×2d, bCA, bH and bα are the learned\\nparameters.⊕ is an operator between a matrix and a\\nvector, where the vector is repeated into the appro-\\npriate size to concatenate with the matrix. We then',\n",
              " 'Wα ∈ R1×2d, bCA, bH and bα are the learned\\nparameters.⊕ is an operator between a matrix and a\\nvector, where the vector is repeated into the appro-\\npriate size to concatenate with the matrix. We then\\nget the aspect-related hidden feature hA\\nt by calcu-\\nlating the weighted sum of all candidate aspects\\nfollowing the below equation:\\nhA\\nt =\\nkX\\ni\\nαt,ihCA\\ni . (3)\\nFor example, if a visual block is strongly associ-\\nated with the j-th aspect, the corresponding αt,j is\\napproximately 1. hA\\nt would be equal to the aspect\\nsemantically. And if the visual block is not related\\nto any specific candidate aspects, both αt and hA\\nt\\nwould be zero-like vectors of no information.\\nConsidering that not every visual block can be\\nused for prediction, βt is learned to add up the\\natomic feature ht and its aspect-related hidden fea-\\nture hA\\nt . Details are as follows:\\nβt = sigmoid(Wβ[W1ht; W2hA\\nt ] +bβ), (4)\\nˆht = βtht + (1− βt)hA\\nt , (5)\\nwhere Wβ, W1, W2, bβ are parameters, and [; ]',\n",
              " 'ture hA\\nt . Details are as follows:\\nβt = sigmoid(Wβ[W1ht; W2hA\\nt ] +bβ), (4)\\nˆht = βtht + (1− βt)hA\\nt , (5)\\nwhere Wβ, W1, W2, bβ are parameters, and [; ]\\ndenotes the concatenation operator for vectors.\\nˆht ∈ ˆH is the final output of A3M after the seman-\\ntic alignment and the noise reduction procedure.\\nThus we get the noiseless and aligned information\\nfor every atomic feature.\\nPre-training To align the two modalities and re-\\nduce noise, we conduct a pre-training task in A3M.\\n1Spacy: https://spacy.io/\\naverage\\nrelated or unrelated\\nH\\nimage-text relation classification layer\\nA\\n3\\nM\\nKyoto\\nMayor\\nKadokawa\\nKyoto\\nMayor\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑉𝑉1\\n�ℎ𝑉𝑉2\\n�ℎ𝑉𝑉3\\n�ℎ𝑉𝑉4\\n�ℎ𝑉𝑉5\\n�ℎ𝑉𝑉6\\n�ℎ𝑉𝑉𝑉𝑉\\nComplain Kyoto\\n�ℎ𝑇𝑇1\\nKyoto Kyoto\\n�ℎ𝑇𝑇3\\nmayor mayor\\n�ℎ𝑇𝑇13\\nMayor\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇17\\nKadokawa\\nMayor\\nKadokawa\\n�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20\\n... ...... ... ... ......\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos>\\n<img> </img>',\n",
              " '�ℎ𝑇𝑇18\\nthanks thanks\\n�ℎ𝑇𝑇20\\n... ...... ... ... ......\\n<bos> Complain about Kyoto a bit and someone takes you to see the \\nmayor. Interesting! Mayor Kadokawa, thanks for your time ! <eos>\\n<img> </img>\\nBART encoder\\nResNet                                        BART embedding\\nFigure 3: The framework of the pre-training task.\\nSpecifically, we detect the image-text relationship\\non the datasets TRC (Vempala and Preo¸ tiuc-Pietro,\\n2019) as illustrated by Figure 3. We first obtain the\\naverage feature of image blocks from the output of\\nA3M and then pass it to a fully connected softmax\\nlayer, which outputs a probability distribution over\\nwhether the image is related to the text. Finally, we\\nuse cross entropy loss to train our model.\\n3.3 Aspect-Guided Graph Convolutional\\nNetwork (AG-GCN)\\nThe aspect-focused interaction between visual\\nmodality and textual modality in A3M concentrates\\non the context semantics, and that is not adequate\\nfor MABSA. Sentiment interference among dif-',\n",
              " 'The aspect-focused interaction between visual\\nmodality and textual modality in A3M concentrates\\non the context semantics, and that is not adequate\\nfor MABSA. Sentiment interference among dif-\\nferent aspects still exists and influences sentiment\\nprediction. Thus, we design the Aspect-Guided\\nGraph Convolutional Network (AG-GCN) module\\nto introduce external sentiment information and\\nmitigate emotional confusion among different as-\\npects to a certain extent.\\nSpecifically, for wordwi in the sentence, we gain\\nits affective score wS\\ni from SenticNet (Ma et al.,\\n2018) and project it to the space with the same\\ndimension as hA\\nt , with si obtained. Then we add\\nthe sentiment feature si to the output of A3M:\\nwS\\ni = SenticNet (wi), (6)\\nsi = WSwS\\ni + bS, (7)\\nhS\\ni = ˆhi + si, (8)\\nwhere WS, bS are the learned parameters. hS\\ni is the\\nfeature with affective knowledge.\\nNext, we build a boolean dependency matrix\\nD among visual blocks and words. First, for the\\nword-to-word part, submatrix DT Trepresenting',\n",
              " 'Complain\\nabout takes\\nKyoto\\nbit\\na someone you . to the\\nmayor\\nsee\\nand\\nInteresting\\n!\\nthanks\\n, !\\nKadokawa\\nMayor\\nfor\\ntime\\nyour\\ndependency tree\\nFigure 4: The dependency tree of the example men-\\ntioned in the introduction.\\nthe dependency tree2 of the input sentence like Fig-\\nure 4. If two words can be associated within two\\ngenerations, the element of DT Twould be set to\\n1, otherwise 0 instead. For example, “Kyoto” is as-\\nsociated with “bit” (child),“a” (grandchild),“about”\\n(father) and “Complain” (grandfather). Second, the\\nvisual dependency submatrix DV V is initialized\\nas a diagonal matrix. And as for the word-image-\\nblock dependency, denoted as DT V and equaled\\nto DT\\nV T, we set all the elements in the i-th line of\\nDT Vto 1 if the i-th word is an aspect, otherwise 0.\\nAnd the matrix D is defined as:\\nD =\\n\\x14DV V DV T\\nDT V DT T\\n\\x15\\n, (9)\\nConsidering the different importance of differ-\\nent dependencies, we attach weights onto D with\\ncosine similarity among ˆhi as follows:',\n",
              " 'D =\\n\\x14DV V DV T\\nDT V DT T\\n\\x15\\n, (9)\\nConsidering the different importance of differ-\\nent dependencies, we attach weights onto D with\\ncosine similarity among ˆhi as follows:\\nAij = DijFcosine_similarity( ˆhi, ˆhj), (10)\\nwhere both D, A∈ R(m+n)×(m+n), and A is the\\nweighted association matrix.\\nAG-GCN takes HS from Eq.8 as initial node\\nrepresentations in the graph. For the i-th node at\\nthe l-th layer, the hidden state hS\\ni,l is updated by the\\nfollowing equation:\\nhS\\ni,l = ReLU(\\nnX\\nj=1\\nAijWlhS\\ni,l−1 + bl), (11)\\nwhere Wl,bl are learned parameters and we use\\nReLU as activation function. Significantly, hS\\ni,0\\nis equal to hS\\ni . Accordingly, we get the final out-\\nput ˆHS from the last GCN layer which is rich in\\nsentiment information. Every underlying aspect\\naggregates its relevant information from both the\\nimage-text pair. Moreover, sentiment confusion\\nof different aspects is weakened because the as-\\nsociation matrix makes little interference among\\ndifferent aspects.',\n",
              " 'image-text pair. Moreover, sentiment confusion\\nof different aspects is weakened because the as-\\nsociation matrix makes little interference among\\ndifferent aspects.\\n2We use spaCy toolkit to construct the dependency tree\\nreferring from https://spacy.io\\nTwitter2015 Twitter2017\\n#sentence 3,502 2,910\\n#with one aspect 2,159 (61.65%) 976 (33.54%)\\n#with multiple aspects 1,343 (38.35%) 1,934 (66.46%)\\n#with multiple sentiments 1,257 1,690\\nTable 1: Statistics of the two benchmark datasets. Line 1\\nis the number of sentences. #X in the last 3 lines denotes\\nthe number of sentences with such characteristics X.\\n3.4 Prediction and Loss Function\\nThe BART decoder takes the combination of ˆH,\\nˆHS, and the previous decoder output Y<t as inputs,\\nand predicts the token probability distribution as\\nfollows:\\n˜H = λ1 ˆH + λ2 ˆHS, (12)\\nhd\\nt = Decoder( ˜H; Y<t) (13)\\nHT = (W + ˜HT )/2 (14)\\nP(yt) =softmax([HT ; Cd]hd\\nt ) (15)\\nwhere λ1, λ2 are the hyper-parameters to control',\n",
              " 'follows:\\n˜H = λ1 ˆH + λ2 ˆHS, (12)\\nhd\\nt = Decoder( ˜H; Y<t) (13)\\nHT = (W + ˜HT )/2 (14)\\nP(yt) =softmax([HT ; Cd]hd\\nt ) (15)\\nwhere λ1, λ2 are the hyper-parameters to control\\nthe contribution from the two modules. ˜HT is the\\ntextual part of ˜H. W denotes the embeddings of\\ninput tokens. Cd means the embeddings of the [\\npositive, neutral, negative, <eos>]. The loss func-\\ntion is as follows:\\nL = −EX∼D\\nOX\\nt=1\\nlogP (yt|Y<t, X), (16)\\nwhere O = 2M + 2N + 2is the length of Y , and\\nX denotes the multimodal input.\\n4 Experiment\\n4.1 Experimental settings\\nDatasets. Our two benchmark datasets are Twit-\\nter2015 and Twitter2017 (Yu and Jiang (2019)). As\\nshown in the statistics of Table 1, sentences with\\nmultiple aspects take up a considerable part of the\\ntwo datasets.\\nImplementation Details. Our model is based on\\nBART (Lewis et al., 2019), and the pre-training\\ntask is trained for 40 epochs with batch size 64,\\nand for 35 epochs with batch size 16 on MABSA.',\n",
              " 'Implementation Details. Our model is based on\\nBART (Lewis et al., 2019), and the pre-training\\ntask is trained for 40 epochs with batch size 64,\\nand for 35 epochs with batch size 16 on MABSA.\\nThe learning rates are both 7e-5 and hidden sizes\\nare 768. Hyper-parameters λ1 and λ2 are 1 and 0.5\\nrespectively. Besides, we pre-train A3M on TRC\\ndataset (Vempala and Preo¸ tiuc-Pietro, 2019), which\\nis divided into two groups according to whether the\\ntext is represented.',\n",
              " 'Twitter2015 Twitter2017\\nMethods P R F1 P R F1\\nText-based\\nSPAN* (Hu et al., 2019) 53.7 53.9 53.8 59.6 61.7 60.6\\nD-GCN* (Chen et al., 2020) 58.3 58.8 59.4 64.2 64.1 64.1\\nBART* (Yan et al., 2021) 62.9 65.0 63.9 65.2 65.6 65.4\\nMultimodal\\nUMT+TomBERT* (Yu et al., 2020; Yu and Jiang, 2019) 58.4 61.3 59.8 62.3 62.4 62.4\\nOSCGA+TomBERT* (Wu et al., 2020c; Yu and Jiang, 2019) 61.7 63.4 62.5 63.4 64.0 63.7\\nOSCGA-collapse* (Wu et al., 2020c) 63.1 63.7 63.2 63.5 63.5 63.5\\nRpBERT-collapse* (Sun et al., 2021) 49.3 46.9 48.0 57.0 55.4 56.2\\nUMT-collapse (Yu et al., 2020) 61.0 60.4 61.6 60.8 60.0 61.7\\nJML (Ju et al., 2021) 65.0 63.2 64.1 66.5 65.5 66.0\\nVLP-MABSA* (Ling et al., 2022) 65.1 68.3 66.6 66.9 69.2 68.0\\nCMMT (Yang et al., 2022) 64.6 68.7 66.5 67.6 69.4 68.5\\nAoM (ours) 67.9 69.3 68.6 68.4 71.0 69.7\\nTable 2: Results of different methods for MABSA on the two Twitter datasets. * denotes the results from Ling et al.\\n(2022). The best results are bold-typed and the second best ones are underlined.',\n",
              " 'Table 2: Results of different methods for MABSA on the two Twitter datasets. * denotes the results from Ling et al.\\n(2022). The best results are bold-typed and the second best ones are underlined.\\nEvaluation Metrics. We evaluate the performance\\nof our model on MABSA task and MATE task by\\nMicro-F1 score (F1), Precision (P) and Recall (R),\\nwhile on MASC task we use Accuracy (Acc) and\\nF1 following previous studies.\\n4.2 Baselines\\nWe compare our proposed model with four types\\nof methods listed below.\\nApproaches for textual ABSA. 1) SPAN (Hu\\net al., 2019) detects opinion targets with their senti-\\nments. 2) D-GCN (Chen et al., 2020) models de-\\npendency relations among words via dependency\\ntree. 3) BART (Yan et al., 2021) solves seven\\nABSA subtasks in an end-to-end framework.\\nApproaches for MATE. 1) RAN (Wu et al.,\\n2020b) focus on alignment of text and object re-\\ngions. 2) UMT (Yu et al., 2020) takes text-based\\nentity span detection as an auxiliary task. 3) OS-',\n",
              " 'Approaches for MATE. 1) RAN (Wu et al.,\\n2020b) focus on alignment of text and object re-\\ngions. 2) UMT (Yu et al., 2020) takes text-based\\nentity span detection as an auxiliary task. 3) OS-\\nCGA (Wu et al., 2020c) foucus on alignments of\\nvisual objects and entities.\\nApproaches for MASC. 1) ESAFN (Yu et al.,\\n2019) is an entity-level sentiment analysis method\\nbased on LSTM. 2) TomBERT (Yu and Jiang,\\n2019) applies BERT to obtain aspect-sensitive tex-\\ntual representations. 3) CapTrBERT (Khan and\\nFu, 2021) translates images into text and construct\\nan auxiliary sentence for fusion.\\nApproaches for MABSA. 1) UMT-collapse\\n(Yu et al., 2020), OSCGA-collapse (Wu et al.,\\n2020c) and RpBERT-collapse (Sun et al., 2021)\\nare adapted from models for MATE by using col-\\nlapsed labels to represent aspect and sentiment\\npairs. 2) UMT+TomBERT, OSCGA+TomBERT\\nare two pipeline approaches by combining UMT\\n(Yu et al., 2020) or OSCGA (Wu et al., 2020c) with\\nTomBERT (Yu and Jiang, 2019). 3)JML (Ju et al.,',\n",
              " 'pairs. 2) UMT+TomBERT, OSCGA+TomBERT\\nare two pipeline approaches by combining UMT\\n(Yu et al., 2020) or OSCGA (Wu et al., 2020c) with\\nTomBERT (Yu and Jiang, 2019). 3)JML (Ju et al.,\\n2021) is the first joint model for MABSA with aux-\\niliary cross-modal relation detection module. 4)\\nCMMT (Yang et al., 2022) implements a gate to\\ncontrol the multimodal information contributions\\nduring inter-modal interactions. 5) VLP-MABSA\\n(Ling et al., 2022) performs five task-specific pre-\\ntraining tasks to model aspects, opinions and align-\\nments.\\n4.3 Main Results\\nIn this section, we show the excellent performance\\nof AoM on the two datasets for the three tasks\\ncompared with SOTAs.\\nPerformance on MABSA: The results for\\nMABSA are shown in Table 2. First, our AoM\\nfar exceeds all text-based models, which means\\ndetection of richer visual information and textual\\ninformation in our model is helpful. Second, multi-\\nmodal pipeline methods and adaptive methods are\\ngenerally unsatisfactory, because they ignore the',\n",
              " 'information in our model is helpful. Second, multi-\\nmodal pipeline methods and adaptive methods are\\ngenerally unsatisfactory, because they ignore the\\ninteraction between the semantic information and\\nsentiment for the two sub-tasks. Last, AoM out-\\nperforms all multimodal methods in every metric.\\nEspecially, AoM achieves the improvement of 2%\\nand 1.2% with respect to F1 in contrast with the sec-\\nond best models on two datasets (VLP-MABSA for\\nTwitter2015 and CMMT for Twitter2017), which\\ndemonstrates the effectiveness of learning aspect-\\nrelevant visual blocks and textual words compared\\nto focusing on all visual and textual inputs.\\nPerformance on MATE:As shown in Table 3,\\nAoM is ahead of most of the current models and\\nperforms the best in Twitter 2015 by 0.3% higher\\nthan the second best CMMT on F1. The perfor-\\nmance of CMMT in Twitter2017 is 0.8% higher',\n",
              " 'Twitter2015 Twitter2017\\nMethods P R F1 P R F1\\nRAN* 80.5 81.5 81.0 90.7 90.7 90.0\\nUMT* 77.8 81.7 79.7 86.7 86.8 86.7\\nOSCGA* 81.7 82.1 81.9 90.2 90.7 90.4\\nJML* 83.6 81.2 82.4 92.0 90.7 91.4\\nVLP-MABSA* 83.6 87.985.7 90.8 92.6 91.7\\nCMMT 83.9 88.1 85.9 92.2 93.9 93.1\\nAoM (ours) 84.6 87.9 86.2 91.8 92.8 92.3\\nTable 3: Results of different methods for MATE. * de-\\nnotes the results from Ling et al. (2022).\\nTwitter2015 Twitter2017\\nMethods ACC F1 ACC F1\\nESAFN 73.4 67.4 67.8 64.2\\nTomBERT 77.2 71.8 70.5 68.0\\nCapTrBERT 78.0 73.2 72.3 70.2\\nJML 78.7 - 72.7 -\\nVLP-MABSA 78.6 73.873.8 71.8\\nCMMT 77.9 - 73.8 -\\nAoM (ours)80.2 75.9 76.4 75.0\\nTable 4: Results of different methods for MASC.\\nthan ours, probably due to our model wrongly pre-\\ndicting some noun phrases as aspects. But consid-\\nering the improvement in MASC and MABSA, it\\nis still worthy treating all noun phrases in the sen-\\ntence as candidate aspects when acquiring aspect-\\nrelevant visual information.\\nPerformance on MASC: Table 4 shows the per-',\n",
              " 'is still worthy treating all noun phrases in the sen-\\ntence as candidate aspects when acquiring aspect-\\nrelevant visual information.\\nPerformance on MASC: Table 4 shows the per-\\nformance of MASC. It is exciting that our model\\noutperforms the second-best results by 1.5% and\\n2.6% in accuracy, 2.1% and 3.2% points in F1 score\\non Twitter2015 and Twitter2017. It demonstrates\\nthat AoM has the ability to detect aspect-related\\nsentiment information from both images and text,\\neven disturbed by other noisy aspects.\\n4.4 Ablation Study\\nIn this section, we research the effectiveness of\\neach component in AoM, the results are shown in\\nTable 5.\\nW/o A3M&AG-GCN shows that after remov-\\ning the two specially designed modules, the per-\\nTwitter2015 Twitter2017\\nMethods P R F1 P R F1\\nFull 67.9 69.3 68.6 68.4 71.0 69.7\\nw/o A3M&AG-GCN 65.7 67.3 66.5 66.5 69.0 67.8\\nw/o A3M&TRC 62.1 61.0 61.6 63.7 64.1 63.9\\nw/o TRC 66.8 68.4 67.6 67.8 69.8 68.8\\nw/o AG-GCN 67.0 69.4 68.2 67.8 69.7 68.8',\n",
              " 'w/o A3M&AG-GCN 65.7 67.3 66.5 66.5 69.0 67.8\\nw/o A3M&TRC 62.1 61.0 61.6 63.7 64.1 63.9\\nw/o TRC 66.8 68.4 67.6 67.8 69.8 68.8\\nw/o AG-GCN 67.0 69.4 68.2 67.8 69.7 68.8\\nw/o SenticNet 65.7 70.5 68.0 68.1 69.4 68.7\\nw/o TRC&AG-GCN 66.7 69.2 68.0 67.8 69.5 68.6\\nTable 5: The performance comparison of our full model\\nand its ablated approaches.\\nImage\\nText\\nVLP-MABSA\\nBART+A3M\\nAoM\\n(a) NBA Western Conference Finals: \\nGolden State Warriors shock \\nOklahoma City Thunder,…\\n( NBA, NEU ) (√, √)\\n---\\n( Oklahoma, NEU ) (×, ×)\\n( NBA, NEU ) (√, √)\\n( Golden State Warriors , POS ) (√, √)\\n( Oklahoma City Thunder, NEG ) (√, √)\\n( NBA, NEU ) (√, √)\\n( Golden State Warriors , POS ) (√, √)\\n( Oklahoma City Thunder, NEG ) (√, √)\\n(b) This subtle difference between \\nDaniel Radcliffe and Elijah Wood\\nis pretty unsettling.\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah, NEU ) (×, ×)\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah Wood, NEG ) (√, √)\\n( Daniel Radcliffe, NEG ) (√, √)\\n( Elijah Wood, NEG ) (√, √)',\n",
              " 'is pretty unsettling.\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah, NEU ) (×, ×)\\n( Daniel Radcliffe, NEU ) (√, ×)\\n( Elijah Wood, NEG ) (√, √)\\n( Daniel Radcliffe, NEG ) (√, √)\\n( Elijah Wood, NEG ) (√, √)\\nFigure 5: Two cases with predictions by VLP-MABSA\\n(Ling et al., 2022), BART+A3M, and our model.\\nformance declines by 2.1% on Twitter2015 and\\n1.9% on Twitter2017. It fully demonstrates their\\ncontributions to learning effective information.\\nW/o A3M&TRC performs worse after remov-\\ning A 3M including the pre-training on TRC. It\\nproves the necessity of modeling semantic align-\\nment between visual blocks and aspects in A 3M.\\nWith the alignment, AG-GCN can obtain appropri-\\nate aspect-image-block and text-text association.\\nW/o TRC pre-training shows a slight drop after\\nwe remove the TRC pre-training on A3M, which\\nimplies relevant pre-training task is useful for the\\nmodel to learn better parameters.\\nW/o AG-GCN displays the performance with-\\nout AG-GCN, declining by 0.42% on Twitter2015',\n",
              " 'implies relevant pre-training task is useful for the\\nmodel to learn better parameters.\\nW/o AG-GCN displays the performance with-\\nout AG-GCN, declining by 0.42% on Twitter2015\\nand 0.9% on Twitter2017. It means that AG-GCN\\ndoes make the prediction focus on specific aspects\\nrelated to blocks and words with syntax dependen-\\ncies. In other words, the multimodal interference\\nfrom other aspects can be mitigated.\\nW/o SenticNet is the model without sentiment\\ninformation in AG-GCN. Its performance shows\\nadding external affective knowledge can enhance\\nthe sentiment comprehension of the model.\\nW/o TRC&AG-GCN is the BART model only\\nwith our A3M module. We can see from Table 5\\nthat w/o TRC&AG-GCNimproves w/o A3M&AG-\\nGCN by 1.5% and 0.8%. So it is effective to align\\nthe fine-grained visual block to related aspect and\\nreduce irrelevant information.\\n4.5 Case Study\\nTo better analyze how the Aspect-Aware Attention\\nModule and Aspect-Guided Graph Convolutional',\n",
              " 'the fine-grained visual block to related aspect and\\nreduce irrelevant information.\\n4.5 Case Study\\nTo better analyze how the Aspect-Aware Attention\\nModule and Aspect-Guided Graph Convolutional\\nNetwork work, we present the case study as fol-\\nlows.\\nFigure 5 displays two examples with predic-\\ntions from VLP-MABSA (Ling et al., 2022),',\n",
              " 'A3M\\nKyoto\\nbit\\nsomeone\\nmayor\\nMayor Kadokawa\\nthanks\\ntime\\n(I.a) attention of CAs\\n(I.b) learned visual attention\\nComplain about\\nKyoto a bit and\\nsomeone takes you\\nto see the mayor.\\nInteresting! Mayor\\nKadokawa, thanks\\nfor your time!\\nInputs\\nAspect-Sentiment Pairs\\n<Kyoto, NEG>\\n<mayor, NEU>\\n<Mayor Kadokawa, POS>\\nThe image-related probability \\ndistribution of candidate aspects.\\nAG-GCN\\n(II.a) word-to-word association matrix (II.b) word-to-visual-block association matrix\\n(II.c) aspect-relevant visual attention map\\nMayor Kadokawa\\nMayor Kadokawa thanks\\nKyoto Mayor Kadokawa\\n(II.d) information relevant to “Mayor Kadokawa”\\n0.8 0.7 0.6 0.1 0.00.20.30.40.5\\nFigure 6: Visualization of the attention maps in A3M and the sub-parts of the weighted-association matrix AG-GCN.\\nBART+A3M and our AoM. In example (a), VLP-\\nMABSA misses the aspect “Golden State War-\\nriors”, gets an incomplete aspect “Oklahoma City\\nThunder” and wrongly predicts the sentiment. It\\nmay be caused by the interference from the vi-',\n",
              " 'MABSA misses the aspect “Golden State War-\\nriors”, gets an incomplete aspect “Oklahoma City\\nThunder” and wrongly predicts the sentiment. It\\nmay be caused by the interference from the vi-\\nsual region which represents pride expression of a\\nperson. However, BART+A3M gets all right pre-\\ndictions due to the ability of aspect-oriented atten-\\ntion. In example (b), compared with our whole\\nmodel, BART+A3M wrongly predicts the senti-\\nment of “Daniel Radcliffe” which should be nega-\\ntive. We attribute the wrong prediction to lacking\\nsyntax association which benefits sentiment trans-\\nmission. In other words, AG-GCN contributes to\\nthe correctness.\\n4.6 Attention Visualization\\nTo investigate the effectiveness of detecting aspect-\\nrelevant information, we visualize the attention pro-\\ncess as shown in Figure 6.\\nFor A3M: (i) Figure 6-(I.a) shows the attention\\nweights of candidate aspects computed according\\nto the images. We can see that “Mayor Kadokawa”\\nis the most relevant aspect. (ii) Figure 6-(I.b)',\n",
              " 'For A3M: (i) Figure 6-(I.a) shows the attention\\nweights of candidate aspects computed according\\nto the images. We can see that “Mayor Kadokawa”\\nis the most relevant aspect. (ii) Figure 6-(I.b)\\nshows the proportions of the visual information re-\\nserved at the last step in A3M, where we weighted\\nadd up the representations of visual blocks and\\nthe corresponding aspects. The heat map shows\\nthat the visual information associated with “Mayor\\nKadokawa” is reserved to a great extent, while the\\nhelpless information from other blocks is disre-\\ngarded as noise. It demonstrates that attention in\\nA3M is able to detect aspect-relevant information.\\nFor AG-GCN: (i) Figure 6-(II.a) shows the\\nword-to-word part of the weighted association ma-\\ntrix. The matrix effectively excludes sentiment\\ninterference from other aspects by adding syntax\\ndependency information. For example, the senti-\\nment of “mayor” cannot be influenced by irrelevant\\nkeywords, such as “Complain” and “thanks”. (ii)',\n",
              " 'interference from other aspects by adding syntax\\ndependency information. For example, the senti-\\nment of “mayor” cannot be influenced by irrelevant\\nkeywords, such as “Complain” and “thanks”. (ii)\\nFigure 6-(II.b) shows the dependencies between\\nvisual blocks and words. (iii) Specifically, we vi-\\nsualize the visual attention of aspects “Kyoto” (see\\nFigure 6-(II.c) left) and “Mayor Kadokawa” (see\\nFigure 6-(II.c) right). We can see that “Kyoto” pays\\nmore attention to the pictures hanging on the wall\\nwhich are full of Japanese elements related to the\\nplace, while “Mayor Kadokawa” focus more on the\\njoyful expressions of the two people. (iv) Figure\\n6-(II.d) shows the words and image blocks “Mayor\\nKadokawa” focused on in sentiment transmission.\\nIt’s obvious that these attentions are helpful for the\\nprediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially',\n",
              " 'prediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,',\n",
              " 'we explicitly add sentiment embedding into AoM.\\nThen, a graph convolutional network is used to ag-\\ngregate the semantic and sentiment embedding un-\\nder the guidance of both image-text similarity and\\nsyntax dependency in sentences. The experimental\\nresults on two widely used datasets demonstrate\\nthe effectiveness of our method.\\nLimitations\\nThough our proposed method outperforms cur-\\nrent state-of-the-art methods, there are still many\\nchallenges we should overcome in future research.\\nFirst, for colloquial expression which confuses cur-\\nrent dependency tree parser, we should come up\\nwith new solutions. Second, emotional prediction\\nof tweet posts describing current issues needs exter-\\nnal knowledge, which is absent in existing research.\\nAcknowledgments\\nWe thank anonymous reviewers for their valu-\\nable comments. This work was supported by\\nthe Natural Science Foundation of Tianjin,\\nChina (No.22JCJQJC00150, 22JCQNJC01580),\\nthe National Natural Science Foundation of',\n",
              " 'able comments. This work was supported by\\nthe Natural Science Foundation of Tianjin,\\nChina (No.22JCJQJC00150, 22JCQNJC01580),\\nthe National Natural Science Foundation of\\nChina (No.62272250), Tianjin Research In-\\nnovation Project for Postgraduate Students\\n(No.2022SKYZ232), and the Fundamental\\nResearch Funds for the Central Universities (No.\\n63231149).\\nReferences\\nMeysam Asgari-Chenaghlu, M. Reza Feizi-Derakhshi,\\nLeili Farzinvash, M. A. Balafar, and Cina Motamed.\\n2021. CWI: A multimodal deep learning approach\\nfor named entity recognition from social media us-\\ning character, word and image features. Neural\\nComputing and Applications, 34(3):1905–1922.\\nYitao Cai, Huiyu Cai, and Xiaojun Wan. 2019.\\nMulti-Modal Sarcasm Detection in Twitter with\\nHierarchical Fusion Model. In Proceedings of\\nthe 57th Annual Meeting of the Association for\\nComputational Linguistics, pages 2506–2515, Flo-\\nrence, Italy. Association for Computational Linguis-\\ntics.\\nGuimin Chen, Yuanhe Tian, and Yan Song. 2020.',\n",
              " 'Computational Linguistics, pages 2506–2515, Flo-\\nrence, Italy. Association for Computational Linguis-\\ntics.\\nGuimin Chen, Yuanhe Tian, and Yan Song. 2020.\\nJoint aspect extraction and sentiment analysis with\\ndirectional graph convolutional networks. In\\nProceedings of the 28th international conference on\\ncomputational linguistics, pages 272–279.\\nHao Chen, Zepeng Zhai, Fangxiang Feng, Ruifan\\nLi, and Xiaojie Wang. 2022. Enhanced Multi-\\nChannel Graph Convolutional Network for\\nAspect Sentiment Triplet Extraction. In\\nProceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics (V olume\\n1: Long Papers), pages 2974–2985, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nTao Chen, Damian Borth, Trevor Darrell, and Shih-\\nFu Chang. 2014. Deepsentibank: Visual sentiment\\nconcept classification with deep convolutional neural\\nnetworks.\\nZhuang Chen and Tieyun Qian. 2020. Relation-\\nAware Collaborative Learning for Unified Aspect-',\n",
              " 'concept classification with deep convolutional neural\\nnetworks.\\nZhuang Chen and Tieyun Qian. 2020. Relation-\\nAware Collaborative Learning for Unified Aspect-\\nBased Sentiment Analysis. In Proceedings of\\nthe 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 3685–3694, On-\\nline. Association for Computational Linguistics.\\nDevamanyu Hazarika, Roger Zimmermann, and Sou-\\njanya Poria. 2020. Misa: Modality-invariant\\nand -specific representations for multimodal senti-\\nment analysis. In Proceedings of the 28th ACM\\nInternational Conference on Multimedia, MM ’20,\\npage 1122–1131, New York, NY , USA. Association\\nfor Computing Machinery.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Dongsheng\\nLi, and Yiwei Lv. 2019. Open-domain targeted senti-\\nment analysis via span-based extraction and classifi-\\ncation. arXiv preprint arXiv:1906.03820.\\nXincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\\nShoushan Li, Min Zhang, and Guodong Zhou.\\n2021. Joint Multi-modal Aspect-Sentiment Anal-',\n",
              " 'cation. arXiv preprint arXiv:1906.03820.\\nXincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\\nShoushan Li, Min Zhang, and Guodong Zhou.\\n2021. Joint Multi-modal Aspect-Sentiment Anal-\\nysis with Auxiliary Cross-modal Relation Detec-\\ntion. In Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 4395–4405, Online and Punta Cana, Domini-\\ncan Republic. Association for Computational Lin-\\nguistics.\\nKS Kalaivani, M Rakshana, K Mounika, and D Sindhu.\\n2022. Senticnet-based feature weighting scheme for\\nsentiment classification. In Mobile Computing and\\nSustainable Informatics, pages 839–848. Springer.\\nZaid Khan and Yun Fu. 2021. Exploiting bert for\\nmultimodal target sentiment classification through\\ninput space translation. In Proceedings of the\\n29th ACM International Conference on Multimedia,\\npages 3034–3042.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-',\n",
              " 'pages 3034–3042.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\\nnoising sequence-to-sequence pre-training for natural\\nlanguage generation, translation, and comprehension.\\narXiv preprint arXiv:1910.13461.\\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu\\nMa, Xiaojie Wang, and Eduard Hovy.\\n2021a. Dual Graph Convolutional Networks\\nfor Aspect-based Sentiment Analysis. In\\nProceedings of the 59th Annual Meeting of\\nthe Association for Computational Linguistics',\n",
              " 'and the 11th International Joint Conference\\non Natural Language Processing (V olume 1:\\nLong Papers), pages 6319–6329, Online. Associa-\\ntion for Computational Linguistics.\\nYuanqing Li, Ke Zhang, Jingyu Wang, and Xinbo Gao.\\n2021b. A cognitive brain model for multimodal sen-\\ntiment analysis based on attention neural networks.\\nNeurocomputing, 430:159–173.\\nBin Liang, Hang Su, Lin Gui, Erik Cambria, and\\nRuifeng Xu. 2022a. Aspect-based sentiment anal-\\nysis via affective knowledge enhanced graph con-\\nvolutional networks. Knowledge-Based Systems,\\n235:107643.\\nBin Liang, Rongdi Yin, Lin Gui, Jiachen Du, and\\nRuifeng Xu. 2020. Jointly Learning Aspect-Focused\\nand Inter-Aspect Relations with Graph Convolu-\\ntional Networks for Aspect Sentiment Analysis. In\\nProceedings of the 28th International Conference\\non Computational Linguistics, pages 150–161,\\nBarcelona, Spain (Online). International Committee\\non Computational Linguistics.\\nShuo Liang, Wei Wei, Xian-Ling Mao, Fei Wang, and',\n",
              " 'on Computational Linguistics, pages 150–161,\\nBarcelona, Spain (Online). International Committee\\non Computational Linguistics.\\nShuo Liang, Wei Wei, Xian-Ling Mao, Fei Wang, and\\nZhiyong He. 2022b. BiSyn-GAT+: Bi-Syntax Aware\\nGraph Attention Network for Aspect-based Senti-\\nment Analysis. In Findings of the Association for\\nComputational Linguistics: ACL 2022, pages 1835–\\n1848, Dublin, Ireland. Association for Computational\\nLinguistics.\\nYan Ling, Jianfei Yu, and Rui Xia. 2022. Vision-\\nLanguage Pre-Training for Multimodal Aspect-\\nBased Sentiment Analysis. In Proceedings of\\nthe 60th Annual Meeting of the Association\\nfor Computational Linguistics (V olume 1:\\nLong Papers), pages 2149–2159, Dublin, Ire-\\nland. Association for Computational Linguistics.\\nYanxia Lv, Fangna Wei, Lihong Cao, Sancheng Peng,\\nJianwei Niu, Shui Yu, and Cuirong Wang. 2021.\\nAspect-level sentiment analysis using context and\\naspect memory network. Neurocomputing, 428:195–\\n205.',\n",
              " 'Yanxia Lv, Fangna Wei, Lihong Cao, Sancheng Peng,\\nJianwei Niu, Shui Yu, and Cuirong Wang. 2021.\\nAspect-level sentiment analysis using context and\\naspect memory network. Neurocomputing, 428:195–\\n205.\\nYukun Ma, Haiyun Peng, and Erik Cambria. 2018. Tar-\\ngeted aspect-based sentiment analysis via embed-\\nding commonsense knowledge into an attentive lstm.\\nProceedings of the AAAI Conference on Artificial\\nIntelligence, 32(1).\\nShinhyeok Oh, Dongyub Lee, Taesun Whang, IlNam\\nPark, Seo Gaeun, EungGyun Kim, and Harksoo\\nKim. 2021. Deep Context- and Relation-Aware\\nLearning for Aspect-based Sentiment Analysis.\\nIn Proceedings of the 59th Annual Meeting of\\nthe Association for Computational Linguistics\\nand the 11th International Joint Conference\\non Natural Language Processing (V olume 2:\\nShort Papers), pages 495–503, Online. Association\\nfor Computational Linguistics.\\nShiguan Pang, Yun Xue, Zehao Yan, Weihao Huang,\\nand Jinhui Feng. 2021. Dynamic and Multi-Channel',\n",
              " 'Short Papers), pages 495–503, Online. Association\\nfor Computational Linguistics.\\nShiguan Pang, Yun Xue, Zehao Yan, Weihao Huang,\\nand Jinhui Feng. 2021. Dynamic and Multi-Channel\\nGraph Convolutional Networks for Aspect-Based\\nSentiment Analysis. In Findings of the Association\\nfor Computational Linguistics: ACL-IJCNLP 2021,\\npages 2627–2636, Online. Association for Computa-\\ntional Linguistics.\\nLin Sun, Jiquan Wang, Kai Zhang, Yindu Su, and Fang-\\nsheng Weng. 2021. Rpbert: A text-image relation\\npropagation-based bert model for multimodal ner.\\nProceedings of the AAAI Conference on Artificial\\nIntelligence, 35(15):13860–13868.\\nAlakananda Vempala and Daniel Preo¸ tiuc-Pietro. 2019.\\nCategorizing and inferring the relationship between\\nthe text and image of twitter posts. In Proceedings\\nof the 57th annual meeting of the Association for\\nComputational Linguistics, pages 2830–2840.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi, and Lian Chi. 2020a. Multimodal as-',\n",
              " 'of the 57th annual meeting of the Association for\\nComputational Linguistics, pages 2830–2840.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi, and Lian Chi. 2020a. Multimodal as-\\npect extraction with region-aware alignment net-\\nwork. In Natural Language Processing and Chinese\\nComputing, pages 145–156, Cham. Springer Interna-\\ntional Publishing.\\nHanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan\\nLi, and Lian Chi. 2020b. Multimodal aspect ex-\\ntraction with region-aware alignment network. In\\nCCF International Conference on Natural Language\\nProcessing and Chinese Computing, pages 145–156.\\nSpringer.\\nYang Wu, Yanyan Zhao, Hao Yang, Song Chen,\\nBing Qin, Xiaohuan Cao, and Wenting Zhao.\\n2022. Sentiment Word Aware Multimodal Re-\\nfinement for Multimodal Sentiment Analysis with\\nASR Errors. In Findings of the Association for\\nComputational Linguistics: ACL 2022, pages 1397–\\n1406, Dublin, Ireland. Association for Computational\\nLinguistics.\\nZhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen,',\n",
              " 'Computational Linguistics: ACL 2022, pages 1397–\\n1406, Dublin, Ireland. Association for Computational\\nLinguistics.\\nZhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen,\\nHo-fung Leung, and Qing Li. 2020c. Multimodal\\nrepresentation with embedded visual guiding ob-\\njects for named entity recognition in social media\\nposts. In Proceedings of the 28th ACM International\\nConference on Multimedia, pages 1038–1046.\\nJunjie Xu, Shuwen Yang, Luwei Xiao, Zhichao Fu,\\nXingjiao Wu, Tianlong Ma, and Liang He. 2022.\\nGraph convolution over the semantic-syntactic hybrid\\ngraph enhanced by affective knowledge for aspect-\\nlevel sentiment classification. In 2022 International\\nJoint Conference on Neural Networks (IJCNN),\\npages 1–8. IEEE.\\nLu Xu, Hao Li, Wei Lu, and Lidong Bing.\\n2020. Position-Aware Tagging for Aspect\\nSentiment Triplet Extraction. In Proceedings\\nof the 2020 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP), pages\\n2339–2349, Online. Association for Computational\\nLinguistics.',\n",
              " 'Hang Yan, Junqi Dai, Xipeng Qiu, Zheng Zhang,\\net al. 2021. A unified generative framework for\\naspect-based sentiment analysis. arXiv preprint\\narXiv:2106.04300.\\nLi Yang, Jin-Cheon Na, and Jianfei Yu. 2022.\\nCross-Modal Multitask Transformer for End-to-\\nEnd Multimodal Aspect-Based Sentiment Anal-\\nysis. Information Processing & Management,\\n59(5):103038.\\nJianfei Yu and Jing Jiang. 2019. Adapting bert\\nfor target-oriented multimodal sentiment classi-\\nfication. In Proceedings of the Twenty-Eighth\\nInternational Joint Conference on Artificial\\nIntelligence, IJCAI-19, pages 5408–5414. Interna-\\ntional Joint Conferences on Artificial Intelligence\\nOrganization.\\nJianfei Yu, Jing Jiang, and Rui Xia. 2019. Entity-\\nsensitive attention and fusion network for entity-level\\nmultimodal sentiment classification. IEEE/ACM\\nTransactions on Audio, Speech, and Language\\nProcessing, 28:429–439.\\nJianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020.\\nImproving multimodal named entity recognition via',\n",
              " 'Transactions on Audio, Speech, and Language\\nProcessing, 28:429–439.\\nJianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020.\\nImproving multimodal named entity recognition via\\nentity span detection with unified multimodal trans-\\nformer. In Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics,\\npages 3342–3352, Online. Association for Computa-\\ntional Linguistics.\\nLi Yuan, Jin Wang, Liang-Chih Yu, and Xuejie Zhang.\\n2020. Graph attention network with memory fusion\\nfor aspect-level sentiment analysis. In Proceedings\\nof the 1st Conference of the Asia-Pacific Chapter of\\nthe Association for Computational Linguistics and\\nthe 10th International Joint Conference on Natural\\nLanguage Processing, pages 27–36, Suzhou, China.\\nAssociation for Computational Linguistics.\\nQi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang.\\n2018. Adaptive co-attention network for named en-\\ntity recognition in tweets. In Thirty-Second AAAI\\nConference on Artificial Intelligence.',\n",
              " 'Qi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang.\\n2018. Adaptive co-attention network for named en-\\ntity recognition in tweets. In Thirty-Second AAAI\\nConference on Artificial Intelligence.\\nYuhao Zhang, Ying Zhang, Wenya Guo, Xiangrui Cai,\\nand Xiaojie Yuan. 2022. Learning disentangled rep-\\nresentation for multimodal cross-domain sentiment\\nanalysis. IEEE Transactions on Neural Networks\\nand Learning Systems.']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bde24ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "93fe2e6a21a348d7af9baf5af848964d",
            "ecf8fc9e18f34de1abc6f407037473fe",
            "0785ce363e26431ea914d68dd48c691b",
            "689a5eb7c1344b17b34a9d88a884d144",
            "93b732c3385e4ea48aadda54e06fdaa6",
            "4a7efdaaaaf24212a56afccf41548355",
            "69f77a6ca641465ba9e3d118fdf92246",
            "b7bde4cc885b4a44ba95b8732643b136",
            "d85171a4d5e14033ac470821f5401902",
            "c8b137a76e1540a3baae562075ec3b78",
            "f04fecf12b24409f817333bfc069966e"
          ]
        },
        "id": "5bde24ed",
        "outputId": "589b310e-f6d0-4003-bcbe-52a7fcec2add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for 57 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93fe2e6a21a348d7af9baf5af848964d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (57, 384)\n",
            "Adding 57 documents to vector store...\n",
            "✅ Successfully added 57 documents to vector store\n",
            "   Total documents in collection: 114\n"
          ]
        }
      ],
      "source": [
        "### Convert the text to embeddings\n",
        "texts=[doc.page_content for doc in chunks]\n",
        "\n",
        "## Generate the Embeddings\n",
        "\n",
        "embeddings=embedding_manager.generate_embeddings(texts)\n",
        "\n",
        "##store int he vector dtaabase\n",
        "vectorstore.add_documents(chunks,embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "498acd10",
      "metadata": {
        "id": "498acd10"
      },
      "source": [
        "### Retriever Pipeline From VectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f7b0ee2",
      "metadata": {
        "id": "0f7b0ee2"
      },
      "outputs": [],
      "source": [
        "class RAGRetriever:\n",
        "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
        "\n",
        "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
        "        \"\"\"\n",
        "        Initialize the retriever\n",
        "\n",
        "        Args:\n",
        "            vector_store: Vector store containing document embeddings\n",
        "            embedding_manager: Manager for generating query embeddings\n",
        "        \"\"\"\n",
        "        self.vector_store = vector_store\n",
        "        self.embedding_manager = embedding_manager\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Retrieve relevant documents for a query\n",
        "\n",
        "        Args:\n",
        "            query: The search query\n",
        "            top_k: Number of top results to return\n",
        "            score_threshold: Minimum similarity score threshold\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries containing retrieved documents and metadata\n",
        "        \"\"\"\n",
        "        print(f\"Retrieving documents for query: '{query}'\")\n",
        "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
        "\n",
        "        # Search in vector store\n",
        "        try:\n",
        "            results = self.vector_store.collection.query(\n",
        "                query_embeddings=[query_embedding.tolist()],\n",
        "                n_results=top_k\n",
        "            )\n",
        "\n",
        "            # Process results\n",
        "            retrieved_docs = []\n",
        "\n",
        "            if results['documents'] and results['documents'][0]:\n",
        "                documents = results['documents'][0]\n",
        "                metadatas = results['metadatas'][0]\n",
        "                distances = results['distances'][0]\n",
        "                ids = results['ids'][0]\n",
        "\n",
        "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
        "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
        "                    similarity_score = 1 - distance\n",
        "\n",
        "                    if similarity_score >= score_threshold:\n",
        "                        retrieved_docs.append({\n",
        "                            'id': doc_id,\n",
        "                            'content': document,\n",
        "                            'metadata': metadata,\n",
        "                            'similarity_score': similarity_score,\n",
        "                            'distance': distance,\n",
        "                            'rank': i + 1\n",
        "                        })\n",
        "\n",
        "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
        "            else:\n",
        "                print(\"No documents found\")\n",
        "\n",
        "            return retrieved_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during retrieval: {e}\")\n",
        "            return []\n",
        "\n",
        "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351730b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "351730b4",
        "outputId": "6ccf6a02-9f75-4323-bbbd-926249e732f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RAGRetriever at 0x7e236f7c9f10>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "rag_retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7e78529",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8ad87e71dde64ecaa24ecf4c6fad30e2",
            "b3ae624764f74796a24b5c9ba78619f2",
            "c3facca00f37460ab18cdc37f8f005ba",
            "899ce9a2e5974038af743e7602f210b5",
            "cafd0c7a36674a279e04f446f11ef708",
            "6628cd55011f4da3a5294afff633c8bf",
            "63650e90f0e94c2aba6dc481f6977fc8",
            "db7871cc80204ee7afdb8e882413029d",
            "1a3f81a68c604fe38cdeaa6e258c30a0",
            "610e9352e92a43bea7709070e1652d5d",
            "ec614a63894c4d87a199ec97150fb8dd"
          ]
        },
        "id": "f7e78529",
        "outputId": "67382bdf-d392-4325-e749-430d3ec9e6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: 'what multimodal aspect-based sentiment analysis '\n",
            "Top K: 5, Score threshold: 0.0\n",
            "Generating embeddings for 1 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ad87e71dde64ecaa24ecf4c6fad30e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (1, 384)\n",
            "Retrieved 5 documents (after filtering)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'doc_0edf29b3_0',\n",
              "  'content': 'AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\\nYing Zhang1 Xiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented',\n",
              "  'metadata': {'content_length': 984,\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'keywords': '',\n",
              "   'doc_index': 0,\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'author': '',\n",
              "   'title': '',\n",
              "   'page_label': '1',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'page': 0,\n",
              "   'subject': '',\n",
              "   'file_type': 'pdf',\n",
              "   'total_pages': 11,\n",
              "   'trapped': '/False',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00'},\n",
              "  'similarity_score': 0.6571823358535767,\n",
              "  'distance': 0.34281766414642334,\n",
              "  'rank': 1},\n",
              " {'id': 'doc_259747e3_0',\n",
              "  'content': 'AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\\nYing Zhang1 Xiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented',\n",
              "  'metadata': {'content_length': 984,\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'subject': '',\n",
              "   'file_type': 'pdf',\n",
              "   'author': '',\n",
              "   'title': '',\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'keywords': '',\n",
              "   'page': 0,\n",
              "   'doc_index': 0,\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'page_label': '1',\n",
              "   'total_pages': 11,\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'trapped': '/False',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00'},\n",
              "  'similarity_score': 0.6571823358535767,\n",
              "  'distance': 0.34281766414642334,\n",
              "  'rank': 2},\n",
              " {'id': 'doc_45103a16_41',\n",
              "  'content': 'prediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,',\n",
              "  'metadata': {'subject': '',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'page': 7,\n",
              "   'title': '',\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'file_type': 'pdf',\n",
              "   'content_length': 556,\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'total_pages': 11,\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'doc_index': 41,\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'author': '',\n",
              "   'trapped': '/False',\n",
              "   'keywords': '',\n",
              "   'page_label': '8',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00'},\n",
              "  'similarity_score': 0.6155039370059967,\n",
              "  'distance': 0.3844960629940033,\n",
              "  'rank': 3},\n",
              " {'id': 'doc_c73012c5_41',\n",
              "  'content': 'prediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,',\n",
              "  'metadata': {'subject': '',\n",
              "   'trapped': '/False',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'page_label': '8',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'keywords': '',\n",
              "   'file_type': 'pdf',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'title': '',\n",
              "   'page': 7,\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'total_pages': 11,\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'author': '',\n",
              "   'doc_index': 41,\n",
              "   'content_length': 556,\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00'},\n",
              "  'similarity_score': 0.6155039370059967,\n",
              "  'distance': 0.3844960629940033,\n",
              "  'rank': 4},\n",
              " {'id': 'doc_330cd16a_8',\n",
              "  'content': 'or graph attention networks (GATs) (Yuan et al.,\\n2020) over dependency with the syntactic structure\\nof a sentence are fully exploited.\\n2.2 Multimodal Aspect-based Sentiment\\nAnalysis\\nWith the enrichment of multimodal users’ posts\\nin social media, researchers find that images of-\\nfer great supplementary information in aspect term\\nextraction (Wu et al., 2020a; Zhang et al., 2018;\\nAsgari-Chenaghlu et al., 2021) and sentiment anal-\\nysis (Wu et al., 2022; Zhang et al., 2022; Li\\net al., 2021b; Hazarika et al., 2020; Cai et al.,\\n2019). Thus, Multimodal Aspect-based Sentiment\\nAnalysis (MABSA) begins to be widely studied.\\nMABSA task can be divided into two independent\\nsub-tasks, i.e., Multimodal Aspect Term Extraction\\n(MATE) and Multimodal Aspect-oriented Senti-\\nment Classification (MASC). The former extracts\\nall aspect terms in the sentence at the prompt of the\\nimage, and the latter predicts the sentiment polari-\\nties for the aspects.\\nJu et al. (2021) first realizes MABSA in a unified',\n",
              "  'metadata': {'trapped': '/False',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'doc_index': 8,\n",
              "   'keywords': '',\n",
              "   'page_label': '2',\n",
              "   'title': '',\n",
              "   'author': '',\n",
              "   'content_length': 990,\n",
              "   'page': 1,\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'file_type': 'pdf',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'total_pages': 11,\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'subject': '',\n",
              "   'producer': 'pdfTeX-1.40.25'},\n",
              "  'similarity_score': 0.4925820827484131,\n",
              "  'distance': 0.5074179172515869,\n",
              "  'rank': 5}]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "rag_retriever.retrieve(\"what multimodal aspect-based sentiment analysis \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8141ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "625c18c4e5bd42cca0c70d9bd01a7531",
            "f922e98401a041c2a915e98b38cda8ed",
            "bd67b520cbe04b688b9d097ccdf94d98",
            "9a91089cdb954c60907c3f59a6727db7",
            "bf079d608d3b403f9bf2776338b2fb82",
            "db61d91f86ad4b66a8833e904a9b1146",
            "df746255baee43ed9552964f51c362bd",
            "db4198295a29405dbdba8de9fd85f0ee",
            "adbf17fe453a44808ff5b4191daf1f1f",
            "0363ea289c874ff7a380ec5da12ddb0c",
            "71ab0504ad8244c9afe849b78b9492f8"
          ]
        },
        "id": "8d8141ee",
        "outputId": "832f977e-4178-4ec0-95aa-b4e2430d1957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: 'Which datasets used for  for multimodal aspect-based sentiment analysis'\n",
            "Top K: 5, Score threshold: 0.0\n",
            "Generating embeddings for 1 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "625c18c4e5bd42cca0c70d9bd01a7531"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (1, 384)\n",
            "Retrieved 5 documents (after filtering)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'doc_0edf29b3_0',\n",
              "  'content': 'AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\\nYing Zhang1 Xiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented',\n",
              "  'metadata': {'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'trapped': '/False',\n",
              "   'page': 0,\n",
              "   'subject': '',\n",
              "   'content_length': 984,\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'page_label': '1',\n",
              "   'total_pages': 11,\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'author': '',\n",
              "   'file_type': 'pdf',\n",
              "   'title': '',\n",
              "   'keywords': '',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'doc_index': 0},\n",
              "  'similarity_score': 0.47709566354751587,\n",
              "  'distance': 0.5229043364524841,\n",
              "  'rank': 1},\n",
              " {'id': 'doc_259747e3_0',\n",
              "  'content': 'AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\\nYing Zhang1 Xiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented',\n",
              "  'metadata': {'producer': 'pdfTeX-1.40.25',\n",
              "   'page': 0,\n",
              "   'page_label': '1',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'subject': '',\n",
              "   'total_pages': 11,\n",
              "   'content_length': 984,\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'file_type': 'pdf',\n",
              "   'doc_index': 0,\n",
              "   'title': '',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'keywords': '',\n",
              "   'trapped': '/False',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'author': ''},\n",
              "  'similarity_score': 0.47709566354751587,\n",
              "  'distance': 0.5229043364524841,\n",
              "  'rank': 2},\n",
              " {'id': 'doc_45103a16_41',\n",
              "  'content': 'prediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,',\n",
              "  'metadata': {'page': 7,\n",
              "   'keywords': '',\n",
              "   'content_length': 556,\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'author': '',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'doc_index': 41,\n",
              "   'total_pages': 11,\n",
              "   'page_label': '8',\n",
              "   'title': '',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'subject': '',\n",
              "   'trapped': '/False',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'file_type': 'pdf'},\n",
              "  'similarity_score': 0.464114248752594,\n",
              "  'distance': 0.535885751247406,\n",
              "  'rank': 3},\n",
              " {'id': 'doc_c73012c5_41',\n",
              "  'content': 'prediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,',\n",
              "  'metadata': {'content_length': 556,\n",
              "   'page': 7,\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'author': '',\n",
              "   'total_pages': 11,\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'trapped': '/False',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'keywords': '',\n",
              "   'file_type': 'pdf',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'doc_index': 41,\n",
              "   'subject': '',\n",
              "   'title': '',\n",
              "   'page_label': '8'},\n",
              "  'similarity_score': 0.464114248752594,\n",
              "  'distance': 0.535885751247406,\n",
              "  'rank': 4},\n",
              " {'id': 'doc_79ffee32_8',\n",
              "  'content': 'or graph attention networks (GATs) (Yuan et al.,\\n2020) over dependency with the syntactic structure\\nof a sentence are fully exploited.\\n2.2 Multimodal Aspect-based Sentiment\\nAnalysis\\nWith the enrichment of multimodal users’ posts\\nin social media, researchers find that images of-\\nfer great supplementary information in aspect term\\nextraction (Wu et al., 2020a; Zhang et al., 2018;\\nAsgari-Chenaghlu et al., 2021) and sentiment anal-\\nysis (Wu et al., 2022; Zhang et al., 2022; Li\\net al., 2021b; Hazarika et al., 2020; Cai et al.,\\n2019). Thus, Multimodal Aspect-based Sentiment\\nAnalysis (MABSA) begins to be widely studied.\\nMABSA task can be divided into two independent\\nsub-tasks, i.e., Multimodal Aspect Term Extraction\\n(MATE) and Multimodal Aspect-oriented Senti-\\nment Classification (MASC). The former extracts\\nall aspect terms in the sentence at the prompt of the\\nimage, and the latter predicts the sentiment polari-\\nties for the aspects.\\nJu et al. (2021) first realizes MABSA in a unified',\n",
              "  'metadata': {'trapped': '/False',\n",
              "   'keywords': '',\n",
              "   'content_length': 990,\n",
              "   'doc_index': 8,\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'file_type': 'pdf',\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'page': 1,\n",
              "   'total_pages': 11,\n",
              "   'subject': '',\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'page_label': '2',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'title': '',\n",
              "   'author': ''},\n",
              "  'similarity_score': 0.40119796991348267,\n",
              "  'distance': 0.5988020300865173,\n",
              "  'rank': 5}]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "rag_retriever.retrieve(\"Which datasets used for  for multimodal aspect-based sentiment analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce23783e",
      "metadata": {
        "id": "ce23783e"
      },
      "source": [
        "### RAG Pipeline- VectorDB To LLM Output Generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"GROQ_API_KEY\"\n",
        "\n",
        "print(os.getenv(\"GROQ_API_KEY\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRznzf3ta8d8",
        "outputId": "d17bee9e-c181-43b8-e327-9e5341b71b31"
      },
      "id": "jRznzf3ta8d8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq"
      ],
      "metadata": {
        "id": "pC4KyQHNUk_i"
      },
      "id": "pC4KyQHNUk_i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4b617e",
      "metadata": {
        "id": "ba4b617e"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40bba05b",
      "metadata": {
        "id": "40bba05b"
      },
      "outputs": [],
      "source": [
        "class GroqLLM:\n",
        "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: str =None):\n",
        "        \"\"\"\n",
        "        Initialize Groq LLM\n",
        "\n",
        "        Args:\n",
        "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
        "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
        "\n",
        "        self.llm = ChatGroq(\n",
        "            groq_api_key=self.api_key,\n",
        "            model_name=self.model_name,\n",
        "            temperature=0.1,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "\n",
        "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
        "\n",
        "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
        "        \"\"\"\n",
        "        Generate response using retrieved context\n",
        "\n",
        "        Args:\n",
        "            query: User question\n",
        "            context: Retrieved document context\n",
        "            max_length: Maximum response length\n",
        "\n",
        "        Returns:\n",
        "            Generated response string\n",
        "        \"\"\"\n",
        "\n",
        "        # Create prompt template\n",
        "        prompt_template = PromptTemplate(\n",
        "            input_variables=[\"context\", \"question\"],\n",
        "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
        "        )\n",
        "\n",
        "        # Format the prompt\n",
        "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
        "\n",
        "        try:\n",
        "            # Generate response\n",
        "            messages = [HumanMessage(content=formatted_prompt)]\n",
        "            response = self.llm.invoke(messages)\n",
        "            return response.content\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "    def generate_response_simple(self, query: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Simple response generation without complex prompting\n",
        "\n",
        "        Args:\n",
        "            query: User question\n",
        "            context: Retrieved context\n",
        "\n",
        "        Returns:\n",
        "            Generated response\n",
        "        \"\"\"\n",
        "        simple_prompt = f\"\"\"Based on this context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        try:\n",
        "            messages = [HumanMessage(content=simple_prompt)]\n",
        "            response = self.llm.invoke(messages)\n",
        "            return response.content\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
        "try:\n",
        "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "    print(\"Groq LLM initialized successfully!\")\n",
        "except ValueError as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
        "    groq_llm = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9IwPcSvVYUS",
        "outputId": "4bf31f1d-96b8-4b6d-b8a1-eb86fa67ad44"
      },
      "id": "A9IwPcSvVYUS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Groq LLM with model: gemma2-9b-it\n",
            "Groq LLM initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1fc0f74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1fc0f74",
        "outputId": "58f35a7d-2531-444b-82a9-49d194fca77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Groq LLM with model: gemma2-9b-it\n",
            "Groq LLM initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
        "try:\n",
        "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "    print(\"Groq LLM initialized successfully!\")\n",
        "except ValueError as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
        "    groq_llm = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4110c55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "91a0bda9bfae43f0a22767ab19bb465f",
            "d739bc23887f4a159c6d4b50e1b09101",
            "9d2e808e0b4c4d26b8993e47c7ca47d7",
            "df95d45f262140768968eebc639e6dbd",
            "75e7c184b3024758a599206a65827902",
            "a2e1d155ecb74288bbecdd51b3048e25",
            "20ea481b712c499c86073c297793774d",
            "5ce7a423cabf4974b1a8b745ff48d1a6",
            "c1a3ff982f5b4478bc33d2ee4a6d1a0f",
            "ed0d8d115b0a4848896eaaff92cb55e0",
            "c3095770258d4bbe99a7d20649454e64"
          ]
        },
        "id": "c4110c55",
        "outputId": "f681a968-ebe1-4720-a3d8-e93a4bec7c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: 'what is multimodal aspect-based sentiment analysis'\n",
            "Top K: 5, Score threshold: 0.0\n",
            "Generating embeddings for 1 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91a0bda9bfae43f0a22767ab19bb465f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (1, 384)\n",
            "Retrieved 5 documents (after filtering)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'doc_0edf29b3_0',\n",
              "  'content': 'AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\\nYing Zhang1 Xiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented',\n",
              "  'metadata': {'total_pages': 11,\n",
              "   'content_length': 984,\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'doc_index': 0,\n",
              "   'subject': '',\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'file_type': 'pdf',\n",
              "   'author': '',\n",
              "   'page_label': '1',\n",
              "   'keywords': '',\n",
              "   'title': '',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'trapped': '/False',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'page': 0,\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00'},\n",
              "  'similarity_score': 0.6424300968647003,\n",
              "  'distance': 0.3575699031352997,\n",
              "  'rank': 1},\n",
              " {'id': 'doc_259747e3_0',\n",
              "  'content': 'AoM: Detecting Aspect-oriented Information for Multimodal\\nAspect-Based Sentiment Analysis\\nRu Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\\nYing Zhang1 Xiaojie Yuan1\\n1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\\n{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\\nyuanxj@nankai.edu.cn\\nAbstract\\nMultimodal aspect-based sentiment analysis\\n(MABSA) aims to extract aspects from text-\\nimage pairs and recognize their sentiments. Ex-\\nisting methods make great efforts to align the\\nwhole image to corresponding aspects. How-\\never, different regions of the image may relate\\nto different aspects in the same sentence, and\\ncoarsely establishing image-aspect alignment\\nwill introduce noise to aspect-based sentiment\\nanalysis (i.e., visual noise). Besides, the sen-\\ntiment of a specific aspect can also be inter-\\nfered by descriptions of other aspects (i.e., tex-\\ntual noise). Considering the aforementioned\\nnoises, this paper proposes an Aspect-oriented',\n",
              "  'metadata': {'keywords': '',\n",
              "   'trapped': '/False',\n",
              "   'author': '',\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'subject': '',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'doc_index': 0,\n",
              "   'file_type': 'pdf',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'content_length': 984,\n",
              "   'total_pages': 11,\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'page_label': '1',\n",
              "   'page': 0,\n",
              "   'title': ''},\n",
              "  'similarity_score': 0.6424300968647003,\n",
              "  'distance': 0.3575699031352997,\n",
              "  'rank': 2},\n",
              " {'id': 'doc_45103a16_41',\n",
              "  'content': 'prediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,',\n",
              "  'metadata': {'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'doc_index': 41,\n",
              "   'trapped': '/False',\n",
              "   'file_type': 'pdf',\n",
              "   'total_pages': 11,\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'page_label': '8',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'title': '',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'author': '',\n",
              "   'subject': '',\n",
              "   'page': 7,\n",
              "   'content_length': 556,\n",
              "   'keywords': ''},\n",
              "  'similarity_score': 0.5977010130882263,\n",
              "  'distance': 0.4022989869117737,\n",
              "  'rank': 3},\n",
              " {'id': 'doc_c73012c5_41',\n",
              "  'content': 'prediction.\\n5 Conclusion\\nIn this paper, we proposed an aspect-oriented\\nmodel (AoM) for the task of multimodal aspect-\\nbased sentiment analysis. We use two specially\\ndesigned modules to detect aspect-relevant infor-\\nmation from the semantic and sentiment perspec-\\ntives. On the one hand, to learn aspect-relevant\\nsemantic information especially from the image,\\nwe construct the Aspect-Aware Attention Module\\nto align the visual information and descriptions to\\nthe corresponding aspect. On the other hand, to\\ndetect the aspect-relevant sentiment information,',\n",
              "  'metadata': {'creator': 'LaTeX with hyperref',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'total_pages': 11,\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'content_length': 556,\n",
              "   'doc_index': 41,\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'page_label': '8',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'trapped': '/False',\n",
              "   'title': '',\n",
              "   'page': 7,\n",
              "   'keywords': '',\n",
              "   'author': '',\n",
              "   'subject': '',\n",
              "   'file_type': 'pdf',\n",
              "   'source_file': 'AoM.pdf',\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00'},\n",
              "  'similarity_score': 0.5977010130882263,\n",
              "  'distance': 0.4022989869117737,\n",
              "  'rank': 4},\n",
              " {'id': 'doc_330cd16a_8',\n",
              "  'content': 'or graph attention networks (GATs) (Yuan et al.,\\n2020) over dependency with the syntactic structure\\nof a sentence are fully exploited.\\n2.2 Multimodal Aspect-based Sentiment\\nAnalysis\\nWith the enrichment of multimodal users’ posts\\nin social media, researchers find that images of-\\nfer great supplementary information in aspect term\\nextraction (Wu et al., 2020a; Zhang et al., 2018;\\nAsgari-Chenaghlu et al., 2021) and sentiment anal-\\nysis (Wu et al., 2022; Zhang et al., 2022; Li\\net al., 2021b; Hazarika et al., 2020; Cai et al.,\\n2019). Thus, Multimodal Aspect-based Sentiment\\nAnalysis (MABSA) begins to be widely studied.\\nMABSA task can be divided into two independent\\nsub-tasks, i.e., Multimodal Aspect Term Extraction\\n(MATE) and Multimodal Aspect-oriented Senti-\\nment Classification (MASC). The former extracts\\nall aspect terms in the sentence at the prompt of the\\nimage, and the latter predicts the sentiment polari-\\nties for the aspects.\\nJu et al. (2021) first realizes MABSA in a unified',\n",
              "  'metadata': {'source_file': 'AoM.pdf',\n",
              "   'doc_index': 8,\n",
              "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
              "   'content_length': 990,\n",
              "   'creationdate': '2023-06-05T01:00:14+00:00',\n",
              "   'keywords': '',\n",
              "   'source': '/content/drive/MyDrive/AI Engineering/Traditional RAG/data/pdf/AoM.pdf',\n",
              "   'trapped': '/False',\n",
              "   'page': 1,\n",
              "   'total_pages': 11,\n",
              "   'author': '',\n",
              "   'title': '',\n",
              "   'moddate': '2023-06-05T01:00:14+00:00',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'subject': '',\n",
              "   'producer': 'pdfTeX-1.40.25',\n",
              "   'file_type': 'pdf',\n",
              "   'page_label': '2'},\n",
              "  'similarity_score': 0.5113329887390137,\n",
              "  'distance': 0.48866701126098633,\n",
              "  'rank': 5}]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "### get the context from the retriever and pass it to the LLM\n",
        "\n",
        "rag_retriever.retrieve(\"what is multimodal aspect-based sentiment analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea465ac",
      "metadata": {
        "id": "5ea465ac"
      },
      "source": [
        "### Integration Vectordb Context pipeline With LLM output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df1bf366",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "f53dfb56c68b4886ad6aa07fef1bd1c8",
            "acfe4d4caf474a33af8b43a2c0eb2811",
            "29b7dfee75c04db8bc1c74b9025f3018",
            "b142c21ec0374a5d8f14237b845f9c4c",
            "1551e65f5c004afbb64d0cb2f08a13df",
            "16aa954ea99a4ed294a971f73701f2b0",
            "17eb0aef4c2a4333a04bbb19bbb0f14e",
            "0aeb351b6bfc4d27a4f49b270e252f3f",
            "745e2e9776094dbf849da396c3dc09da",
            "fe46e265e3b448f5a45344455296a562",
            "59975289cff04376ac77d55abba9e03d"
          ]
        },
        "id": "df1bf366",
        "outputId": "f0e54068-08ce-4656-e90c-b8de8e02bbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: 'What is attention mechanism?'\n",
            "Top K: 3, Score threshold: 0.0\n",
            "Generating embeddings for 1 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f53dfb56c68b4886ad6aa07fef1bd1c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (1, 384)\n",
            "Retrieved 0 documents (after filtering)\n",
            "No relevant context found to answer the question.\n"
          ]
        }
      ],
      "source": [
        "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Simple RAG pipeline with Groq LLM\n",
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables (make sure GROQ_API_KEY is in your .env file)\n",
        "load_dotenv()\n",
        "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# Initialize the Groq LLM\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=groq_api_key,\n",
        "    model_name=\"gemma2-9b-it\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "## Simple RAG function: retrieve context + generate response\n",
        "def rag_simple(query, retriever, llm, top_k=3):\n",
        "    # retrieve relevant docs\n",
        "    results = retriever.retrieve(query, top_k=top_k)\n",
        "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
        "    if not context:\n",
        "        return \"No relevant context found to answer the question.\"\n",
        "\n",
        "    # build prompt safely\n",
        "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # generate response\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "kDNJU1YPfAgA"
      },
      "id": "kDNJU1YPfAgA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (rag_retriever must already be defined)\n",
        "answer = rag_simple(\"What is multimodal aspect-based sentiment analysis?\", rag_retriever, llm)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "011b280a7b53487ea34e5edf807e0b90",
            "6a8100ffa10a41a7a293fc866c71120c",
            "cb0997d5beed48d4967907d8d8fea0f8",
            "b574e93dfa6c4df495a74ee2298f0145",
            "97a6d99000274b0d96d6f17352a7a182",
            "defde1cfe6114b499a5af773920b6f3f",
            "b578bbb4bb1f4a32901dc7927d52c0ad",
            "dc1ed7fac25a4eb7be4bdbe47d4abf5b",
            "b4b31ac5dc2c4cc085ec18da5573b270",
            "04110c806a0145949d1cffa883ff5943",
            "8eebb95109e243bd89a91cc1c2f21037"
          ]
        },
        "id": "Sr2mctpUfKiS",
        "outputId": "4d92a87c-57d8-4d86-c62d-76e1d5869298"
      },
      "id": "Sr2mctpUfKiS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: 'What is multimodal aspect-based sentiment analysis?'\n",
            "Top K: 3, Score threshold: 0.0\n",
            "Generating embeddings for 1 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "011b280a7b53487ea34e5edf807e0b90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (1, 384)\n",
            "Retrieved 3 documents (after filtering)\n",
            "Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and determine the sentiment associated with each aspect.  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (rag_retriever must already be defined)\n",
        "answer = rag_simple(\"What the archetecture applayed in multimodal aspect-based sentiment analysis?\", rag_retriever, llm)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "45da704d9ee545eda003fa69f9bfda2d",
            "d624517fd29e480f87007497ed73e5ef",
            "4818f799b4524294bedaf858c180ea35",
            "1949728e9a5941798c7f7f74a38a9213",
            "caf5ebffab5f4e7eb117165ca701a674",
            "6dde1a15fd014c609f00b64bb057f7d6",
            "31adb31ed04a4c0d8c6e9fe0d6d4a1a3",
            "10a2a21e9ee34e81be7f7648a24818d2",
            "da2974bc64644b3fae09e89627ad332d",
            "54ebfc21c4004344903ea69a2c636bcb",
            "db3cb83e467b4d208dd0826eea63940d"
          ]
        },
        "id": "9wjvJ8L8fNyd",
        "outputId": "52e92eb0-83d0-4dba-e13e-aef21c0d7ae6"
      },
      "id": "9wjvJ8L8fNyd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: 'What the archetecture applayed in multimodal aspect-based sentiment analysis?'\n",
            "Top K: 3, Score threshold: 0.0\n",
            "Generating embeddings for 1 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45da704d9ee545eda003fa69f9bfda2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (1, 384)\n",
            "Retrieved 3 documents (after filtering)\n",
            "The paper proposes an **Aspect-oriented model (AoM)**. \n",
            "\n",
            "It uses two modules:\n",
            "\n",
            "* **Aspect-Aware Attention Module:**  To align visual information and descriptions to corresponding aspects.\n",
            "*  A module to detect aspect-relevant sentiment information. \n",
            "\n",
            "\n",
            "Let me know if you'd like more details about either module! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6857b1c2",
      "metadata": {
        "id": "6857b1c2"
      },
      "source": [
        "### Enhanced RAG Pipeline Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2832fd17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "23c16f99982246739515c10d66ee31e7",
            "0ddc19c150d349d49f7a19138566b567",
            "1639e884e7ed4e708f4925818e2f5b22",
            "fceefb0dc57e42758bf43ef2c5db6ed6",
            "5f50d42163964019b5c6f3fa777586a2",
            "7b1d11ebcfd74de3928066997e127b33",
            "253820e3878745659d1c4e0148d258f7",
            "724a9139f1854686bd4a2ebbba33a815",
            "45e4a97466f74dc8a1c493201c6c8631",
            "3867c6a9452c4b5aa53024b86a1ef970",
            "d12cbf2d551c4bd79f157bac0ca6be33"
          ]
        },
        "id": "2832fd17",
        "outputId": "6c967a67-241d-45a4-d971-4a9b533a4929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: 'What names of datasets Twitter-2015 used in multimoda aspect-based sentiment analysis'\n",
            "Top K: 3, Score threshold: 0.1\n",
            "Generating embeddings for 1 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23c16f99982246739515c10d66ee31e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (1, 384)\n",
            "Retrieved 3 documents (after filtering)\n",
            "Answer: The provided text does not mention any datasets used in the context of multimoda aspect-based sentiment analysis. \n",
            "\n",
            "\n",
            "\n",
            "Sources: [{'source': 'AoM.pdf', 'page': 8, 'score': 0.24316471815109253, 'preview': 'cation. arXiv preprint arXiv:1906.03820.\\nXincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\\nShoushan Li, Min Zhang, and Guodong Zhou.\\n2021. Joint Multi-modal Aspect-Sentiment Anal-\\nysis with Auxiliary Cross-modal Relation Detec-\\ntion. In Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural...'}, {'source': 'AoM.pdf', 'page': 8, 'score': 0.24316471815109253, 'preview': 'cation. arXiv preprint arXiv:1906.03820.\\nXincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\\nShoushan Li, Min Zhang, and Guodong Zhou.\\n2021. Joint Multi-modal Aspect-Sentiment Anal-\\nysis with Auxiliary Cross-modal Relation Detec-\\ntion. In Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural...'}, {'source': 'AoM.pdf', 'page': 10, 'score': 0.24081236124038696, 'preview': 'Hang Yan, Junqi Dai, Xipeng Qiu, Zheng Zhang,\\net al. 2021. A unified generative framework for\\naspect-based sentiment analysis. arXiv preprint\\narXiv:2106.04300.\\nLi Yang, Jin-Cheon Na, and Jianfei Yu. 2022.\\nCross-Modal Multitask Transformer for End-to-\\nEnd Multimodal Aspect-Based Sentiment Anal-\\nysis....'}]\n",
            "Confidence: 0.24316471815109253\n",
            "Context Preview: cation. arXiv preprint arXiv:1906.03820.\n",
            "Xincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,\n",
            "Shoushan Li, Min Zhang, and Guodong Zhou.\n",
            "2021. Joint Multi-modal Aspect-Sentiment Anal-\n",
            "ysis with Auxiliary Cross-modal Relation Detec-\n",
            "tion. In Proceedings of the 2021 Conference on\n",
            "Empirical Methods in Natural\n"
          ]
        }
      ],
      "source": [
        "# --- Enhanced RAG Pipeline Features ---\n",
        "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
        "    \"\"\"\n",
        "    RAG pipeline with extra features:\n",
        "    - Returns answer, sources, confidence score, and optionally full context.\n",
        "    \"\"\"\n",
        "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
        "    if not results:\n",
        "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
        "\n",
        "    # Prepare context and sources\n",
        "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
        "    sources = [{\n",
        "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
        "        'page': doc['metadata'].get('page', 'unknown'),\n",
        "        'score': doc['similarity_score'],\n",
        "        'preview': doc['content'][:300] + '...'\n",
        "    } for doc in results]\n",
        "    confidence = max([doc['similarity_score'] for doc in results])\n",
        "\n",
        "    # Generate answer\n",
        "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
        "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
        "\n",
        "    output = {\n",
        "        'answer': response.content,\n",
        "        'sources': sources,\n",
        "        'confidence': confidence\n",
        "    }\n",
        "    if return_context:\n",
        "        output['context'] = context\n",
        "    return output\n",
        "\n",
        "# Example usage:\n",
        "result = rag_advanced(\"What names of datasets Twitter-2015 used in multimoda aspect-based sentiment analysis\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
        "print(\"Answer:\", result['answer'])\n",
        "print(\"Sources:\", result['sources'])\n",
        "print(\"Confidence:\", result['confidence'])\n",
        "print(\"Context Preview:\", result['context'][:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6150d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "102532a7332045eabf4da8bc0d9227fd",
            "75d02b87acc4489ba6812d4635674a44",
            "5608bcb11f274ab9b710e903b87d6431",
            "18ba5faa8ea44db3aa6a5b53dfff1ddc",
            "5ce168adddab49a191129f8ef48fcf08",
            "40e3d78a5ac74e699d14b0e09a30f54c",
            "a301d01b0a6a4efa885c50d4be35efed",
            "9730737eea9e44b2b105661fbdaee4b5",
            "0db645f07791425b9bdcdda7d1a3442c",
            "b5fec64478eb423c9b7dd5d6d556eb21",
            "e2684f8aebce47edbd8571642787d99b"
          ]
        },
        "id": "aa6150d0",
        "outputId": "4ad2c46d-01f9-4f6e-b5be-d1176417ab0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: 'what is multimoda aspect-based sentiment analysis'\n",
            "Top K: 3, Score threshold: 0.1\n",
            "Generating embeddings for 1 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "102532a7332045eabf4da8bc0d9227fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (1, 384)\n",
            "Retrieved 3 documents (after filtering)\n",
            "Streaming answer:\n",
            "Use the following context to answer the question concisely.\n",
            "Context:\n",
            "prediction.\n",
            "5 Conclusion\n",
            "In this paper, we proposed an aspect-oriented\n",
            "model (AoM) for the task of multimodal aspect-\n",
            "based sentiment analysis. We use two specially\n",
            "designed modules to detect aspect-relevant infor-\n",
            "mation from the semantic and sentiment perspec-\n",
            "tives. On the one hand, to learn aspect-relevant\n",
            "semantic information especially from the image,\n",
            "we construct the Aspect-Aware Attention Module\n",
            "to align the visual information and descriptions to\n",
            "the corresponding aspect. On the other hand, to\n",
            "detect the aspect-relevant sentiment information,\n",
            "\n",
            "prediction.\n",
            "5 Conclusion\n",
            "In this paper, we proposed an aspect-oriented\n",
            "model (AoM) for the task of multimodal aspect-\n",
            "based sentiment analysis. We use two specially\n",
            "designed modules to detect aspect-relevant infor-\n",
            "mation from the semantic and sentiment perspec-\n",
            "tives. On the one hand, to learn aspect-relevant\n",
            "semantic information especially from the image,\n",
            "we construct the Aspect-Aware Attention Module\n",
            "to align the visual information and descriptions to\n",
            "the corresponding aspect. On the other hand, to\n",
            "detect the aspect-relevant sentiment information,\n",
            "\n",
            "AoM: Detecting Aspect-oriented Information for Multimodal\n",
            "Aspect-Based Sentiment Analysis\n",
            "Ru Zhou1 Wenya Guo1∗ Xumeng Liu1 Shenglong Yu1\n",
            "Ying Zhang1 Xiaojie Yuan1\n",
            "1 College of Computer Science, TKLNDST, Nankai University, Tianjin, China\n",
            "{zhouru,guowenya,liuxumeng,yushenglong,zhangying}@dbis.nankai.edu.cn\n",
            "yuanxj@nankai.edu.cn\n",
            "Abstract\n",
            "Multimodal aspect-based sentiment analysis\n",
            "(MABSA) aims to extract aspects from text-\n",
            "image pairs and recognize their sentiments. Ex-\n",
            "isting methods make great efforts to align the\n",
            "whole image to corresponding aspects. How-\n",
            "ever, different regions of the image may relate\n",
            "to different aspects in the same sentence, and\n",
            "coarsely establishing image-aspect alignment\n",
            "will introduce noise to aspect-based sentiment\n",
            "analysis (i.e., visual noise). Besides, the sen-\n",
            "timent of a specific aspect can also be inter-\n",
            "fered by descriptions of other aspects (i.e., tex-\n",
            "tual noise). Considering the aforementioned\n",
            "noises, this paper proposes an Aspect-oriented\n",
            "\n",
            "Question: what is multimoda aspect-based sentiment analysis\n",
            "\n",
            "Answer:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'zhouru,guowenya,liuxumeng,yushenglong,zhangying'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2475930128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0madv_rag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdvancedRAGPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrag_retriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_rag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what is multimoda aspect-based sentiment analysis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFinal Answer:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summary:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2475930128.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, question, top_k, min_score, stream, summarize)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'zhouru,guowenya,liuxumeng,yushenglong,zhangying'"
          ]
        }
      ],
      "source": [
        "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
        "from typing import List, Dict, Any\n",
        "import time\n",
        "\n",
        "class AdvancedRAGPipeline:\n",
        "    def __init__(self, retriever, llm):\n",
        "        self.retriever = retriever\n",
        "        self.llm = llm\n",
        "        self.history = []  # Store query history\n",
        "\n",
        "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
        "        # Retrieve relevant documents\n",
        "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
        "        if not results:\n",
        "            answer = \"No relevant context found.\"\n",
        "            sources = []\n",
        "            context = \"\"\n",
        "        else:\n",
        "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
        "            sources = [{\n",
        "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
        "                'page': doc['metadata'].get('page', 'unknown'),\n",
        "                'score': doc['similarity_score'],\n",
        "                'preview': doc['content'][:120] + '...'\n",
        "            } for doc in results]\n",
        "            # Streaming answer simulation\n",
        "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
        "            if stream:\n",
        "                print(\"Streaming answer:\")\n",
        "                for i in range(0, len(prompt), 80):\n",
        "                    print(prompt[i:i+80], end='', flush=True)\n",
        "                    time.sleep(0.05)\n",
        "                print()\n",
        "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
        "            answer = response.content\n",
        "\n",
        "        # Add citations to answer\n",
        "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
        "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
        "\n",
        "        # Optionally summarize answer\n",
        "        summary = None\n",
        "        if summarize and answer:\n",
        "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
        "            summary_resp = self.llm.invoke([summary_prompt])\n",
        "            summary = summary_resp.content\n",
        "\n",
        "        # Store query history\n",
        "        self.history.append({\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'sources': sources,\n",
        "            'summary': summary\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            'question': question,\n",
        "            'answer': answer_with_citations,\n",
        "            'sources': sources,\n",
        "            'summary': summary,\n",
        "            'history': self.history\n",
        "        }\n",
        "\n",
        "# Example usage:\n",
        "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
        "result = adv_rag.query(\"what is multimoda aspect-based sentiment analysis\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
        "print(\"\\nFinal Answer:\", result['answer'])\n",
        "print(\"Summary:\", result['summary'])\n",
        "print(\"History:\", result['history'][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d695e1b2",
      "metadata": {
        "id": "d695e1b2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "YTRAG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93fe2e6a21a348d7af9baf5af848964d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ecf8fc9e18f34de1abc6f407037473fe",
              "IPY_MODEL_0785ce363e26431ea914d68dd48c691b",
              "IPY_MODEL_689a5eb7c1344b17b34a9d88a884d144"
            ],
            "layout": "IPY_MODEL_93b732c3385e4ea48aadda54e06fdaa6"
          }
        },
        "ecf8fc9e18f34de1abc6f407037473fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a7efdaaaaf24212a56afccf41548355",
            "placeholder": "​",
            "style": "IPY_MODEL_69f77a6ca641465ba9e3d118fdf92246",
            "value": "Batches: 100%"
          }
        },
        "0785ce363e26431ea914d68dd48c691b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7bde4cc885b4a44ba95b8732643b136",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d85171a4d5e14033ac470821f5401902",
            "value": 2
          }
        },
        "689a5eb7c1344b17b34a9d88a884d144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8b137a76e1540a3baae562075ec3b78",
            "placeholder": "​",
            "style": "IPY_MODEL_f04fecf12b24409f817333bfc069966e",
            "value": " 2/2 [00:07&lt;00:00,  3.65s/it]"
          }
        },
        "93b732c3385e4ea48aadda54e06fdaa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a7efdaaaaf24212a56afccf41548355": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69f77a6ca641465ba9e3d118fdf92246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7bde4cc885b4a44ba95b8732643b136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d85171a4d5e14033ac470821f5401902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8b137a76e1540a3baae562075ec3b78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f04fecf12b24409f817333bfc069966e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ad87e71dde64ecaa24ecf4c6fad30e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3ae624764f74796a24b5c9ba78619f2",
              "IPY_MODEL_c3facca00f37460ab18cdc37f8f005ba",
              "IPY_MODEL_899ce9a2e5974038af743e7602f210b5"
            ],
            "layout": "IPY_MODEL_cafd0c7a36674a279e04f446f11ef708"
          }
        },
        "b3ae624764f74796a24b5c9ba78619f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6628cd55011f4da3a5294afff633c8bf",
            "placeholder": "​",
            "style": "IPY_MODEL_63650e90f0e94c2aba6dc481f6977fc8",
            "value": "Batches: 100%"
          }
        },
        "c3facca00f37460ab18cdc37f8f005ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db7871cc80204ee7afdb8e882413029d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a3f81a68c604fe38cdeaa6e258c30a0",
            "value": 1
          }
        },
        "899ce9a2e5974038af743e7602f210b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_610e9352e92a43bea7709070e1652d5d",
            "placeholder": "​",
            "style": "IPY_MODEL_ec614a63894c4d87a199ec97150fb8dd",
            "value": " 1/1 [00:00&lt;00:00,  7.44it/s]"
          }
        },
        "cafd0c7a36674a279e04f446f11ef708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6628cd55011f4da3a5294afff633c8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63650e90f0e94c2aba6dc481f6977fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db7871cc80204ee7afdb8e882413029d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a3f81a68c604fe38cdeaa6e258c30a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "610e9352e92a43bea7709070e1652d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec614a63894c4d87a199ec97150fb8dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "625c18c4e5bd42cca0c70d9bd01a7531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f922e98401a041c2a915e98b38cda8ed",
              "IPY_MODEL_bd67b520cbe04b688b9d097ccdf94d98",
              "IPY_MODEL_9a91089cdb954c60907c3f59a6727db7"
            ],
            "layout": "IPY_MODEL_bf079d608d3b403f9bf2776338b2fb82"
          }
        },
        "f922e98401a041c2a915e98b38cda8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db61d91f86ad4b66a8833e904a9b1146",
            "placeholder": "​",
            "style": "IPY_MODEL_df746255baee43ed9552964f51c362bd",
            "value": "Batches: 100%"
          }
        },
        "bd67b520cbe04b688b9d097ccdf94d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db4198295a29405dbdba8de9fd85f0ee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adbf17fe453a44808ff5b4191daf1f1f",
            "value": 1
          }
        },
        "9a91089cdb954c60907c3f59a6727db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0363ea289c874ff7a380ec5da12ddb0c",
            "placeholder": "​",
            "style": "IPY_MODEL_71ab0504ad8244c9afe849b78b9492f8",
            "value": " 1/1 [00:00&lt;00:00, 20.44it/s]"
          }
        },
        "bf079d608d3b403f9bf2776338b2fb82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db61d91f86ad4b66a8833e904a9b1146": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df746255baee43ed9552964f51c362bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db4198295a29405dbdba8de9fd85f0ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adbf17fe453a44808ff5b4191daf1f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0363ea289c874ff7a380ec5da12ddb0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71ab0504ad8244c9afe849b78b9492f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91a0bda9bfae43f0a22767ab19bb465f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d739bc23887f4a159c6d4b50e1b09101",
              "IPY_MODEL_9d2e808e0b4c4d26b8993e47c7ca47d7",
              "IPY_MODEL_df95d45f262140768968eebc639e6dbd"
            ],
            "layout": "IPY_MODEL_75e7c184b3024758a599206a65827902"
          }
        },
        "d739bc23887f4a159c6d4b50e1b09101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2e1d155ecb74288bbecdd51b3048e25",
            "placeholder": "​",
            "style": "IPY_MODEL_20ea481b712c499c86073c297793774d",
            "value": "Batches: 100%"
          }
        },
        "9d2e808e0b4c4d26b8993e47c7ca47d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ce7a423cabf4974b1a8b745ff48d1a6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1a3ff982f5b4478bc33d2ee4a6d1a0f",
            "value": 1
          }
        },
        "df95d45f262140768968eebc639e6dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed0d8d115b0a4848896eaaff92cb55e0",
            "placeholder": "​",
            "style": "IPY_MODEL_c3095770258d4bbe99a7d20649454e64",
            "value": " 1/1 [00:00&lt;00:00, 11.64it/s]"
          }
        },
        "75e7c184b3024758a599206a65827902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2e1d155ecb74288bbecdd51b3048e25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ea481b712c499c86073c297793774d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ce7a423cabf4974b1a8b745ff48d1a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a3ff982f5b4478bc33d2ee4a6d1a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed0d8d115b0a4848896eaaff92cb55e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3095770258d4bbe99a7d20649454e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f53dfb56c68b4886ad6aa07fef1bd1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acfe4d4caf474a33af8b43a2c0eb2811",
              "IPY_MODEL_29b7dfee75c04db8bc1c74b9025f3018",
              "IPY_MODEL_b142c21ec0374a5d8f14237b845f9c4c"
            ],
            "layout": "IPY_MODEL_1551e65f5c004afbb64d0cb2f08a13df"
          }
        },
        "acfe4d4caf474a33af8b43a2c0eb2811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16aa954ea99a4ed294a971f73701f2b0",
            "placeholder": "​",
            "style": "IPY_MODEL_17eb0aef4c2a4333a04bbb19bbb0f14e",
            "value": "Batches: 100%"
          }
        },
        "29b7dfee75c04db8bc1c74b9025f3018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aeb351b6bfc4d27a4f49b270e252f3f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_745e2e9776094dbf849da396c3dc09da",
            "value": 1
          }
        },
        "b142c21ec0374a5d8f14237b845f9c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe46e265e3b448f5a45344455296a562",
            "placeholder": "​",
            "style": "IPY_MODEL_59975289cff04376ac77d55abba9e03d",
            "value": " 1/1 [00:00&lt;00:00, 26.72it/s]"
          }
        },
        "1551e65f5c004afbb64d0cb2f08a13df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16aa954ea99a4ed294a971f73701f2b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17eb0aef4c2a4333a04bbb19bbb0f14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0aeb351b6bfc4d27a4f49b270e252f3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "745e2e9776094dbf849da396c3dc09da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe46e265e3b448f5a45344455296a562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59975289cff04376ac77d55abba9e03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "011b280a7b53487ea34e5edf807e0b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a8100ffa10a41a7a293fc866c71120c",
              "IPY_MODEL_cb0997d5beed48d4967907d8d8fea0f8",
              "IPY_MODEL_b574e93dfa6c4df495a74ee2298f0145"
            ],
            "layout": "IPY_MODEL_97a6d99000274b0d96d6f17352a7a182"
          }
        },
        "6a8100ffa10a41a7a293fc866c71120c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_defde1cfe6114b499a5af773920b6f3f",
            "placeholder": "​",
            "style": "IPY_MODEL_b578bbb4bb1f4a32901dc7927d52c0ad",
            "value": "Batches: 100%"
          }
        },
        "cb0997d5beed48d4967907d8d8fea0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc1ed7fac25a4eb7be4bdbe47d4abf5b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4b31ac5dc2c4cc085ec18da5573b270",
            "value": 1
          }
        },
        "b574e93dfa6c4df495a74ee2298f0145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04110c806a0145949d1cffa883ff5943",
            "placeholder": "​",
            "style": "IPY_MODEL_8eebb95109e243bd89a91cc1c2f21037",
            "value": " 1/1 [00:00&lt;00:00, 15.46it/s]"
          }
        },
        "97a6d99000274b0d96d6f17352a7a182": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "defde1cfe6114b499a5af773920b6f3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b578bbb4bb1f4a32901dc7927d52c0ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc1ed7fac25a4eb7be4bdbe47d4abf5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4b31ac5dc2c4cc085ec18da5573b270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04110c806a0145949d1cffa883ff5943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eebb95109e243bd89a91cc1c2f21037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45da704d9ee545eda003fa69f9bfda2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d624517fd29e480f87007497ed73e5ef",
              "IPY_MODEL_4818f799b4524294bedaf858c180ea35",
              "IPY_MODEL_1949728e9a5941798c7f7f74a38a9213"
            ],
            "layout": "IPY_MODEL_caf5ebffab5f4e7eb117165ca701a674"
          }
        },
        "d624517fd29e480f87007497ed73e5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dde1a15fd014c609f00b64bb057f7d6",
            "placeholder": "​",
            "style": "IPY_MODEL_31adb31ed04a4c0d8c6e9fe0d6d4a1a3",
            "value": "Batches: 100%"
          }
        },
        "4818f799b4524294bedaf858c180ea35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a2a21e9ee34e81be7f7648a24818d2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da2974bc64644b3fae09e89627ad332d",
            "value": 1
          }
        },
        "1949728e9a5941798c7f7f74a38a9213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54ebfc21c4004344903ea69a2c636bcb",
            "placeholder": "​",
            "style": "IPY_MODEL_db3cb83e467b4d208dd0826eea63940d",
            "value": " 1/1 [00:00&lt;00:00, 10.10it/s]"
          }
        },
        "caf5ebffab5f4e7eb117165ca701a674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dde1a15fd014c609f00b64bb057f7d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31adb31ed04a4c0d8c6e9fe0d6d4a1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10a2a21e9ee34e81be7f7648a24818d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2974bc64644b3fae09e89627ad332d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54ebfc21c4004344903ea69a2c636bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db3cb83e467b4d208dd0826eea63940d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23c16f99982246739515c10d66ee31e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ddc19c150d349d49f7a19138566b567",
              "IPY_MODEL_1639e884e7ed4e708f4925818e2f5b22",
              "IPY_MODEL_fceefb0dc57e42758bf43ef2c5db6ed6"
            ],
            "layout": "IPY_MODEL_5f50d42163964019b5c6f3fa777586a2"
          }
        },
        "0ddc19c150d349d49f7a19138566b567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1d11ebcfd74de3928066997e127b33",
            "placeholder": "​",
            "style": "IPY_MODEL_253820e3878745659d1c4e0148d258f7",
            "value": "Batches: 100%"
          }
        },
        "1639e884e7ed4e708f4925818e2f5b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_724a9139f1854686bd4a2ebbba33a815",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45e4a97466f74dc8a1c493201c6c8631",
            "value": 1
          }
        },
        "fceefb0dc57e42758bf43ef2c5db6ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3867c6a9452c4b5aa53024b86a1ef970",
            "placeholder": "​",
            "style": "IPY_MODEL_d12cbf2d551c4bd79f157bac0ca6be33",
            "value": " 1/1 [00:00&lt;00:00, 17.87it/s]"
          }
        },
        "5f50d42163964019b5c6f3fa777586a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b1d11ebcfd74de3928066997e127b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "253820e3878745659d1c4e0148d258f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "724a9139f1854686bd4a2ebbba33a815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45e4a97466f74dc8a1c493201c6c8631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3867c6a9452c4b5aa53024b86a1ef970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d12cbf2d551c4bd79f157bac0ca6be33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "102532a7332045eabf4da8bc0d9227fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75d02b87acc4489ba6812d4635674a44",
              "IPY_MODEL_5608bcb11f274ab9b710e903b87d6431",
              "IPY_MODEL_18ba5faa8ea44db3aa6a5b53dfff1ddc"
            ],
            "layout": "IPY_MODEL_5ce168adddab49a191129f8ef48fcf08"
          }
        },
        "75d02b87acc4489ba6812d4635674a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40e3d78a5ac74e699d14b0e09a30f54c",
            "placeholder": "​",
            "style": "IPY_MODEL_a301d01b0a6a4efa885c50d4be35efed",
            "value": "Batches: 100%"
          }
        },
        "5608bcb11f274ab9b710e903b87d6431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9730737eea9e44b2b105661fbdaee4b5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0db645f07791425b9bdcdda7d1a3442c",
            "value": 1
          }
        },
        "18ba5faa8ea44db3aa6a5b53dfff1ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5fec64478eb423c9b7dd5d6d556eb21",
            "placeholder": "​",
            "style": "IPY_MODEL_e2684f8aebce47edbd8571642787d99b",
            "value": " 1/1 [00:00&lt;00:00, 14.69it/s]"
          }
        },
        "5ce168adddab49a191129f8ef48fcf08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40e3d78a5ac74e699d14b0e09a30f54c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a301d01b0a6a4efa885c50d4be35efed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9730737eea9e44b2b105661fbdaee4b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0db645f07791425b9bdcdda7d1a3442c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5fec64478eb423c9b7dd5d6d556eb21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2684f8aebce47edbd8571642787d99b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}